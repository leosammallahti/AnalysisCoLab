{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNsDQGeMvy/gw8q80wYu3I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leosammallahti/AnalysisCoLab/blob/main/QualityImprovement1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EQOyLzsYQHi",
        "outputId": "ff51fc19-4bfb-4468-95b1-beb22d906b82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Total rows: 2289\n",
            "Rows with empty abstract: 158\n",
            "Rows with populated abstract: 2131\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Setup and Load Data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('/content/drive/MyDrive/AEA_RCT_Parsed/structured_studies_full_v2.csv')\n",
        "\n",
        "print(f\"Total rows: {len(df)}\")\n",
        "print(f\"Rows with empty abstract: {df['abstract'].isna().sum()}\")\n",
        "print(f\"Rows with populated abstract: {df['abstract'].notna().sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Identify Abstract Recovery Candidates\n",
        "# Find rows where abstract is empty/null\n",
        "empty_abstract_mask = df['abstract'].isna() | (df['abstract'].str.strip() == '')\n",
        "df_empty_abstract = df[empty_abstract_mask].copy()\n",
        "\n",
        "print(f\"Found {len(df_empty_abstract)} rows with empty/null abstract\")\n",
        "\n",
        "# Define the relevant abstract columns to check\n",
        "relevant_abstract_cols = [\n",
        "    'relevant_1_abstract', 'relevant_2_abstract', 'relevant_3_abstract',\n",
        "    'relevant_4_abstract', 'relevant_5_abstract', 'relevant_6_abstract',\n",
        "    'relevant_7_abstract'\n",
        "]\n",
        "\n",
        "# Check which of these columns exist in the dataframe\n",
        "existing_relevant_cols = [col for col in relevant_abstract_cols if col in df.columns]\n",
        "print(f\"\\nFound {len(existing_relevant_cols)} relevant_abstract columns in dataset\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMDS4jeYYiX3",
        "outputId": "09ec1542-1650-4eba-c9bd-6e2cbae25901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 158 rows with empty/null abstract\n",
            "\n",
            "Found 7 relevant_abstract columns in dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Analyze Recovery Potential\n",
        "recovery_candidates = []\n",
        "\n",
        "for idx, row in df_empty_abstract.iterrows():\n",
        "    # Check each relevant_abstract column for this row\n",
        "    for col in existing_relevant_cols:\n",
        "        if pd.notna(row[col]) and str(row[col]).strip() != '':\n",
        "            recovery_candidates.append({\n",
        "                'rct_id': row['rct_id'],\n",
        "                'title': row['title'][:100] if pd.notna(row['title']) else 'NO TITLE',\n",
        "                'source_column': col,\n",
        "                'abstract_preview': str(row[col])[:200] + '...' if len(str(row[col])) > 200 else str(row[col]),\n",
        "                'abstract_length': len(str(row[col]))\n",
        "            })\n",
        "\n",
        "recovery_df = pd.DataFrame(recovery_candidates)\n",
        "\n",
        "print(f\"Found {len(recovery_df)} potential abstract recovery cases\")\n",
        "print(f\"Unique RCTs that could be recovered: {recovery_df['rct_id'].nunique()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "JrKKRqwFYqMs",
        "outputId": "c15ac288-98a0-4eca-e08f-9a7aa0da7be2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 potential abstract recovery cases\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'rct_id'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1501175510.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Found {len(recovery_df)} potential abstract recovery cases\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unique RCTs that could be recovered: {recovery_df['rct_id'].nunique()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/range.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    415\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHashable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_indexing_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'rct_id'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Analyze Recovery Potential (Fixed)\n",
        "recovery_candidates = []\n",
        "\n",
        "for idx, row in df_empty_abstract.iterrows():\n",
        "    # Check each relevant_abstract column for this row\n",
        "    for col in existing_relevant_cols:\n",
        "        if pd.notna(row[col]) and str(row[col]).strip() != '':\n",
        "            recovery_candidates.append({\n",
        "                'rct_id': row['rct_id'],\n",
        "                'title': row['title'][:100] if pd.notna(row['title']) else 'NO TITLE',\n",
        "                'source_column': col,\n",
        "                'abstract_preview': str(row[col])[:200] + '...' if len(str(row[col])) > 200 else str(row[col]),\n",
        "                'abstract_length': len(str(row[col]))\n",
        "            })\n",
        "\n",
        "recovery_df = pd.DataFrame(recovery_candidates)\n",
        "\n",
        "print(f\"Found {len(recovery_df)} potential abstract recovery cases\")\n",
        "\n",
        "if len(recovery_df) > 0:\n",
        "    print(f\"Unique RCTs that could be recovered: {recovery_df['rct_id'].nunique()}\")\n",
        "else:\n",
        "    print(\"\\nNo recovery candidates found. Investigating further...\")\n",
        "\n",
        "    # Diagnostic: Check if relevant columns have ANY data at all\n",
        "    print(\"\\nChecking relevant_abstract columns overall population:\")\n",
        "    for col in existing_relevant_cols:\n",
        "        populated = df[col].notna().sum()\n",
        "        print(f\"  {col}: {populated} populated rows (out of {len(df)} total)\")\n",
        "\n",
        "    # Check if these columns have data but NOT for the empty abstract rows\n",
        "    print(\"\\nChecking if empty abstract rows have ANY relevant_abstract data:\")\n",
        "    for col in existing_relevant_cols:\n",
        "        if col in df_empty_abstract.columns:\n",
        "            populated_in_empty = df_empty_abstract[col].notna().sum()\n",
        "            print(f\"  {col}: {populated_in_empty} populated (out of {len(df_empty_abstract)} empty abstract rows)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEQ9LVsEZGgm",
        "outputId": "300a6cdb-35f2-4338-9511-ff5e136859c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 potential abstract recovery cases\n",
            "\n",
            "No recovery candidates found. Investigating further...\n",
            "\n",
            "Checking relevant_abstract columns overall population:\n",
            "  relevant_1_abstract: 645 populated rows (out of 2289 total)\n",
            "  relevant_2_abstract: 94 populated rows (out of 2289 total)\n",
            "  relevant_3_abstract: 18 populated rows (out of 2289 total)\n",
            "  relevant_4_abstract: 5 populated rows (out of 2289 total)\n",
            "  relevant_5_abstract: 3 populated rows (out of 2289 total)\n",
            "  relevant_6_abstract: 2 populated rows (out of 2289 total)\n",
            "  relevant_7_abstract: 0 populated rows (out of 2289 total)\n",
            "\n",
            "Checking if empty abstract rows have ANY relevant_abstract data:\n",
            "  relevant_1_abstract: 0 populated (out of 158 empty abstract rows)\n",
            "  relevant_2_abstract: 0 populated (out of 158 empty abstract rows)\n",
            "  relevant_3_abstract: 0 populated (out of 158 empty abstract rows)\n",
            "  relevant_4_abstract: 0 populated (out of 158 empty abstract rows)\n",
            "  relevant_5_abstract: 0 populated (out of 158 empty abstract rows)\n",
            "  relevant_6_abstract: 0 populated (out of 158 empty abstract rows)\n",
            "  relevant_7_abstract: 0 populated (out of 158 empty abstract rows)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Analyze Field Availability and Quality for New CSV\n",
        "print(\"FIELD MAPPING ANALYSIS FOR NEW CONSOLIDATED CSV\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Check what we have vs what we need\n",
        "field_mapping = {\n",
        "    'Title': 'title',\n",
        "    'Researcher(s)': ['primary_investigator_name', 'other_pi_1_name', 'other_pi_2_name', 'other_pi_3_name', 'other_pi_4_name'],\n",
        "    'Researcher affiliation': ['primary_investigator_affiliation', 'other_pi_1_affiliation', 'other_pi_2_affiliation', 'other_pi_3_affiliation', 'other_pi_4_affiliation'],\n",
        "    'Abstract': 'abstract',\n",
        "    'Description of the intervention': 'interventions',\n",
        "    'Primary outcomes': 'primary_outcomes_endpoints',\n",
        "    'Secondary outcomes': 'secondary_outcomes_endpoints',\n",
        "    'Findings': ['final_findings_snippet', 'findings_snippet', 'llm_snippet', 'findings_fulltext'],\n",
        "    'Country': 'country',\n",
        "    'Subregion': 'region',\n",
        "    'Year': ['start_date', 'first_published', 'initial_registration_date'],\n",
        "    'Population': 'population_extracted',  # From AI enrichment\n",
        "    'Search vector': 'search_vector',\n",
        "    'keywords': 'keywords',\n",
        "    'keywords_additional': 'keywords_additional',\n",
        "    'jel_codes': 'jel_codes'\n",
        "}\n",
        "\n",
        "# Check availability and population rates\n",
        "for target, source in field_mapping.items():\n",
        "    if isinstance(source, list):\n",
        "        print(f\"\\n{target} (needs consolidation from multiple fields):\")\n",
        "        for s in source:\n",
        "            if s in df.columns:\n",
        "                populated = df[s].notna().sum()\n",
        "                print(f\"  - {s}: {populated}/{len(df)} ({populated/len(df)*100:.1f}%)\")\n",
        "            else:\n",
        "                print(f\"  - {s}: NOT IN DATASET\")\n",
        "    else:\n",
        "        if source in df.columns:\n",
        "            populated = df[source].notna().sum()\n",
        "            print(f\"\\n{target}: {source} - {populated}/{len(df)} ({populated/len(df)*100:.1f}%)\")\n",
        "        else:\n",
        "            print(f\"\\n{target}: {source} - NOT IN DATASET\")\n",
        "\n",
        "# Check for AI-generated keyword fields that might not exist yet\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CHECKING FOR KEYWORD SUBCATEGORIES:\")\n",
        "keyword_fields = ['keywords_sector', 'keywords_mechanisms', 'keywords_implementation', 'keywords_context']\n",
        "for field in keyword_fields:\n",
        "    if field in df.columns:\n",
        "        populated = df[field].notna().sum()\n",
        "        print(f\"  {field}: {populated}/{len(df)} ({populated/len(df)*100:.1f}%)\")\n",
        "    else:\n",
        "        print(f\"  {field}: NOT IN DATASET - needs to be generated\")\n",
        "\n",
        "# Check for duplicates\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DUPLICATE ANALYSIS:\")\n",
        "print(f\"Total rows: {len(df)}\")\n",
        "print(f\"Unique rct_ids: {df['rct_id'].nunique()}\")\n",
        "print(f\"Duplicate rct_ids: {df['rct_id'].duplicated().sum()}\")\n",
        "\n",
        "if df['rct_id'].duplicated().any():\n",
        "    print(\"\\nDuplicate RCT IDs found:\")\n",
        "    duplicates = df[df['rct_id'].duplicated(keep=False)].sort_values('rct_id')\n",
        "    print(duplicates[['rct_id', 'title']].head(10))\n",
        "\n",
        "# Check title duplicates (different studies with same title)\n",
        "title_duplicates = df[df['title'].duplicated(keep=False) & df['title'].notna()]\n",
        "if len(title_duplicates) > 0:\n",
        "    print(f\"\\nRows with duplicate titles: {len(title_duplicates)}\")\n",
        "    print(\"Sample of duplicate titles:\")\n",
        "    for title in title_duplicates['title'].unique()[:3]:\n",
        "        matching = df[df['title'] == title][['rct_id', 'title', 'primary_investigator_name']]\n",
        "        print(f\"\\n  Title: '{title[:80]}...'\")\n",
        "        print(f\"  Found in {len(matching)} rows\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6nuufNicnV1",
        "outputId": "c05915e8-67b0-45b1-9f4a-1f778af26c9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FIELD MAPPING ANALYSIS FOR NEW CONSOLIDATED CSV\n",
            "================================================================================\n",
            "\n",
            "Title: title - 2286/2289 (99.9%)\n",
            "\n",
            "Researcher(s) (needs consolidation from multiple fields):\n",
            "  - primary_investigator_name: 2288/2289 (100.0%)\n",
            "  - other_pi_1_name: 1720/2289 (75.1%)\n",
            "  - other_pi_2_name: 14/2289 (0.6%)\n",
            "  - other_pi_3_name: 4/2289 (0.2%)\n",
            "  - other_pi_4_name: 1/2289 (0.0%)\n",
            "\n",
            "Researcher affiliation (needs consolidation from multiple fields):\n",
            "  - primary_investigator_affiliation: 2136/2289 (93.3%)\n",
            "  - other_pi_1_affiliation: 468/2289 (20.4%)\n",
            "  - other_pi_2_affiliation: 1/2289 (0.0%)\n",
            "  - other_pi_3_affiliation: 2/2289 (0.1%)\n",
            "  - other_pi_4_affiliation: 0/2289 (0.0%)\n",
            "\n",
            "Abstract: abstract - 2131/2289 (93.1%)\n",
            "\n",
            "Description of the intervention: interventions - 2136/2289 (93.3%)\n",
            "\n",
            "Primary outcomes: primary_outcomes_endpoints - 2136/2289 (93.3%)\n",
            "\n",
            "Secondary outcomes: secondary_outcomes_endpoints - 2136/2289 (93.3%)\n",
            "\n",
            "Findings (needs consolidation from multiple fields):\n",
            "  - final_findings_snippet: 2289/2289 (100.0%)\n",
            "  - findings_snippet: 766/2289 (33.5%)\n",
            "  - llm_snippet: 1066/2289 (46.6%)\n",
            "  - findings_fulltext: 766/2289 (33.5%)\n",
            "\n",
            "Country: country - 2134/2289 (93.2%)\n",
            "\n",
            "Subregion: region - 1002/2289 (43.8%)\n",
            "\n",
            "Year (needs consolidation from multiple fields):\n",
            "  - start_date: 2136/2289 (93.3%)\n",
            "  - first_published: 2136/2289 (93.3%)\n",
            "  - initial_registration_date: 2118/2289 (92.5%)\n",
            "\n",
            "Population: population_extracted - 2110/2289 (92.2%)\n",
            "\n",
            "Search vector: search_vector - 2289/2289 (100.0%)\n",
            "\n",
            "keywords: keywords - 2136/2289 (93.3%)\n",
            "\n",
            "keywords_additional: keywords_additional - 2136/2289 (93.3%)\n",
            "\n",
            "jel_codes: jel_codes - 1229/2289 (53.7%)\n",
            "\n",
            "================================================================================\n",
            "CHECKING FOR KEYWORD SUBCATEGORIES:\n",
            "  keywords_sector: 2116/2289 (92.4%)\n",
            "  keywords_mechanisms: 2116/2289 (92.4%)\n",
            "  keywords_implementation: 2117/2289 (92.5%)\n",
            "  keywords_context: 2114/2289 (92.4%)\n",
            "\n",
            "================================================================================\n",
            "DUPLICATE ANALYSIS:\n",
            "Total rows: 2289\n",
            "Unique rct_ids: 2281\n",
            "Duplicate rct_ids: 7\n",
            "\n",
            "Duplicate RCT IDs found:\n",
            "      rct_id                                              title\n",
            "124   2868.0  Experimental evaluation of Semillas de Apego, ...\n",
            "1752  2868.0  Experimental evaluation of Semillas de Apego, ...\n",
            "179   3283.0  Networks and Global Health: Experimental Evide...\n",
            "1808  3283.0  Networks and Global Health: Experimental Evide...\n",
            "1817  3344.0  Bangladesh Chars Tobacco Assessment Project (C...\n",
            "187   3344.0  Bangladesh Chars Tobacco Assessment Project (C...\n",
            "448   5735.0  The Behavioral Effect of Facial Masks During t...\n",
            "2021  5735.0  The Behavioral Effect of Facial Masks During t...\n",
            "453   5781.0                                                NaN\n",
            "2024  5781.0                                                NaN\n",
            "\n",
            "Rows with duplicate titles: 24\n",
            "Sample of duplicate titles:\n",
            "\n",
            "  Title: 'Experimental evaluation of Semillas de Apego, a group-based program to foster ma...'\n",
            "  Found in 2 rows\n",
            "\n",
            "  Title: 'Networks and Global Health: Experimental Evidence of Women’s Social Networks, Re...'\n",
            "  Found in 2 rows\n",
            "\n",
            "  Title: 'Bangladesh Chars Tobacco Assessment Project (CTAP) 2018...'\n",
            "  Found in 2 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Investigate Duplicate RCT IDs in Detail\n",
        "print(\"DETAILED DUPLICATE ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Get all duplicate rct_ids\n",
        "duplicate_mask = df['rct_id'].duplicated(keep=False)\n",
        "df_duplicates = df[duplicate_mask].sort_values('rct_id')\n",
        "\n",
        "print(f\"Found {len(df_duplicates)} rows with duplicate rct_ids\")\n",
        "print(f\"Unique duplicate rct_ids: {df_duplicates['rct_id'].nunique()}\")\n",
        "\n",
        "# Analyze each duplicate group\n",
        "duplicate_groups = df_duplicates.groupby('rct_id')\n",
        "\n",
        "for rct_id, group in duplicate_groups:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"RCT ID: {rct_id} - Found in {len(group)} rows\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # Check key fields for differences\n",
        "    fields_to_compare = [\n",
        "        'title', 'primary_investigator_name', 'final_has_findings',\n",
        "        'final_findings_source_type', 'start_date', 'country'\n",
        "    ]\n",
        "\n",
        "    for field in fields_to_compare:\n",
        "        if field in group.columns:\n",
        "            unique_values = group[field].unique()\n",
        "            if len(unique_values) > 1:\n",
        "                print(f\"  {field}: DIFFERENT VALUES\")\n",
        "                for idx, row in group.iterrows():\n",
        "                    print(f\"    Row {idx}: {str(row[field])[:100]}\")\n",
        "            else:\n",
        "                print(f\"  {field}: {unique_values[0] if pd.notna(unique_values[0]) else 'NaN'}\")\n",
        "\n",
        "    # Check which has findings\n",
        "    print(\"\\n  Findings status:\")\n",
        "    for idx, row in group.iterrows():\n",
        "        has_findings = row.get('final_has_findings', 'N/A')\n",
        "        snippet_length = len(str(row.get('final_findings_snippet', ''))) if pd.notna(row.get('final_findings_snippet')) else 0\n",
        "        print(f\"    Row {idx}: has_findings={has_findings}, snippet_length={snippet_length}\")\n",
        "\n",
        "# Check if duplicates come from different source files (if that column exists)\n",
        "if 'source_file' in df.columns:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SOURCE FILE ANALYSIS FOR DUPLICATES:\")\n",
        "    for rct_id, group in duplicate_groups:\n",
        "        sources = group['source_file'].unique()\n",
        "        print(f\"  RCT {rct_id}: from files {sources}\")\n",
        "\n",
        "# Recommendation for handling duplicates\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DUPLICATE RESOLUTION STRATEGY:\")\n",
        "print(\"1. Check if duplicates are from WITH_findings vs WITHOUT_findings merges\")\n",
        "print(\"2. Keep the row with the most complete data (especially findings)\")\n",
        "print(\"3. Create a deduplication log showing which row was kept and why\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDscLKf3eA4b",
        "outputId": "688ce472-2821-4ecf-bd9e-cd54f6b11973"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DETAILED DUPLICATE ANALYSIS\n",
            "================================================================================\n",
            "Found 14 rows with duplicate rct_ids\n",
            "Unique duplicate rct_ids: 7\n",
            "\n",
            "================================================================================\n",
            "RCT ID: 2868.0 - Found in 2 rows\n",
            "--------------------------------------------------------------------------------\n",
            "  title: Experimental evaluation of Semillas de Apego, a group-based program to foster maternal mental health and early childhood development among violence exposed communities in Colombia.\n",
            "  primary_investigator_name: Andres Moya\n",
            "  final_has_findings: Yes\n",
            "  final_findings_source_type: DIFFERENT VALUES\n",
            "    Row 124: institutional\n",
            "    Row 1752: unknown\n",
            "  start_date: DIFFERENT VALUES\n",
            "    Row 124: 2018-08-19\n",
            "    Row 1752: nan\n",
            "  country: DIFFERENT VALUES\n",
            "    Row 124: Colombia\n",
            "    Row 1752: nan\n",
            "\n",
            "  Findings status:\n",
            "    Row 124: has_findings=Yes, snippet_length=357\n",
            "    Row 1752: has_findings=Yes, snippet_length=361\n",
            "\n",
            "================================================================================\n",
            "RCT ID: 3283.0 - Found in 2 rows\n",
            "--------------------------------------------------------------------------------\n",
            "  title: Networks and Global Health: Experimental Evidence of Women’s Social Networks, Reproductive Health, and Well-Being\n",
            "  primary_investigator_name: S ANUKRITI\n",
            "  final_has_findings: Yes\n",
            "  final_findings_source_type: institutional\n",
            "  start_date: DIFFERENT VALUES\n",
            "    Row 179: 2018-08-19\n",
            "    Row 1808: nan\n",
            "  country: DIFFERENT VALUES\n",
            "    Row 179: India\n",
            "    Row 1808: nan\n",
            "\n",
            "  Findings status:\n",
            "    Row 179: has_findings=Yes, snippet_length=411\n",
            "    Row 1808: has_findings=Yes, snippet_length=276\n",
            "\n",
            "================================================================================\n",
            "RCT ID: 3344.0 - Found in 2 rows\n",
            "--------------------------------------------------------------------------------\n",
            "  title: Bangladesh Chars Tobacco Assessment Project (CTAP) 2018\n",
            "  primary_investigator_name: Adnan M. S. Fakir\n",
            "  final_has_findings: Yes\n",
            "  final_findings_source_type: institutional\n",
            "  start_date: DIFFERENT VALUES\n",
            "    Row 1817: nan\n",
            "    Row 187: 2018-08-19\n",
            "  country: DIFFERENT VALUES\n",
            "    Row 1817: nan\n",
            "    Row 187: Bangladesh\n",
            "\n",
            "  Findings status:\n",
            "    Row 1817: has_findings=Yes, snippet_length=410\n",
            "    Row 187: has_findings=Yes, snippet_length=392\n",
            "\n",
            "================================================================================\n",
            "RCT ID: 5735.0 - Found in 2 rows\n",
            "--------------------------------------------------------------------------------\n",
            "  title: The Behavioral Effect of Facial Masks During the COVID-19 Pandemic\n",
            "  primary_investigator_name: Jana Friedrichsen\n",
            "  final_has_findings: Yes\n",
            "  final_findings_source_type: working_paper\n",
            "  start_date: DIFFERENT VALUES\n",
            "    Row 448: 2020-08-19\n",
            "    Row 2021: nan\n",
            "  country: DIFFERENT VALUES\n",
            "    Row 448: Germany\n",
            "    Row 2021: nan\n",
            "\n",
            "  Findings status:\n",
            "    Row 448: has_findings=Yes, snippet_length=524\n",
            "    Row 2021: has_findings=Yes, snippet_length=341\n",
            "\n",
            "================================================================================\n",
            "RCT ID: 5781.0 - Found in 2 rows\n",
            "--------------------------------------------------------------------------------\n",
            "  title: NaN\n",
            "  primary_investigator_name: Xavier Gine\n",
            "  final_has_findings: Yes\n",
            "  final_findings_source_type: working_paper\n",
            "  start_date: DIFFERENT VALUES\n",
            "    Row 453: 2016-08-19\n",
            "    Row 2024: nan\n",
            "  country: DIFFERENT VALUES\n",
            "    Row 453: Malawi\n",
            "    Row 2024: nan\n",
            "\n",
            "  Findings status:\n",
            "    Row 453: has_findings=Yes, snippet_length=161\n",
            "    Row 2024: has_findings=Yes, snippet_length=172\n",
            "\n",
            "================================================================================\n",
            "RCT ID: 7650.0 - Found in 2 rows\n",
            "--------------------------------------------------------------------------------\n",
            "  title: Keep your eyes on the prize: How to make penalty contracts work 2\n",
            "  primary_investigator_name: Andrea Essl\n",
            "  final_has_findings: Yes\n",
            "  final_findings_source_type: journal\n",
            "  start_date: DIFFERENT VALUES\n",
            "    Row 2139: nan\n",
            "    Row 652: 2021-08-19\n",
            "  country: DIFFERENT VALUES\n",
            "    Row 2139: nan\n",
            "    Row 652: Switzerland\n",
            "\n",
            "  Findings status:\n",
            "    Row 2139: has_findings=Yes, snippet_length=309\n",
            "    Row 652: has_findings=Yes, snippet_length=515\n",
            "\n",
            "================================================================================\n",
            "RCT ID: 7741.0 - Found in 2 rows\n",
            "--------------------------------------------------------------------------------\n",
            "  title: Non-Linear Public Goods Games\n",
            "  primary_investigator_name: Maximilian Andres\n",
            "  final_has_findings: Yes\n",
            "  final_findings_source_type: working_paper\n",
            "  start_date: DIFFERENT VALUES\n",
            "    Row 667: 2021-08-19\n",
            "    Row 2151: nan\n",
            "  country: DIFFERENT VALUES\n",
            "    Row 667: Germany\n",
            "    Row 2151: nan\n",
            "\n",
            "  Findings status:\n",
            "    Row 667: has_findings=Yes, snippet_length=181\n",
            "    Row 2151: has_findings=Yes, snippet_length=194\n",
            "\n",
            "================================================================================\n",
            "SOURCE FILE ANALYSIS FOR DUPLICATES:\n",
            "  RCT 2868.0: from files ['sonar_append_all']\n",
            "  RCT 3283.0: from files ['sonar_append_all']\n",
            "  RCT 3344.0: from files ['sonar_append_all']\n",
            "  RCT 5735.0: from files ['sonar_append_all']\n",
            "  RCT 5781.0: from files ['sonar_append_all']\n",
            "  RCT 7650.0: from files ['sonar_append_all']\n",
            "  RCT 7741.0: from files ['sonar_append_all']\n",
            "\n",
            "================================================================================\n",
            "DUPLICATE RESOLUTION STRATEGY:\n",
            "1. Check if duplicates are from WITH_findings vs WITHOUT_findings merges\n",
            "2. Keep the row with the most complete data (especially findings)\n",
            "3. Create a deduplication log showing which row was kept and why\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Deduplicate by Keeping Most Complete Row\n",
        "print(\"DEDUPLICATION PROCESS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create a copy for deduplication\n",
        "df_clean = df.copy()\n",
        "\n",
        "# Function to score row completeness\n",
        "def calculate_completeness_score(row):\n",
        "    \"\"\"Score based on important fields being populated\"\"\"\n",
        "    score = 0\n",
        "    important_fields = {\n",
        "        'start_date': 10,  # High weight for metadata\n",
        "        'country': 10,\n",
        "        'abstract': 5,\n",
        "        'interventions': 5,\n",
        "        'primary_outcomes_endpoints': 3,\n",
        "        'region': 3,\n",
        "        'final_findings_snippet': 2,\n",
        "    }\n",
        "\n",
        "    for field, weight in important_fields.items():\n",
        "        if field in row.index and pd.notna(row[field]) and str(row[field]).strip() != '':\n",
        "            score += weight\n",
        "            # Bonus for longer content in text fields\n",
        "            if field in ['abstract', 'final_findings_snippet', 'interventions']:\n",
        "                score += min(len(str(row[field])) / 100, 5)  # Up to 5 bonus points\n",
        "\n",
        "    return score\n",
        "\n",
        "# Process duplicates\n",
        "dedup_log = []\n",
        "rows_to_drop = []\n",
        "\n",
        "for rct_id in df_clean[df_clean['rct_id'].duplicated(keep=False)]['rct_id'].unique():\n",
        "    duplicate_rows = df_clean[df_clean['rct_id'] == rct_id]\n",
        "\n",
        "    # Score each duplicate\n",
        "    scores = {}\n",
        "    for idx in duplicate_rows.index:\n",
        "        scores[idx] = calculate_completeness_score(duplicate_rows.loc[idx])\n",
        "\n",
        "    # Keep the row with highest score\n",
        "    best_idx = max(scores, key=scores.get)\n",
        "    drop_indices = [idx for idx in scores.keys() if idx != best_idx]\n",
        "\n",
        "    # Log the decision\n",
        "    dedup_log.append({\n",
        "        'rct_id': rct_id,\n",
        "        'kept_row': best_idx,\n",
        "        'kept_score': scores[best_idx],\n",
        "        'dropped_rows': drop_indices,\n",
        "        'dropped_scores': [scores[idx] for idx in drop_indices],\n",
        "        'title': df_clean.loc[best_idx, 'title'] if pd.notna(df_clean.loc[best_idx, 'title']) else 'NO TITLE'\n",
        "    })\n",
        "\n",
        "    rows_to_drop.extend(drop_indices)\n",
        "\n",
        "# Drop duplicate rows\n",
        "df_clean = df_clean.drop(index=rows_to_drop)\n",
        "\n",
        "# Report results\n",
        "print(f\"Original rows: {len(df)}\")\n",
        "print(f\"Rows removed: {len(rows_to_drop)}\")\n",
        "print(f\"Final rows: {len(df_clean)}\")\n",
        "print(f\"Unique rct_ids: {df_clean['rct_id'].nunique()}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DEDUPLICATION LOG:\")\n",
        "for entry in dedup_log:\n",
        "    print(f\"\\nRCT {entry['rct_id']}: {entry['title'][:60]}...\")\n",
        "    print(f\"  Kept row {entry['kept_row']} (score: {entry['kept_score']:.1f})\")\n",
        "    print(f\"  Dropped row(s) {entry['dropped_rows']} (scores: {entry['dropped_scores']})\")\n",
        "\n",
        "# Verify no more duplicates\n",
        "if df_clean['rct_id'].duplicated().any():\n",
        "    print(\"\\nWARNING: Still have duplicates!\")\n",
        "else:\n",
        "    print(\"\\nSUCCESS: No more duplicate rct_ids\")\n",
        "\n",
        "# Update df to use the clean version\n",
        "df = df_clean\n",
        "print(f\"\\nDataset updated. Working with {len(df)} unique studies.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOjBriofeyvv",
        "outputId": "cf29a654-4e52-412c-927a-c2bc6f30e3f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEDUPLICATION PROCESS\n",
            "================================================================================\n",
            "Original rows: 2289\n",
            "Rows removed: 7\n",
            "Final rows: 2282\n",
            "Unique rct_ids: 2281\n",
            "\n",
            "================================================================================\n",
            "DEDUPLICATION LOG:\n",
            "\n",
            "RCT 2868.0: Experimental evaluation of Semillas de Apego, a group-based ...\n",
            "  Kept row 124 (score: 51.6)\n",
            "  Dropped row(s) [1752] (scores: [5.609999999999999])\n",
            "\n",
            "RCT 3283.0: Networks and Global Health: Experimental Evidence of Women’s...\n",
            "  Kept row 179 (score: 52.1)\n",
            "  Dropped row(s) [1808] (scores: [4.76])\n",
            "\n",
            "RCT 3344.0: Bangladesh Chars Tobacco Assessment Project (CTAP) 2018...\n",
            "  Kept row 187 (score: 51.0)\n",
            "  Dropped row(s) [1817] (scores: [6.1])\n",
            "\n",
            "RCT 5735.0: The Behavioral Effect of Facial Masks During the COVID-19 Pa...\n",
            "  Kept row 448 (score: 53.0)\n",
            "  Dropped row(s) [2021] (scores: [5.41])\n",
            "\n",
            "RCT 5781.0: NO TITLE...\n",
            "  Kept row 453 (score: 46.6)\n",
            "  Dropped row(s) [2024] (scores: [3.7199999999999998])\n",
            "\n",
            "RCT 7650.0: Keep your eyes on the prize: How to make penalty contracts w...\n",
            "  Kept row 652 (score: 50.0)\n",
            "  Dropped row(s) [2139] (scores: [5.09])\n",
            "\n",
            "RCT 7741.0: Non-Linear Public Goods Games...\n",
            "  Kept row 667 (score: 43.1)\n",
            "  Dropped row(s) [2151] (scores: [3.94])\n",
            "\n",
            "SUCCESS: No more duplicate rct_ids\n",
            "\n",
            "Dataset updated. Working with 2282 unique studies.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Build Consolidated Dataset with Required Fields\n",
        "print(\"BUILDING CONSOLIDATED DATASET\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Initialize new dataframe with required columns\n",
        "consolidated = pd.DataFrame()\n",
        "\n",
        "# 1. Simple direct mappings\n",
        "consolidated['Title'] = df['title']\n",
        "consolidated['Country'] = df['country']\n",
        "consolidated['Subregion'] = df['region']\n",
        "consolidated['Abstract'] = df['abstract']\n",
        "consolidated['Description_of_intervention'] = df['interventions']\n",
        "consolidated['Primary_outcomes'] = df['primary_outcomes_endpoints']\n",
        "consolidated['Secondary_outcomes'] = df['secondary_outcomes_endpoints']\n",
        "consolidated['Population'] = df.get('population_extracted', pd.Series([None]*len(df)))\n",
        "consolidated['Search_vector'] = df['search_vector']\n",
        "consolidated['keywords'] = df['keywords']\n",
        "consolidated['keywords_additional'] = df['keywords_additional']\n",
        "consolidated['jel_codes'] = df['jel_codes']\n",
        "\n",
        "# Keyword subcategories\n",
        "consolidated['keywords_sector'] = df.get('keywords_sector', pd.Series([None]*len(df)))\n",
        "consolidated['keywords_mechanisms'] = df.get('keywords_mechanisms', pd.Series([None]*len(df)))\n",
        "consolidated['keywords_implementation'] = df.get('keywords_implementation', pd.Series([None]*len(df)))\n",
        "consolidated['keywords_context'] = df.get('keywords_context', pd.Series([None]*len(df)))\n",
        "\n",
        "# 2. Consolidated Researchers (combine all PI names)\n",
        "def consolidate_researchers(row):\n",
        "    researchers = []\n",
        "    pi_cols = ['primary_investigator_name', 'other_pi_1_name', 'other_pi_2_name', 'other_pi_3_name', 'other_pi_4_name']\n",
        "    for col in pi_cols:\n",
        "        if col in row.index and pd.notna(row[col]) and str(row[col]).strip():\n",
        "            researchers.append(str(row[col]).strip())\n",
        "    return '; '.join(researchers) if researchers else None\n",
        "\n",
        "consolidated['Researchers'] = df.apply(consolidate_researchers, axis=1)\n",
        "\n",
        "# 3. Consolidated Affiliations\n",
        "def consolidate_affiliations(row):\n",
        "    affiliations = []\n",
        "    aff_cols = ['primary_investigator_affiliation', 'other_pi_1_affiliation', 'other_pi_2_affiliation', 'other_pi_3_affiliation', 'other_pi_4_affiliation']\n",
        "    for col in aff_cols:\n",
        "        if col in row.index and pd.notna(row[col]) and str(row[col]).strip():\n",
        "            affiliations.append(str(row[col]).strip())\n",
        "    return '; '.join(affiliations) if affiliations else None\n",
        "\n",
        "consolidated['Researcher_affiliation'] = df.apply(consolidate_affiliations, axis=1)\n",
        "\n",
        "# 4. Extract Year from dates (prioritize start_date, then first_published, then initial_registration_date)\n",
        "def extract_year(row):\n",
        "    for date_col in ['start_date', 'first_published', 'initial_registration_date']:\n",
        "        if date_col in row.index and pd.notna(row[date_col]):\n",
        "            try:\n",
        "                # Handle different date formats\n",
        "                date_str = str(row[date_col])\n",
        "                if len(date_str) >= 4:\n",
        "                    return date_str[:4]\n",
        "            except:\n",
        "                continue\n",
        "    return None\n",
        "\n",
        "consolidated['Year'] = df.apply(extract_year, axis=1)\n",
        "\n",
        "# 5. Consolidated Findings (prioritize final_findings_snippet as it's 100% populated)\n",
        "consolidated['Findings'] = df['final_findings_snippet']\n",
        "\n",
        "# Add RCT ID for reference\n",
        "consolidated['rct_id'] = df['rct_id']\n",
        "\n",
        "# Report on consolidated dataset\n",
        "print(f\"Created consolidated dataset with {len(consolidated)} rows and {len(consolidated.columns)} columns\")\n",
        "print(\"\\nColumn population rates:\")\n",
        "for col in consolidated.columns:\n",
        "    populated = consolidated[col].notna().sum()\n",
        "    pct = (populated / len(consolidated)) * 100\n",
        "    print(f\"  {col}: {populated}/{len(consolidated)} ({pct:.1f}%)\")\n",
        "\n",
        "# Check for any remaining issues\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"QUALITY CHECKS:\")\n",
        "print(f\"Unique rct_ids: {consolidated['rct_id'].nunique()}\")\n",
        "print(f\"Rows with Title: {consolidated['Title'].notna().sum()}\")\n",
        "print(f\"Rows with Researchers: {consolidated['Researchers'].notna().sum()}\")\n",
        "print(f\"Rows with Abstract: {consolidated['Abstract'].notna().sum()}\")\n",
        "print(f\"Rows with Findings: {consolidated['Findings'].notna().sum()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRn22K_TfbVa",
        "outputId": "5dd6cbbc-6d40-4394-ec87-8eddaa6aef24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BUILDING CONSOLIDATED DATASET\n",
            "================================================================================\n",
            "Created consolidated dataset with 2282 rows and 21 columns\n",
            "\n",
            "Column population rates:\n",
            "  Title: 2280/2282 (99.9%)\n",
            "  Country: 2134/2282 (93.5%)\n",
            "  Subregion: 1002/2282 (43.9%)\n",
            "  Abstract: 2131/2282 (93.4%)\n",
            "  Description_of_intervention: 2136/2282 (93.6%)\n",
            "  Primary_outcomes: 2136/2282 (93.6%)\n",
            "  Secondary_outcomes: 2136/2282 (93.6%)\n",
            "  Population: 2110/2282 (92.5%)\n",
            "  Search_vector: 2282/2282 (100.0%)\n",
            "  keywords: 2136/2282 (93.6%)\n",
            "  keywords_additional: 2136/2282 (93.6%)\n",
            "  jel_codes: 1229/2282 (53.9%)\n",
            "  keywords_sector: 2116/2282 (92.7%)\n",
            "  keywords_mechanisms: 2116/2282 (92.7%)\n",
            "  keywords_implementation: 2117/2282 (92.8%)\n",
            "  keywords_context: 2114/2282 (92.6%)\n",
            "  Researchers: 2281/2282 (100.0%)\n",
            "  Researcher_affiliation: 2136/2282 (93.6%)\n",
            "  Year: 2136/2282 (93.6%)\n",
            "  Findings: 2282/2282 (100.0%)\n",
            "  rct_id: 2281/2282 (100.0%)\n",
            "\n",
            "================================================================================\n",
            "QUALITY CHECKS:\n",
            "Unique rct_ids: 2281\n",
            "Rows with Title: 2280\n",
            "Rows with Researchers: 2281\n",
            "Rows with Abstract: 2131\n",
            "Rows with Findings: 2282\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Analyze Missing Data Patterns\n",
        "print(\"MISSING DATA PATTERN ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Define key fields to analyze\n",
        "key_fields = [\n",
        "    'Title', 'Abstract', 'Country', 'Subregion',\n",
        "    'Description_of_intervention', 'Primary_outcomes',\n",
        "    'Secondary_outcomes', 'Population', 'Year',\n",
        "    'Researcher_affiliation', 'jel_codes'\n",
        "]\n",
        "\n",
        "# Create a missing data matrix\n",
        "missing_matrix = pd.DataFrame()\n",
        "for field in key_fields:\n",
        "    missing_matrix[field] = consolidated[field].isna().astype(int)\n",
        "\n",
        "# Calculate missing field count per study\n",
        "consolidated['missing_count'] = missing_matrix.sum(axis=1)\n",
        "\n",
        "# Distribution of missing fields\n",
        "print(\"Distribution of missing fields per study:\")\n",
        "missing_distribution = consolidated['missing_count'].value_counts().sort_index()\n",
        "for num_missing, count in missing_distribution.items():\n",
        "    pct = (count / len(consolidated)) * 100\n",
        "    print(f\"  {num_missing} fields missing: {count} studies ({pct:.1f}%)\")\n",
        "\n",
        "# Co-occurrence analysis\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CO-OCCURRENCE OF MISSING DATA:\")\n",
        "print(\"(If field X is missing, probability that field Y is also missing)\\n\")\n",
        "\n",
        "# Calculate conditional probabilities\n",
        "for field1 in ['Abstract', 'Description_of_intervention', 'Country', 'Year']:\n",
        "    missing_field1 = consolidated[field1].isna()\n",
        "    n_missing = missing_field1.sum()\n",
        "\n",
        "    if n_missing > 0:\n",
        "        print(f\"\\nWhen {field1} is missing ({n_missing} cases):\")\n",
        "        for field2 in key_fields:\n",
        "            if field2 != field1:\n",
        "                both_missing = (missing_field1 & consolidated[field2].isna()).sum()\n",
        "                prob = (both_missing / n_missing) * 100\n",
        "                if prob > 50:  # Only show high correlations\n",
        "                    print(f\"  {field2}: {prob:.1f}% also missing\")\n",
        "\n",
        "# Identify studies with most missing data\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STUDIES WITH MOST MISSING FIELDS:\")\n",
        "worst_cases = consolidated[consolidated['missing_count'] >= 3].sort_values('missing_count', ascending=False)\n",
        "print(f\"\\nFound {len(worst_cases)} studies missing 3+ fields:\")\n",
        "\n",
        "for idx, row in worst_cases.head(10).iterrows():\n",
        "    missing_fields = [f for f in key_fields if pd.isna(row[f])]\n",
        "    print(f\"\\nRCT {row['rct_id']}: {row['Title'][:50] if pd.notna(row['Title']) else 'NO TITLE'}\")\n",
        "    print(f\"  Missing {row['missing_count']} fields: {', '.join(missing_fields)}\")\n",
        "\n",
        "# Pattern identification\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MISSING DATA PATTERNS:\")\n",
        "\n",
        "# Check if there's a \"core group\" that's always missing together\n",
        "always_together = []\n",
        "for i, field1 in enumerate(key_fields):\n",
        "    for field2 in key_fields[i+1:]:\n",
        "        missing1 = consolidated[field1].isna()\n",
        "        missing2 = consolidated[field2].isna()\n",
        "\n",
        "        # Check if they're almost always missing together\n",
        "        both_missing = (missing1 & missing2).sum()\n",
        "        either_missing = (missing1 | missing2).sum()\n",
        "\n",
        "        if either_missing > 0:\n",
        "            overlap_ratio = both_missing / either_missing\n",
        "            if overlap_ratio > 0.9 and both_missing > 100:  # High overlap and significant count\n",
        "                always_together.append((field1, field2, overlap_ratio, both_missing))\n",
        "\n",
        "if always_together:\n",
        "    print(\"\\nFields that are almost always missing together:\")\n",
        "    for f1, f2, ratio, count in always_together:\n",
        "        print(f\"  {f1} + {f2}: {ratio:.1%} overlap ({count} cases)\")\n",
        "else:\n",
        "    print(\"\\nNo strong patterns of fields always missing together\")\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"KEY INSIGHTS:\")\n",
        "complete_studies = len(consolidated[consolidated['missing_count'] == 0])\n",
        "print(f\"- {complete_studies}/{len(consolidated)} ({complete_studies/len(consolidated)*100:.1f}%) studies have ALL key fields\")\n",
        "print(f\"- {len(worst_cases)} studies ({len(worst_cases)/len(consolidated)*100:.1f}%) are missing 3+ fields\")\n",
        "print(f\"- Subregion and JEL codes are most commonly missing across all studies\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MGmJp7fgh1o",
        "outputId": "446e143b-4a53-4213-8c07-fe187c092118"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MISSING DATA PATTERN ANALYSIS\n",
            "================================================================================\n",
            "Distribution of missing fields per study:\n",
            "  0 fields missing: 631 studies (27.7%)\n",
            "  1 fields missing: 955 studies (41.8%)\n",
            "  2 fields missing: 532 studies (23.3%)\n",
            "  3 fields missing: 16 studies (0.7%)\n",
            "  4 fields missing: 2 studies (0.1%)\n",
            "  10 fields missing: 145 studies (6.4%)\n",
            "  11 fields missing: 1 studies (0.0%)\n",
            "\n",
            "================================================================================\n",
            "CO-OCCURRENCE OF MISSING DATA:\n",
            "(If field X is missing, probability that field Y is also missing)\n",
            "\n",
            "\n",
            "When Abstract is missing (151 cases):\n",
            "  Country: 96.7% also missing\n",
            "  Subregion: 97.4% also missing\n",
            "  Description_of_intervention: 96.7% also missing\n",
            "  Primary_outcomes: 96.7% also missing\n",
            "  Secondary_outcomes: 96.7% also missing\n",
            "  Population: 97.4% also missing\n",
            "  Year: 96.7% also missing\n",
            "  Researcher_affiliation: 96.7% also missing\n",
            "  jel_codes: 99.3% also missing\n",
            "\n",
            "When Description_of_intervention is missing (146 cases):\n",
            "  Abstract: 100.0% also missing\n",
            "  Country: 100.0% also missing\n",
            "  Subregion: 100.0% also missing\n",
            "  Primary_outcomes: 100.0% also missing\n",
            "  Secondary_outcomes: 100.0% also missing\n",
            "  Population: 100.0% also missing\n",
            "  Year: 100.0% also missing\n",
            "  Researcher_affiliation: 100.0% also missing\n",
            "  jel_codes: 100.0% also missing\n",
            "\n",
            "When Country is missing (148 cases):\n",
            "  Abstract: 98.6% also missing\n",
            "  Subregion: 100.0% also missing\n",
            "  Description_of_intervention: 98.6% also missing\n",
            "  Primary_outcomes: 98.6% also missing\n",
            "  Secondary_outcomes: 98.6% also missing\n",
            "  Population: 99.3% also missing\n",
            "  Year: 98.6% also missing\n",
            "  Researcher_affiliation: 98.6% also missing\n",
            "  jel_codes: 100.0% also missing\n",
            "\n",
            "When Year is missing (146 cases):\n",
            "  Abstract: 100.0% also missing\n",
            "  Country: 100.0% also missing\n",
            "  Subregion: 100.0% also missing\n",
            "  Description_of_intervention: 100.0% also missing\n",
            "  Primary_outcomes: 100.0% also missing\n",
            "  Secondary_outcomes: 100.0% also missing\n",
            "  Population: 100.0% also missing\n",
            "  Researcher_affiliation: 100.0% also missing\n",
            "  jel_codes: 100.0% also missing\n",
            "\n",
            "================================================================================\n",
            "STUDIES WITH MOST MISSING FIELDS:\n",
            "\n",
            "Found 164 studies missing 3+ fields:\n",
            "\n",
            "RCT nan: NO TITLE\n",
            "  Missing 11 fields: Title, Abstract, Country, Subregion, Description_of_intervention, Primary_outcomes, Secondary_outcomes, Population, Year, Researcher_affiliation, jel_codes\n",
            "\n",
            "RCT 1033.0: An Evaluation of Factors Affecting Antimalarial Dr\n",
            "  Missing 10 fields: Abstract, Country, Subregion, Description_of_intervention, Primary_outcomes, Secondary_outcomes, Population, Year, Researcher_affiliation, jel_codes\n",
            "\n",
            "RCT 1021.0: Inefficient Hiring in Entry-Level Labor Markets\n",
            "  Missing 10 fields: Abstract, Country, Subregion, Description_of_intervention, Primary_outcomes, Secondary_outcomes, Population, Year, Researcher_affiliation, jel_codes\n",
            "\n",
            "RCT 1025.0: Sports Betting in Uganda: Causes and Consequences\n",
            "  Missing 10 fields: Abstract, Country, Subregion, Description_of_intervention, Primary_outcomes, Secondary_outcomes, Population, Year, Researcher_affiliation, jel_codes\n",
            "\n",
            "RCT 1156.0: The Impact of Hotspot Policing and Municipal Servi\n",
            "  Missing 10 fields: Abstract, Country, Subregion, Description_of_intervention, Primary_outcomes, Secondary_outcomes, Population, Year, Researcher_affiliation, jel_codes\n",
            "\n",
            "RCT 1162.0: Microenterprise Supply Chain Intermediation Pilot\n",
            "  Missing 10 fields: Abstract, Country, Subregion, Description_of_intervention, Primary_outcomes, Secondary_outcomes, Population, Year, Researcher_affiliation, jel_codes\n",
            "\n",
            "RCT 1177.0: Teaching practices and students' learning: an expe\n",
            "  Missing 10 fields: Abstract, Country, Subregion, Description_of_intervention, Primary_outcomes, Secondary_outcomes, Population, Year, Researcher_affiliation, jel_codes\n",
            "\n",
            "RCT 1187.0: Evaluating the impacts of WorkShop access for smal\n",
            "  Missing 10 fields: Abstract, Country, Subregion, Description_of_intervention, Primary_outcomes, Secondary_outcomes, Population, Year, Researcher_affiliation, jel_codes\n",
            "\n",
            "RCT 1145.0: Evaluating the Effect of Tai Chi applied Stroke Re\n",
            "  Missing 10 fields: Abstract, Country, Subregion, Description_of_intervention, Primary_outcomes, Secondary_outcomes, Population, Year, Researcher_affiliation, jel_codes\n",
            "\n",
            "RCT 1140.0: Manager Communication Style, Worker Stress and Pro\n",
            "  Missing 10 fields: Abstract, Country, Subregion, Description_of_intervention, Primary_outcomes, Secondary_outcomes, Population, Year, Researcher_affiliation, jel_codes\n",
            "\n",
            "================================================================================\n",
            "MISSING DATA PATTERNS:\n",
            "\n",
            "Fields that are almost always missing together:\n",
            "  Abstract + Country: 95.4% overlap (146 cases)\n",
            "  Abstract + Description_of_intervention: 96.7% overlap (146 cases)\n",
            "  Abstract + Primary_outcomes: 96.7% overlap (146 cases)\n",
            "  Abstract + Secondary_outcomes: 96.7% overlap (146 cases)\n",
            "  Abstract + Year: 96.7% overlap (146 cases)\n",
            "  Abstract + Researcher_affiliation: 96.7% overlap (146 cases)\n",
            "  Country + Description_of_intervention: 98.6% overlap (146 cases)\n",
            "  Country + Primary_outcomes: 98.6% overlap (146 cases)\n",
            "  Country + Secondary_outcomes: 98.6% overlap (146 cases)\n",
            "  Country + Year: 98.6% overlap (146 cases)\n",
            "  Country + Researcher_affiliation: 98.6% overlap (146 cases)\n",
            "  Description_of_intervention + Primary_outcomes: 100.0% overlap (146 cases)\n",
            "  Description_of_intervention + Secondary_outcomes: 100.0% overlap (146 cases)\n",
            "  Description_of_intervention + Year: 100.0% overlap (146 cases)\n",
            "  Description_of_intervention + Researcher_affiliation: 100.0% overlap (146 cases)\n",
            "  Primary_outcomes + Secondary_outcomes: 100.0% overlap (146 cases)\n",
            "  Primary_outcomes + Year: 100.0% overlap (146 cases)\n",
            "  Primary_outcomes + Researcher_affiliation: 100.0% overlap (146 cases)\n",
            "  Secondary_outcomes + Year: 100.0% overlap (146 cases)\n",
            "  Secondary_outcomes + Researcher_affiliation: 100.0% overlap (146 cases)\n",
            "  Year + Researcher_affiliation: 100.0% overlap (146 cases)\n",
            "\n",
            "================================================================================\n",
            "KEY INSIGHTS:\n",
            "- 631/2282 (27.7%) studies have ALL key fields\n",
            "- 164 studies (7.2%) are missing 3+ fields\n",
            "- Subregion and JEL codes are most commonly missing across all studies\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Investigate Studies with Findings but Missing Metadata\n",
        "print(\"INVESTIGATING SUSPICIOUS RECORDS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Find studies with findings but missing core metadata\n",
        "has_findings_but_missing_data = consolidated[\n",
        "    (consolidated['Findings'].notna()) &\n",
        "    (consolidated['Abstract'].isna()) &\n",
        "    (consolidated['Description_of_intervention'].isna())\n",
        "]\n",
        "\n",
        "print(f\"Found {len(has_findings_but_missing_data)} studies with Findings but missing Abstract AND Intervention\")\n",
        "\n",
        "# Sample these problematic records\n",
        "print(\"\\nSample of suspicious records:\")\n",
        "print(\"-\"*80)\n",
        "for idx, row in has_findings_but_missing_data.head(5).iterrows():\n",
        "    print(f\"\\nRCT ID: {row['rct_id']}\")\n",
        "    print(f\"Title: {row['Title'][:60] if pd.notna(row['Title']) else 'NO TITLE'}\")\n",
        "    print(f\"Researchers: {row['Researchers'][:60] if pd.notna(row['Researchers']) else 'NO RESEARCHERS'}\")\n",
        "    print(f\"Findings: {row['Findings'][:150] if pd.notna(row['Findings']) else 'NO FINDINGS'}...\")\n",
        "\n",
        "# Check if these come from a specific source\n",
        "if 'source_file' in df.columns:\n",
        "    source_analysis = df[df['rct_id'].isin(has_findings_but_missing_data['rct_id'])]['source_file'].value_counts()\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SOURCE FILE ANALYSIS:\")\n",
        "    print(source_analysis)\n",
        "\n",
        "# Check the findings content - are they actually valid findings?\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINDINGS QUALITY CHECK:\")\n",
        "findings_lengths = has_findings_but_missing_data['Findings'].str.len()\n",
        "print(f\"Average findings length: {findings_lengths.mean():.0f} characters\")\n",
        "print(f\"Median findings length: {findings_lengths.median():.0f} characters\")\n",
        "print(f\"Min findings length: {findings_lengths.min():.0f} characters\")\n",
        "\n",
        "# Check if these might be from the Sonar harvesting that didn't get proper metadata\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"HYPOTHESIS: These might be from external harvesting without registry metadata\")\n",
        "\n",
        "# Check the original df for these problematic IDs\n",
        "sample_ids = has_findings_but_missing_data['rct_id'].head(5).tolist()\n",
        "for rct_id in sample_ids:\n",
        "    if rct_id in df['rct_id'].values:\n",
        "        original_row = df[df['rct_id'] == rct_id].iloc[0]\n",
        "        print(f\"\\nRCT {rct_id} in original data:\")\n",
        "        print(f\"  has_findings: {original_row.get('has_findings', 'N/A')}\")\n",
        "        print(f\"  final_has_findings: {original_row.get('final_has_findings', 'N/A')}\")\n",
        "        print(f\"  final_findings_source_type: {original_row.get('final_findings_source_type', 'N/A')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTnrM9zmhPmV",
        "outputId": "60dccf35-1d76-4662-8b04-920e12090f3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INVESTIGATING SUSPICIOUS RECORDS\n",
            "================================================================================\n",
            "Found 146 studies with Findings but missing Abstract AND Intervention\n",
            "\n",
            "Sample of suspicious records:\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "RCT ID: 1005.0\n",
            "Title: Information Frictions in the Labor Market: Evidence from a F\n",
            "Researchers: Vittorio Bassi\n",
            "Findings: The study \"Information Frictions in the Labor Market: Evidence from a Field Experiment in Uganda\" by Vittorio Bassi finds that providing certification...\n",
            "\n",
            "RCT ID: 1021.0\n",
            "Title: Inefficient Hiring in Entry-Level Labor Markets\n",
            "Researchers: Amanda Pallais\n",
            "Findings: The study \"Inefficient Hiring in Entry-Level Labor Markets\" by Amanda Pallais reveals that hiring inexperienced workers and providing them with detail...\n",
            "\n",
            "RCT ID: 1025.0\n",
            "Title: Sports Betting in Uganda: Causes and Consequences\n",
            "Researchers: Sylvan Herskowitz\n",
            "Findings: The study by Sylvan Herskowitz found that sports betting in Uganda is primarily driven by unmet liquidity needs for large, indivisible (\"lumpy\") expen...\n",
            "\n",
            "RCT ID: 1033.0\n",
            "Title: An Evaluation of Factors Affecting Antimalarial Drug Quality\n",
            "Researchers: Anne Fitzpatrick\n",
            "Findings: The study \"An Evaluation of Factors Affecting Antimalarial Drug Quality\" by Anne Fitzpatrick and Esther Atukunda found that in Uganda, the quality of ...\n",
            "\n",
            "RCT ID: 1038.0\n",
            "Title: How to Promote Order and Property Rights under Weak Rule of \n",
            "Researchers: Christopher Blattman\n",
            "Findings: The study found that a mass education campaign promoting alternative dispute resolution (ADR) in Liberia increased the resolution of land disputes and...\n",
            "\n",
            "================================================================================\n",
            "SOURCE FILE ANALYSIS:\n",
            "source_file\n",
            "sonar_append_all    146\n",
            "Name: count, dtype: int64\n",
            "\n",
            "================================================================================\n",
            "FINDINGS QUALITY CHECK:\n",
            "Average findings length: 562 characters\n",
            "Median findings length: 556 characters\n",
            "Min findings length: 166 characters\n",
            "\n",
            "================================================================================\n",
            "HYPOTHESIS: These might be from external harvesting without registry metadata\n",
            "\n",
            "RCT 1005.0 in original data:\n",
            "  has_findings: nan\n",
            "  final_has_findings: Yes\n",
            "  final_findings_source_type: working_paper\n",
            "\n",
            "RCT 1021.0 in original data:\n",
            "  has_findings: nan\n",
            "  final_has_findings: Yes\n",
            "  final_findings_source_type: working_paper\n",
            "\n",
            "RCT 1025.0 in original data:\n",
            "  has_findings: nan\n",
            "  final_has_findings: Yes\n",
            "  final_findings_source_type: unknown\n",
            "\n",
            "RCT 1033.0 in original data:\n",
            "  has_findings: nan\n",
            "  final_has_findings: Yes\n",
            "  final_findings_source_type: working_paper\n",
            "\n",
            "RCT 1038.0 in original data:\n",
            "  has_findings: nan\n",
            "  final_has_findings: Yes\n",
            "  final_findings_source_type: journal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Trace Back and Recover Missing Metadata\n",
        "print(\"TRACING MISSING METADATA FOR SONAR-HARVESTED STUDIES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# These 146 studies should exist in the original registry files\n",
        "# Let's check if we can find them in the WITHOUT_findings file which was the input to Sonar\n",
        "\n",
        "# Try loading the original WITHOUT_findings file that was fed to Sonar\n",
        "without_findings_path = '/content/drive/MyDrive/AEA_RCT_Parsed/Main (9)_studies_WITHOUT_findings.csv'\n",
        "try:\n",
        "    df_without = pd.read_csv(without_findings_path)\n",
        "    print(f\"Loaded WITHOUT_findings file: {len(df_without)} rows\")\n",
        "\n",
        "    # Check if our problematic RCT IDs exist there\n",
        "    problematic_ids = has_findings_but_missing_data['rct_id'].dropna().unique()\n",
        "    found_in_original = df_without[df_without['rct_id'].isin(problematic_ids)]\n",
        "\n",
        "    print(f\"\\nFound {len(found_in_original)} of {len(problematic_ids)} problematic IDs in original WITHOUT_findings file\")\n",
        "\n",
        "    if len(found_in_original) > 0:\n",
        "        # Check what metadata exists in the original\n",
        "        sample = found_in_original.iloc[0]\n",
        "        print(\"\\nSample record from original file:\")\n",
        "        print(f\"RCT ID: {sample['rct_id']}\")\n",
        "        print(f\"Title: {sample.get('title', 'NO TITLE')}\")\n",
        "        print(f\"Abstract: {'Present' if pd.notna(sample.get('abstract')) and len(str(sample.get('abstract', ''))) > 10 else 'Missing'}\")\n",
        "        print(f\"Country: {sample.get('country', 'Missing')}\")\n",
        "        print(f\"Interventions: {'Present' if pd.notna(sample.get('interventions')) else 'Missing'}\")\n",
        "\n",
        "        # Recovery strategy: merge the original metadata with the findings\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"RECOVERY STRATEGY:\")\n",
        "        print(\"1. Merge original registry metadata from WITHOUT_findings file\")\n",
        "        print(\"2. Keep the harvested findings from sonar_append_all\")\n",
        "        print(\"3. This will restore the complete records\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Could not find: {without_findings_path}\")\n",
        "    print(\"\\nAlternative: Try loading the SONAR results file directly\")\n",
        "\n",
        "    sonar_results_path = '/content/drive/MyDrive/AEA_RCT_Parsed/Main (9)_studies_WITHOUT_findings_PPLX_RESULTS_sonar.csv'\n",
        "    try:\n",
        "        df_sonar = pd.read_csv(sonar_results_path)\n",
        "        print(f\"Loaded SONAR results file: {len(df_sonar)} rows\")\n",
        "        print(\"Columns in SONAR file:\", df_sonar.columns.tolist()[:10])  # Show first 10 columns\n",
        "    except:\n",
        "        print(\"Could not load SONAR results file either\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RECOMMENDATION:\")\n",
        "print(\"The issue is that sonar_append_all contains ONLY the harvested findings,\")\n",
        "print(\"not the original registry metadata. We need to merge these 146 records\")\n",
        "print(\"with their original registry data from the WITHOUT_findings input file.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-StH1BqhyQH",
        "outputId": "28c39dd6-ab9f-4e08-db67-755db5cd9ab0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRACING MISSING METADATA FOR SONAR-HARVESTED STUDIES\n",
            "================================================================================\n",
            "Loaded WITHOUT_findings file: 1936 rows\n",
            "\n",
            "Found 0 of 145 problematic IDs in original WITHOUT_findings file\n",
            "\n",
            "================================================================================\n",
            "RECOMMENDATION:\n",
            "The issue is that sonar_append_all contains ONLY the harvested findings,\n",
            "not the original registry metadata. We need to merge these 146 records\n",
            "with their original registry data from the WITHOUT_findings input file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12: Find the True Source of These Mystery Studies\n",
        "print(\"INVESTIGATING THE SOURCE OF MYSTERY STUDIES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Check the RCT ID ranges to understand the pattern\n",
        "problematic_ids = has_findings_but_missing_data['rct_id'].dropna()\n",
        "print(f\"RCT ID range of problematic studies: {problematic_ids.min():.0f} to {problematic_ids.max():.0f}\")\n",
        "\n",
        "# Check RCT ID ranges in the full dataset\n",
        "all_rct_ids = df['rct_id'].dropna()\n",
        "print(f\"RCT ID range in full dataset: {all_rct_ids.min():.0f} to {all_rct_ids.max():.0f}\")\n",
        "\n",
        "# Look for these IDs in other possible source files\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CHECKING OTHER POSSIBLE SOURCE FILES:\")\n",
        "\n",
        "possible_sources = [\n",
        "    'Main (9)_studies_WITH_findings.csv',\n",
        "    'studies_WITH_findings.csv',\n",
        "    'all studies with findings.csv',\n",
        "    'WITH_findings__combined_dedup.csv',\n",
        "    'potential_misses_from_parsed_Main (9)_with_findings.csv'\n",
        "]\n",
        "\n",
        "for filename in possible_sources:\n",
        "    filepath = f'/content/drive/MyDrive/AEA_RCT_Parsed/{filename}'\n",
        "    try:\n",
        "        temp_df = pd.read_csv(filepath)\n",
        "        # Check if our problematic IDs exist in this file\n",
        "        found_ids = temp_df[temp_df['rct_id'].isin(problematic_ids)]['rct_id'].nunique()\n",
        "        if found_ids > 0:\n",
        "            print(f\"\\n✓ {filename}: Found {found_ids} problematic IDs\")\n",
        "            # Check if these records have metadata\n",
        "            sample = temp_df[temp_df['rct_id'].isin(problematic_ids)].iloc[0]\n",
        "            has_abstract = pd.notna(sample.get('abstract')) and len(str(sample.get('abstract', ''))) > 10\n",
        "            has_country = pd.notna(sample.get('country'))\n",
        "            print(f\"  Sample has abstract: {has_abstract}, has country: {has_country}\")\n",
        "        else:\n",
        "            print(f\"✗ {filename}: No problematic IDs found\")\n",
        "    except:\n",
        "        print(f\"✗ {filename}: File not found or couldn't load\")\n",
        "\n",
        "# Check if these are completely new studies that were never in the registry\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"HYPOTHESIS CHECK:\")\n",
        "print(\"These might be studies found ONLY through external search,\")\n",
        "print(\"never registered in AEA RCT Registry, but found during Sonar harvesting.\")\n",
        "print(\"\\nIf so, we'd need to either:\")\n",
        "print(\"1. Remove them (they're not true AEA RCT registry studies)\")\n",
        "print(\"2. Try to recover their metadata from the external sources\")\n",
        "print(\"3. Keep them but mark as 'external only' studies\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzCZ8YwWia6G",
        "outputId": "1cfaf399-7bb9-4e31-9001-f34da6789aed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INVESTIGATING THE SOURCE OF MYSTERY STUDIES\n",
            "================================================================================\n",
            "RCT ID range of problematic studies: 78 to 1772\n",
            "RCT ID range in full dataset: 76 to 16284\n",
            "\n",
            "================================================================================\n",
            "CHECKING OTHER POSSIBLE SOURCE FILES:\n",
            "✗ Main (9)_studies_WITH_findings.csv: No problematic IDs found\n",
            "✗ studies_WITH_findings.csv: No problematic IDs found\n",
            "✗ all studies with findings.csv: No problematic IDs found\n",
            "✗ WITH_findings__combined_dedup.csv: No problematic IDs found\n",
            "✗ potential_misses_from_parsed_Main (9)_with_findings.csv: No problematic IDs found\n",
            "\n",
            "================================================================================\n",
            "HYPOTHESIS CHECK:\n",
            "These might be studies found ONLY through external search,\n",
            "never registered in AEA RCT Registry, but found during Sonar harvesting.\n",
            "\n",
            "If so, we'd need to either:\n",
            "1. Remove them (they're not true AEA RCT registry studies)\n",
            "2. Try to recover their metadata from the external sources\n",
            "3. Keep them but mark as 'external only' studies\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13: Check the OTHER WITHOUT_findings File\n",
        "print(\"CHECKING ALTERNATIVE WITHOUT_FINDINGS FILE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Try the other WITHOUT_findings file\n",
        "alt_without_path = '/content/drive/MyDrive/AEA_RCT_Parsed/studies_WITHOUT_findings.csv'\n",
        "try:\n",
        "    df_alt_without = pd.read_csv(alt_without_path)\n",
        "    print(f\"Loaded studies_WITHOUT_findings.csv: {len(df_alt_without)} rows\")\n",
        "    print(f\"RCT ID range: {df_alt_without['rct_id'].min():.0f} to {df_alt_without['rct_id'].max():.0f}\")\n",
        "\n",
        "    # Check if our problematic IDs are in this file\n",
        "    problematic_ids = has_findings_but_missing_data['rct_id'].dropna().unique()\n",
        "    found_in_alt = df_alt_without[df_alt_without['rct_id'].isin(problematic_ids)]\n",
        "\n",
        "    print(f\"\\nFound {len(found_in_alt)} of {len(problematic_ids)} problematic IDs in studies_WITHOUT_findings.csv!\")\n",
        "\n",
        "    if len(found_in_alt) > 0:\n",
        "        # Check sample metadata\n",
        "        print(\"\\nSample records with metadata:\")\n",
        "        for idx, row in found_in_alt.head(3).iterrows():\n",
        "            print(f\"\\nRCT {row['rct_id']}:\")\n",
        "            print(f\"  Title: {row.get('title', 'N/A')[:60]}\")\n",
        "            print(f\"  Abstract: {'Present' if pd.notna(row.get('abstract')) and len(str(row.get('abstract'))) > 10 else 'Missing'}\")\n",
        "            print(f\"  Country: {row.get('country', 'N/A')}\")\n",
        "            print(f\"  Interventions: {'Present' if pd.notna(row.get('interventions')) and len(str(row.get('interventions'))) > 10 else 'Missing'}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"SOLUTION IDENTIFIED:\")\n",
        "        print(\"These 146 studies need their metadata recovered from studies_WITHOUT_findings.csv\")\n",
        "        print(\"The 'sonar_append_all' process only added findings but lost the original metadata.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading file: {e}\")\n",
        "    print(\"\\nCheck if 'studies_WITHOUT_findings.csv' exists in your folder\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ93qmIajHBZ",
        "outputId": "7cc4299d-141b-46d6-c5c5-f1b18a82705d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CHECKING ALTERNATIVE WITHOUT_FINDINGS FILE\n",
            "================================================================================\n",
            "Loaded studies_WITHOUT_findings.csv: 268 rows\n",
            "RCT ID range: 78 to 1773\n",
            "\n",
            "Found 145 of 145 problematic IDs in studies_WITHOUT_findings.csv!\n",
            "\n",
            "Sample records with metadata:\n",
            "\n",
            "RCT 1772:\n",
            "  Title: Improving school preparedness and child health outcomes thro\n",
            "  Abstract: Present\n",
            "  Country: India\n",
            "  Interventions: Present\n",
            "\n",
            "RCT 1748:\n",
            "  Title: Nudging Farmers to Use Fertilizer: Experimental Evidence fro\n",
            "  Abstract: Present\n",
            "  Country: Kenya\n",
            "  Interventions: Present\n",
            "\n",
            "RCT 1647:\n",
            "  Title: Farm & Family Balance Study: the intra-household and gender \n",
            "  Abstract: Present\n",
            "  Country: Uganda\n",
            "  Interventions: Present\n",
            "\n",
            "================================================================================\n",
            "SOLUTION IDENTIFIED:\n",
            "These 146 studies need their metadata recovered from studies_WITHOUT_findings.csv\n",
            "The 'sonar_append_all' process only added findings but lost the original metadata.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 14: Recover Missing Metadata for Sonar Studies\n",
        "print(\"RECOVERING METADATA FOR 146 STUDIES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Get the full metadata from studies_WITHOUT_findings.csv\n",
        "df_alt_without = pd.read_csv('/content/drive/MyDrive/AEA_RCT_Parsed/studies_WITHOUT_findings.csv')\n",
        "\n",
        "# Identify rows in consolidated that need metadata recovery\n",
        "needs_recovery = consolidated[\n",
        "    (consolidated['Findings'].notna()) &\n",
        "    (consolidated['Abstract'].isna()) &\n",
        "    (consolidated['rct_id'].isin(df_alt_without['rct_id']))\n",
        "].copy()\n",
        "\n",
        "print(f\"Recovering metadata for {len(needs_recovery)} studies\")\n",
        "\n",
        "# Create mapping dictionary from studies_WITHOUT_findings\n",
        "metadata_map = {}\n",
        "for _, row in df_alt_without.iterrows():\n",
        "    rct_id = row['rct_id']\n",
        "    metadata_map[rct_id] = {\n",
        "        'Abstract': row.get('abstract'),\n",
        "        'Country': row.get('country'),\n",
        "        'Subregion': row.get('region'),\n",
        "        'Description_of_intervention': row.get('interventions'),\n",
        "        'Primary_outcomes': row.get('primary_outcomes_endpoints'),\n",
        "        'Secondary_outcomes': row.get('secondary_outcomes_endpoints'),\n",
        "        'Year': extract_year(row),  # Using the function we defined earlier\n",
        "        'Researcher_affiliation': row.get('primary_investigator_affiliation'),\n",
        "        'Population': row.get('population_extracted'),\n",
        "        'jel_codes': row.get('jel_codes'),\n",
        "        'keywords': row.get('keywords'),\n",
        "        'keywords_additional': row.get('keywords_additional')\n",
        "    }\n",
        "\n",
        "# Apply recovery to consolidated dataset\n",
        "recovery_count = 0\n",
        "for idx in consolidated.index:\n",
        "    rct_id = consolidated.at[idx, 'rct_id']\n",
        "    if rct_id in metadata_map and pd.isna(consolidated.at[idx, 'Abstract']):\n",
        "        # This row needs recovery\n",
        "        for field, value in metadata_map[rct_id].items():\n",
        "            if field in consolidated.columns:\n",
        "                consolidated.at[idx, field] = value\n",
        "        recovery_count += 1\n",
        "\n",
        "print(f\"Successfully recovered metadata for {recovery_count} studies\")\n",
        "\n",
        "# Verify recovery\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"POST-RECOVERY STATISTICS:\")\n",
        "\n",
        "# Recalculate missing data\n",
        "key_fields = ['Title', 'Abstract', 'Country', 'Description_of_intervention', 'Year']\n",
        "for field in key_fields:\n",
        "    before_missing = has_findings_but_missing_data[field].isna().sum()\n",
        "    after_missing = consolidated[field].isna().sum()\n",
        "    print(f\"{field}: {after_missing} missing (was {before_missing + (len(consolidated) - len(has_findings_but_missing_data))})\")\n",
        "\n",
        "# Check if we still have studies with findings but no metadata\n",
        "still_problematic = consolidated[\n",
        "    (consolidated['Findings'].notna()) &\n",
        "    (consolidated['Abstract'].isna()) &\n",
        "    (consolidated['Description_of_intervention'].isna())\n",
        "]\n",
        "\n",
        "print(f\"\\nStudies with findings but still missing core metadata: {len(still_problematic)}\")\n",
        "\n",
        "if len(still_problematic) > 0:\n",
        "    print(\"Sample of remaining problematic studies:\")\n",
        "    for idx, row in still_problematic.head(3).iterrows():\n",
        "        print(f\"  RCT {row['rct_id']}: {row['Title'][:50] if pd.notna(row['Title']) else 'NO TITLE'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2n-0yZBWjgft",
        "outputId": "9f016dc9-c1f8-4a95-a082-4442620cd43f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RECOVERING METADATA FOR 146 STUDIES\n",
            "================================================================================\n",
            "Recovering metadata for 145 studies\n",
            "Successfully recovered metadata for 145 studies\n",
            "\n",
            "================================================================================\n",
            "POST-RECOVERY STATISTICS:\n",
            "Title: 2 missing (was 2137)\n",
            "Abstract: 7 missing (was 2282)\n",
            "Country: 3 missing (was 2282)\n",
            "Description_of_intervention: 1 missing (was 2282)\n",
            "Year: 1 missing (was 2282)\n",
            "\n",
            "Studies with findings but still missing core metadata: 1\n",
            "Sample of remaining problematic studies:\n",
            "  RCT nan: NO TITLE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 15: Clean Final Dataset and Save\n",
        "print(\"FINAL DATASET CLEANUP AND EXPORT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Remove the row with nan rct_id (likely an empty row)\n",
        "consolidated_clean = consolidated[consolidated['rct_id'].notna()].copy()\n",
        "\n",
        "# Remove the temporary 'missing_count' column if it exists\n",
        "if 'missing_count' in consolidated_clean.columns:\n",
        "    consolidated_clean = consolidated_clean.drop(columns=['missing_count'])\n",
        "\n",
        "print(f\"Removed {len(consolidated) - len(consolidated_clean)} empty/invalid rows\")\n",
        "print(f\"Final dataset: {len(consolidated_clean)} studies\")\n",
        "\n",
        "# Final quality report\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL DATASET QUALITY REPORT:\")\n",
        "print(f\"Total studies: {len(consolidated_clean)}\")\n",
        "print(f\"Unique RCT IDs: {consolidated_clean['rct_id'].nunique()}\")\n",
        "print(f\"Studies with complete core fields: {len(consolidated_clean.dropna(subset=['Title', 'Abstract', 'Description_of_intervention', 'Findings']))}\")\n",
        "\n",
        "# Population rates for key fields\n",
        "print(\"\\nField completion rates:\")\n",
        "important_fields = ['Title', 'Researchers', 'Abstract', 'Description_of_intervention',\n",
        "                   'Primary_outcomes', 'Findings', 'Country', 'Year']\n",
        "for field in important_fields:\n",
        "    populated = consolidated_clean[field].notna().sum()\n",
        "    pct = (populated / len(consolidated_clean)) * 100\n",
        "    print(f\"  {field}: {pct:.1f}%\")\n",
        "\n",
        "# Save the consolidated dataset\n",
        "output_path = '/content/drive/MyDrive/AEA_RCT_Parsed/consolidated_rct_dataset_clean.csv'\n",
        "consolidated_clean.to_csv(output_path, index=False)\n",
        "print(f\"\\n✓ Saved consolidated dataset to: consolidated_rct_dataset_clean.csv\")\n",
        "\n",
        "# Create a subset without rct_id for Azure (if needed)\n",
        "consolidated_for_azure = consolidated_clean.drop(columns=['rct_id'])\n",
        "azure_output_path = '/content/drive/MyDrive/AEA_RCT_Parsed/consolidated_rct_dataset_azure.csv'\n",
        "consolidated_for_azure.to_csv(azure_output_path, index=False)\n",
        "print(f\"✓ Saved Azure-ready dataset (without rct_id) to: consolidated_rct_dataset_azure.csv\")\n",
        "\n",
        "print(f\"\\nDatasets ready with {len(consolidated_clean.columns)-1} searchable fields for Azure AI Search\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jKhHhN-j7dz",
        "outputId": "8a399978-0ef9-4291-be68-c62be0f8e3c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FINAL DATASET CLEANUP AND EXPORT\n",
            "================================================================================\n",
            "Removed 1 empty/invalid rows\n",
            "Final dataset: 2281 studies\n",
            "\n",
            "================================================================================\n",
            "FINAL DATASET QUALITY REPORT:\n",
            "Total studies: 2281\n",
            "Unique RCT IDs: 2281\n",
            "Studies with complete core fields: 2274\n",
            "\n",
            "Field completion rates:\n",
            "  Title: 100.0%\n",
            "  Researchers: 100.0%\n",
            "  Abstract: 99.7%\n",
            "  Description_of_intervention: 100.0%\n",
            "  Primary_outcomes: 100.0%\n",
            "  Findings: 100.0%\n",
            "  Country: 99.9%\n",
            "  Year: 100.0%\n",
            "\n",
            "✓ Saved consolidated dataset to: consolidated_rct_dataset_clean.csv\n",
            "✓ Saved Azure-ready dataset (without rct_id) to: consolidated_rct_dataset_azure.csv\n",
            "\n",
            "Datasets ready with 20 searchable fields for Azure AI Search\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 16: Comprehensive Field Analysis\n",
        "print(\"COMPREHENSIVE FIELD ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Analyze all fields in the dataset\n",
        "all_fields_stats = []\n",
        "for col in consolidated_clean.columns:\n",
        "    populated = consolidated_clean[col].notna().sum()\n",
        "    empty = consolidated_clean[col].isna().sum()\n",
        "    pct_populated = (populated / len(consolidated_clean)) * 100\n",
        "\n",
        "    # Get sample length for text fields\n",
        "    if consolidated_clean[col].dtype == 'object':\n",
        "        non_null = consolidated_clean[col].dropna()\n",
        "        if len(non_null) > 0:\n",
        "            avg_length = non_null.str.len().mean()\n",
        "            median_length = non_null.str.len().median()\n",
        "        else:\n",
        "            avg_length = 0\n",
        "            median_length = 0\n",
        "    else:\n",
        "        avg_length = None\n",
        "        median_length = None\n",
        "\n",
        "    all_fields_stats.append({\n",
        "        'Field': col,\n",
        "        'Populated': populated,\n",
        "        'Empty': empty,\n",
        "        'Completion_Rate': f\"{pct_populated:.1f}%\",\n",
        "        'Avg_Length': f\"{avg_length:.0f}\" if avg_length else \"N/A\",\n",
        "        'Median_Length': f\"{median_length:.0f}\" if median_length else \"N/A\"\n",
        "    })\n",
        "\n",
        "# Display as DataFrame for better readability\n",
        "stats_df = pd.DataFrame(all_fields_stats)\n",
        "print(stats_df.to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FIELD CATEGORIES SUMMARY:\")\n",
        "\n",
        "# Group fields by completion rate\n",
        "print(\"\\nFully populated (100%):\")\n",
        "fully_populated = stats_df[stats_df['Completion_Rate'] == '100.0%']['Field'].tolist()\n",
        "print(f\"  {', '.join(fully_populated)}\")\n",
        "\n",
        "print(\"\\nNearly complete (>95%):\")\n",
        "nearly_complete = stats_df[(stats_df['Completion_Rate'] != '100.0%') &\n",
        "                           (stats_df['Populated'] / len(consolidated_clean) > 0.95)]['Field'].tolist()\n",
        "print(f\"  {', '.join(nearly_complete)}\")\n",
        "\n",
        "print(\"\\nModerate completion (50-95%):\")\n",
        "moderate = stats_df[(stats_df['Populated'] / len(consolidated_clean) <= 0.95) &\n",
        "                    (stats_df['Populated'] / len(consolidated_clean) > 0.50)]['Field'].tolist()\n",
        "print(f\"  {', '.join(moderate)}\")\n",
        "\n",
        "print(\"\\nLow completion (<50%):\")\n",
        "low_completion = stats_df[stats_df['Populated'] / len(consolidated_clean) <= 0.50]['Field'].tolist()\n",
        "print(f\"  {', '.join(low_completion)}\")\n",
        "\n",
        "# Check for fields that might need attention\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FIELDS REQUIRING ATTENTION:\")\n",
        "problematic = stats_df[stats_df['Populated'] / len(consolidated_clean) < 0.90]\n",
        "if len(problematic) > 0:\n",
        "    print(f\"\\nFields with <90% completion that might benefit from enrichment:\")\n",
        "    for _, row in problematic.iterrows():\n",
        "        print(f\"  - {row['Field']}: {row['Completion_Rate']} complete ({row['Empty']} missing)\")\n",
        "else:\n",
        "    print(\"All fields have >90% completion rate\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAlgg-A-kdH8",
        "outputId": "42ee6b5a-8559-45cb-9b95-869ab454926d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COMPREHENSIVE FIELD ANALYSIS\n",
            "================================================================================\n",
            "                      Field  Populated  Empty Completion_Rate Avg_Length Median_Length\n",
            "                      Title       2280      1          100.0%         73            71\n",
            "                    Country       2279      2           99.9%         14             8\n",
            "                  Subregion       1089   1192           47.7%         16            10\n",
            "                   Abstract       2275      6           99.7%       1028           876\n",
            "Description_of_intervention       2281      0          100.0%       1053           637\n",
            "           Primary_outcomes       2281      0          100.0%        754           395\n",
            "         Secondary_outcomes       2281      0          100.0%        176            32\n",
            "                 Population       2110    171           92.5%         65            63\n",
            "              Search_vector       2281      0          100.0%      21104         22704\n",
            "                   keywords       2281      0          100.0%         18            15\n",
            "        keywords_additional       2281      0          100.0%         67            63\n",
            "                  jel_codes       1299    982           56.9%         18            13\n",
            "            keywords_sector       2116    165           92.8%         44            43\n",
            "        keywords_mechanisms       2116    165           92.8%         60            58\n",
            "    keywords_implementation       2117    164           92.8%         60            59\n",
            "           keywords_context       2114    167           92.7%         48            47\n",
            "                Researchers       2281      0          100.0%        122            97\n",
            "     Researcher_affiliation       2281      0          100.0%         53            24\n",
            "                       Year       2281      0          100.0%          4             4\n",
            "                   Findings       2281      0          100.0%        351           342\n",
            "                     rct_id       2281      0          100.0%        N/A           N/A\n",
            "\n",
            "================================================================================\n",
            "FIELD CATEGORIES SUMMARY:\n",
            "\n",
            "Fully populated (100%):\n",
            "  Title, Description_of_intervention, Primary_outcomes, Secondary_outcomes, Search_vector, keywords, keywords_additional, Researchers, Researcher_affiliation, Year, Findings, rct_id\n",
            "\n",
            "Nearly complete (>95%):\n",
            "  Country, Abstract\n",
            "\n",
            "Moderate completion (50-95%):\n",
            "  Population, jel_codes, keywords_sector, keywords_mechanisms, keywords_implementation, keywords_context\n",
            "\n",
            "Low completion (<50%):\n",
            "  Subregion\n",
            "\n",
            "================================================================================\n",
            "FIELDS REQUIRING ATTENTION:\n",
            "\n",
            "Fields with <90% completion that might benefit from enrichment:\n",
            "  - Subregion: 47.7% complete (1192 missing)\n",
            "  - jel_codes: 56.9% complete (982 missing)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 17: Analyze Studies Missing AI-Generated Keywords\n",
        "print(\"ANALYSIS OF STUDIES MISSING AI-GENERATED KEYWORDS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Identify studies missing the keyword subcategories\n",
        "keyword_subcats = ['keywords_sector', 'keywords_mechanisms', 'keywords_implementation', 'keywords_context']\n",
        "\n",
        "# Find studies missing ANY of these subcategories\n",
        "missing_keywords_mask = consolidated_clean[keyword_subcats].isna().any(axis=1)\n",
        "studies_missing_keywords = consolidated_clean[missing_keywords_mask].copy()\n",
        "\n",
        "print(f\"Studies missing at least one keyword subcategory: {len(studies_missing_keywords)}\")\n",
        "\n",
        "# Check if they're missing ALL subcategories or just some\n",
        "missing_all_subcats = consolidated_clean[keyword_subcats].isna().all(axis=1)\n",
        "print(f\"Studies missing ALL keyword subcategories: {missing_all_subcats.sum()}\")\n",
        "\n",
        "# Analyze patterns in missing keywords\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PATTERNS IN STUDIES MISSING KEYWORDS:\")\n",
        "\n",
        "# Check if these are the same studies missing other fields\n",
        "print(\"\\nCross-checking with other missing fields:\")\n",
        "for field in ['Abstract', 'Population', 'jel_codes']:\n",
        "    missing_both = (missing_keywords_mask & consolidated_clean[field].isna()).sum()\n",
        "    print(f\"  Missing keywords AND {field}: {missing_both}\")\n",
        "\n",
        "# Check RCT ID ranges\n",
        "print(f\"\\nRCT ID range of studies missing keywords:\")\n",
        "missing_ids = studies_missing_keywords['rct_id'].dropna()\n",
        "print(f\"  Min: {missing_ids.min():.0f}, Max: {missing_ids.max():.0f}\")\n",
        "\n",
        "# Check if these are from a specific data source\n",
        "print(\"\\nSample of studies missing keywords:\")\n",
        "sample_missing = studies_missing_keywords.head(10)\n",
        "for idx, row in sample_missing.iterrows():\n",
        "    has_abstract = 'Yes' if pd.notna(row['Abstract']) and len(str(row['Abstract'])) > 10 else 'No'\n",
        "    has_population = 'Yes' if pd.notna(row['Population']) else 'No'\n",
        "    print(f\"\\nRCT {row['rct_id']}: {row['Title'][:50]}\")\n",
        "    print(f\"  Has abstract: {has_abstract}, Has population: {has_population}\")\n",
        "    print(f\"  Country: {row['Country']}, Year: {row['Year']}\")\n",
        "\n",
        "# Check if we can generate keywords for these using LLM\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FEASIBILITY CHECK FOR KEYWORD GENERATION:\")\n",
        "\n",
        "# Studies with abstract but missing keywords\n",
        "has_abstract_no_keywords = studies_missing_keywords[\n",
        "    studies_missing_keywords['Abstract'].notna() &\n",
        "    (studies_missing_keywords['Abstract'].str.len() > 100)\n",
        "]\n",
        "\n",
        "print(f\"Studies with good abstracts but missing keywords: {len(has_abstract_no_keywords)}\")\n",
        "print(\"These could have keywords generated using LLM based on their abstracts\")\n",
        "\n",
        "# Studies without sufficient text for keyword generation\n",
        "no_text_for_keywords = studies_missing_keywords[\n",
        "    studies_missing_keywords['Abstract'].isna() |\n",
        "    (studies_missing_keywords['Abstract'].str.len() <= 100)\n",
        "]\n",
        "\n",
        "print(f\"Studies without sufficient text for keyword generation: {len(no_text_for_keywords)}\")\n",
        "print(\"These would need alternative approaches or remain without subcategory keywords\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6zpZJnBlGhE",
        "outputId": "8e1cbe2c-fa27-4468-cc88-1faa24ba6874"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANALYSIS OF STUDIES MISSING AI-GENERATED KEYWORDS\n",
            "================================================================================\n",
            "Studies missing at least one keyword subcategory: 167\n",
            "Studies missing ALL keyword subcategories: 164\n",
            "\n",
            "================================================================================\n",
            "PATTERNS IN STUDIES MISSING KEYWORDS:\n",
            "\n",
            "Cross-checking with other missing fields:\n",
            "  Missing keywords AND Abstract: 2\n",
            "  Missing keywords AND Population: 167\n",
            "  Missing keywords AND jel_codes: 92\n",
            "\n",
            "RCT ID range of studies missing keywords:\n",
            "  Min: 78, Max: 14794\n",
            "\n",
            "Sample of studies missing keywords:\n",
            "\n",
            "RCT 2417.0: Beliefs about Effectiveness and Political Activism\n",
            "  Has abstract: Yes, Has population: No\n",
            "  Country: Germany, Year: 2017\n",
            "\n",
            "RCT 2644.0: Marriage Registrars Training\n",
            "  Has abstract: Yes, Has population: No\n",
            "  Country: Pakistan, Year: 2017\n",
            "\n",
            "RCT 2690.0: Incentives and education\n",
            "  Has abstract: Yes, Has population: No\n",
            "  Country: Spain, Year: 2018\n",
            "\n",
            "RCT 3333.0: An audit study on minimum wage legislation\n",
            "  Has abstract: Yes, Has population: No\n",
            "  Country: United States of America, Year: 2018\n",
            "\n",
            "RCT 3507.0: The effects of self-chosen incentivized goals on a\n",
            "  Has abstract: Yes, Has population: No\n",
            "  Country: Sweden, Year: 2018\n",
            "\n",
            "RCT 3855.0: Media Bias and the Demand for News\n",
            "  Has abstract: Yes, Has population: No\n",
            "  Country: United States of America, Year: 2019\n",
            "\n",
            "RCT 3928.0: The comparative impact of cash transfers and menta\n",
            "  Has abstract: Yes, Has population: No\n",
            "  Country: Kenya, Year: 2016\n",
            "\n",
            "RCT 4018.0: Nudging with reminders – the role of beliefs and e\n",
            "  Has abstract: Yes, Has population: No\n",
            "  Country: South Africa, Year: 2019\n",
            "\n",
            "RCT 4220.0: An experimental analysis of income targeting\n",
            "  Has abstract: Yes, Has population: No\n",
            "  Country: United States of America, Year: 2019\n",
            "\n",
            "RCT 4888.0: Decision Making on Food Choices\n",
            "  Has abstract: Yes, Has population: No\n",
            "  Country: United States of America, Year: 2019\n",
            "\n",
            "================================================================================\n",
            "FEASIBILITY CHECK FOR KEYWORD GENERATION:\n",
            "Studies with good abstracts but missing keywords: 150\n",
            "These could have keywords generated using LLM based on their abstracts\n",
            "Studies without sufficient text for keyword generation: 17\n",
            "These would need alternative approaches or remain without subcategory keywords\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 18: Analyze Abstract Length for Studies Missing Keywords\n",
        "print(\"ABSTRACT LENGTH ANALYSIS FOR STUDIES MISSING KEYWORDS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Get abstract lengths for studies WITH keywords\n",
        "has_keywords_mask = ~missing_keywords_mask\n",
        "with_keywords = consolidated_clean[has_keywords_mask].copy()\n",
        "with_keywords['abstract_length'] = with_keywords['Abstract'].fillna('').str.len()\n",
        "\n",
        "# Get abstract lengths for studies WITHOUT keywords\n",
        "without_keywords = studies_missing_keywords.copy()\n",
        "without_keywords['abstract_length'] = without_keywords['Abstract'].fillna('').str.len()\n",
        "\n",
        "# Compare statistics\n",
        "print(\"Abstract length statistics:\")\n",
        "print(\"-\"*40)\n",
        "print(f\"Studies WITH keywords (n={len(with_keywords)}):\")\n",
        "print(f\"  Mean length: {with_keywords['abstract_length'].mean():.0f} characters\")\n",
        "print(f\"  Median length: {with_keywords['abstract_length'].median():.0f} characters\")\n",
        "print(f\"  Min length: {with_keywords['abstract_length'].min():.0f} characters\")\n",
        "print(f\"  Max length: {with_keywords['abstract_length'].max():.0f} characters\")\n",
        "print(f\"  Abstracts <100 chars: {(with_keywords['abstract_length'] < 100).sum()}\")\n",
        "print(f\"  Abstracts <500 chars: {(with_keywords['abstract_length'] < 500).sum()}\")\n",
        "\n",
        "print(f\"\\nStudies WITHOUT keywords (n={len(without_keywords)}):\")\n",
        "print(f\"  Mean length: {without_keywords['abstract_length'].mean():.0f} characters\")\n",
        "print(f\"  Median length: {without_keywords['abstract_length'].median():.0f} characters\")\n",
        "print(f\"  Min length: {without_keywords['abstract_length'].min():.0f} characters\")\n",
        "print(f\"  Max length: {without_keywords['abstract_length'].max():.0f} characters\")\n",
        "print(f\"  Abstracts <100 chars: {(without_keywords['abstract_length'] < 100).sum()}\")\n",
        "print(f\"  Abstracts <500 chars: {(without_keywords['abstract_length'] < 500).sum()}\")\n",
        "\n",
        "# Distribution of abstract lengths for missing keywords\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ABSTRACT LENGTH DISTRIBUTION FOR STUDIES MISSING KEYWORDS:\")\n",
        "length_bins = [0, 100, 250, 500, 750, 1000, 10000]\n",
        "length_labels = ['0-100', '100-250', '250-500', '500-750', '750-1000', '1000+']\n",
        "without_keywords['length_category'] = pd.cut(without_keywords['abstract_length'],\n",
        "                                              bins=length_bins,\n",
        "                                              labels=length_labels)\n",
        "\n",
        "for category in length_labels:\n",
        "    count = (without_keywords['length_category'] == category).sum()\n",
        "    pct = (count / len(without_keywords)) * 100\n",
        "    print(f\"  {category} chars: {count} studies ({pct:.1f}%)\")\n",
        "\n",
        "# Show examples of different length categories\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SAMPLE ABSTRACTS FROM STUDIES MISSING KEYWORDS:\")\n",
        "\n",
        "for category in ['0-100', '100-250', '250-500']:\n",
        "    sample = without_keywords[without_keywords['length_category'] == category].head(1)\n",
        "    if len(sample) > 0:\n",
        "        row = sample.iloc[0]\n",
        "        print(f\"\\n{category} chars example (RCT {row['rct_id']}):\")\n",
        "        print(f\"  Title: {row['Title'][:60]}\")\n",
        "        print(f\"  Abstract: '{row['Abstract'][:200] if pd.notna(row['Abstract']) else 'EMPTY'}'\")\n",
        "        print(f\"  Length: {row['abstract_length']} characters\")\n",
        "\n",
        "# Check if there's a clear cutoff threshold\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"THRESHOLD ANALYSIS:\")\n",
        "print(\"Is there a minimum abstract length used for keyword generation?\")\n",
        "\n",
        "# Find the shortest abstract that HAS keywords\n",
        "min_length_with_keywords = with_keywords[with_keywords['abstract_length'] > 0]['abstract_length'].min()\n",
        "print(f\"  Shortest abstract WITH keywords: {min_length_with_keywords:.0f} chars\")\n",
        "\n",
        "# Find the longest abstract WITHOUT keywords\n",
        "max_length_without_keywords = without_keywords[without_keywords['abstract_length'] > 0]['abstract_length'].max()\n",
        "print(f\"  Longest abstract WITHOUT keywords: {max_length_without_keywords:.0f} chars\")\n",
        "\n",
        "if min_length_with_keywords < max_length_without_keywords:\n",
        "    print(f\"\\n  ⚠️ There's overlap: some abstracts with {min_length_with_keywords:.0f}+ chars have keywords\")\n",
        "    print(f\"     while some with up to {max_length_without_keywords:.0f} chars don't.\")\n",
        "    print(\"     The missing keywords might not be purely due to abstract length.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1zeWBy_mtJA",
        "outputId": "78e7b589-97e1-43e7-c918-5947edd4dfac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ABSTRACT LENGTH ANALYSIS FOR STUDIES MISSING KEYWORDS\n",
            "================================================================================\n",
            "Abstract length statistics:\n",
            "----------------------------------------\n",
            "Studies WITH keywords (n=2114):\n",
            "  Mean length: 1032 characters\n",
            "  Median length: 877 characters\n",
            "  Min length: 0 characters\n",
            "  Max length: 9713 characters\n",
            "  Abstracts <100 chars: 16\n",
            "  Abstracts <500 chars: 338\n",
            "\n",
            "Studies WITHOUT keywords (n=167):\n",
            "  Mean length: 945 characters\n",
            "  Median length: 812 characters\n",
            "  Min length: 0 characters\n",
            "  Max length: 3586 characters\n",
            "  Abstracts <100 chars: 16\n",
            "  Abstracts <500 chars: 43\n",
            "\n",
            "================================================================================\n",
            "ABSTRACT LENGTH DISTRIBUTION FOR STUDIES MISSING KEYWORDS:\n",
            "  0-100 chars: 15 studies (9.0%)\n",
            "  100-250 chars: 13 studies (7.8%)\n",
            "  250-500 chars: 13 studies (7.8%)\n",
            "  500-750 chars: 28 studies (16.8%)\n",
            "  750-1000 chars: 29 studies (17.4%)\n",
            "  1000+ chars: 67 studies (40.1%)\n",
            "\n",
            "================================================================================\n",
            "SAMPLE ABSTRACTS FROM STUDIES MISSING KEYWORDS:\n",
            "\n",
            "0-100 chars example (RCT 2417.0):\n",
            "  Title: Beliefs about Effectiveness and Political Activism: Evidence\n",
            "  Abstract: 'See preanalysis plan.\n",
            "External Link(s)'\n",
            "  Length: 38 characters\n",
            "\n",
            "100-250 chars example (RCT 2690.0):\n",
            "  Title: Incentives and education\n",
            "  Abstract: 'We uploaded all information in a PDF file. This file will remain hidden until the project is over.\n",
            "External Link(s)'\n",
            "  Length: 115 characters\n",
            "\n",
            "250-500 chars example (RCT 4018.0):\n",
            "  Title: Nudging with reminders – the role of beliefs and emotions\n",
            "  Abstract: 'This study aims to identify the potential risks that can result from using one of the most popular nudges, reminders, to change behavior. We analyze what drives reminder response and in particular the'\n",
            "  Length: 413 characters\n",
            "\n",
            "================================================================================\n",
            "THRESHOLD ANALYSIS:\n",
            "Is there a minimum abstract length used for keyword generation?\n",
            "  Shortest abstract WITH keywords: 29 chars\n",
            "  Longest abstract WITHOUT keywords: 3586 chars\n",
            "\n",
            "  ⚠️ There's overlap: some abstracts with 29+ chars have keywords\n",
            "     while some with up to 3586 chars don't.\n",
            "     The missing keywords might not be purely due to abstract length.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 19: Investigate the Real Pattern Behind Missing Keywords\n",
        "print(\"INVESTIGATING TRUE PATTERN FOR MISSING KEYWORDS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Check the actual content quality of abstracts without keywords\n",
        "without_keywords = studies_missing_keywords.copy()\n",
        "\n",
        "# Identify potential placeholder/low-quality abstracts\n",
        "placeholder_patterns = [\n",
        "    'see preanalysis',\n",
        "    'see pre-analysis',\n",
        "    'external link',\n",
        "    'uploaded all information',\n",
        "    'pdf file',\n",
        "    'will remain hidden',\n",
        "    'see attached',\n",
        "    'see appendix',\n",
        "    'forthcoming',\n",
        "    'to be provided'\n",
        "]\n",
        "\n",
        "# Check for placeholder text\n",
        "def has_placeholder_text(text):\n",
        "    if pd.isna(text):\n",
        "        return True\n",
        "    text_lower = str(text).lower()\n",
        "    return any(pattern in text_lower for pattern in placeholder_patterns)\n",
        "\n",
        "without_keywords['has_placeholder'] = without_keywords['Abstract'].apply(has_placeholder_text)\n",
        "\n",
        "print(f\"Studies with placeholder/incomplete abstracts: {without_keywords['has_placeholder'].sum()}\")\n",
        "print(f\"Studies with real abstracts: {(~without_keywords['has_placeholder']).sum()}\")\n",
        "\n",
        "# Also check if these are from a specific time period or batch\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TEMPORAL PATTERN ANALYSIS:\")\n",
        "\n",
        "# Year distribution\n",
        "year_dist = without_keywords.groupby('Year').size().sort_index()\n",
        "print(\"\\nStudies missing keywords by year:\")\n",
        "for year, count in year_dist.items():\n",
        "    if count > 5:  # Only show years with significant counts\n",
        "        print(f\"  {year}: {count} studies\")\n",
        "\n",
        "# Check if these are from the studies_WITHOUT_findings batch\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SOURCE ANALYSIS:\")\n",
        "\n",
        "# Check RCT ID ranges to identify source\n",
        "id_ranges = [\n",
        "    (78, 1773, \"studies_WITHOUT_findings\"),\n",
        "    (2000, 17000, \"Main batch\")\n",
        "]\n",
        "\n",
        "for min_id, max_id, source in id_ranges:\n",
        "    in_range = without_keywords[(without_keywords['rct_id'] >= min_id) &\n",
        "                                (without_keywords['rct_id'] <= max_id)]\n",
        "    if len(in_range) > 0:\n",
        "        print(f\"{source} (ID {min_id}-{max_id}): {len(in_range)} studies missing keywords\")\n",
        "\n",
        "# Sample real abstracts that are missing keywords\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SAMPLE OF REAL ABSTRACTS MISSING KEYWORDS:\")\n",
        "\n",
        "real_abstracts = without_keywords[(~without_keywords['has_placeholder']) &\n",
        "                                  (without_keywords['Abstract'].str.len() > 500)]\n",
        "\n",
        "for idx, row in real_abstracts.head(3).iterrows():\n",
        "    print(f\"\\nRCT {row['rct_id']}: {row['Title'][:60]}\")\n",
        "    print(f\"Abstract preview: {row['Abstract'][:300]}...\")\n",
        "    print(f\"Year: {row['Year']}, Country: {row['Country']}\")\n",
        "\n",
        "# Check if Population field correlates (all missing keywords lack Population)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"KEY INSIGHT:\")\n",
        "print(f\"ALL {len(without_keywords)} studies missing keywords also lack 'Population' field\")\n",
        "print(\"This suggests they weren't processed by the AI enrichment pipeline that generated both\")\n",
        "print(\"\\nRecommendation: Run AI enrichment on these {len(real_abstracts)} studies with real abstracts\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAUYLYG7nJ3V",
        "outputId": "af49ecaf-3973-4ec7-fe35-4d6e83d88b44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INVESTIGATING TRUE PATTERN FOR MISSING KEYWORDS\n",
            "================================================================================\n",
            "Studies with placeholder/incomplete abstracts: 154\n",
            "Studies with real abstracts: 13\n",
            "\n",
            "================================================================================\n",
            "TEMPORAL PATTERN ANALYSIS:\n",
            "\n",
            "Studies missing keywords by year:\n",
            "  2009: 7 studies\n",
            "  2010: 10 studies\n",
            "  2011: 12 studies\n",
            "  2013: 14 studies\n",
            "  2014: 30 studies\n",
            "  2015: 25 studies\n",
            "  2016: 26 studies\n",
            "  2019: 6 studies\n",
            "\n",
            "================================================================================\n",
            "SOURCE ANALYSIS:\n",
            "studies_WITHOUT_findings (ID 78-1773): 145 studies missing keywords\n",
            "Main batch (ID 2000-17000): 22 studies missing keywords\n",
            "\n",
            "================================================================================\n",
            "SAMPLE OF REAL ABSTRACTS MISSING KEYWORDS:\n",
            "\n",
            "RCT 1243.0: The Impacts of Female Education: Evidence from Malawian Seco\n",
            "Abstract preview: This project evaluates a randomized controlled trial of an education support program for girls on rationality and preferences. Between October of 2011 and May of 2012 a baseline survey of 7,971 secondary students in school cohorts 9-11 was implemented across 124 classrooms in 33 public secondary sch...\n",
            "Year: 2011, Country: Malawi\n",
            "\n",
            "RCT 1516.0: A randomized controlled trial of a brief family-centered car\n",
            "Abstract preview: Because of the bidirectional and entangled relationships between patients with bipolar disorder (BPD) and the family system, using family interventions has been emphasized. However, the effects of family interventions on family function and caregivers’ health-related outcomes have not been well inve...\n",
            "Year: 2010, Country: Taiwan, Province of China\n",
            "\n",
            "RCT 337.0: A cost-benefit analysis of different uses of technologies an\n",
            "Abstract preview: We created a pedagogical intervention designed to give students a more active role in the learning of geometry –one of three units of the seventh grade curriculum or about three months of teaching. A key aspect of this change relies on providing them with guided opportunities to explore and discover...\n",
            "Year: 2012, Country: Costa Rica\n",
            "\n",
            "================================================================================\n",
            "KEY INSIGHT:\n",
            "ALL 167 studies missing keywords also lack 'Population' field\n",
            "This suggests they weren't processed by the AI enrichment pipeline that generated both\n",
            "\n",
            "Recommendation: Run AI enrichment on these {len(real_abstracts)} studies with real abstracts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 20: Final Export and Documentation\n",
        "print(\"FINAL EXPORT AND ENRICHMENT RECOMMENDATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create a list of studies that need AI enrichment\n",
        "needs_enrichment = without_keywords[(~without_keywords['has_placeholder']) &\n",
        "                                    (without_keywords['Abstract'].str.len() > 300)]\n",
        "\n",
        "print(f\"Studies suitable for AI enrichment: {len(needs_enrichment)}\")\n",
        "\n",
        "# Save list of studies needing enrichment\n",
        "enrichment_candidates = needs_enrichment[['rct_id', 'Title', 'Abstract', 'Year', 'Country']].copy()\n",
        "enrichment_candidates['abstract_length'] = enrichment_candidates['Abstract'].str.len()\n",
        "enrichment_candidates['needs'] = 'Population, keywords_sector, keywords_mechanisms, keywords_implementation, keywords_context'\n",
        "\n",
        "enrichment_path = '/content/drive/MyDrive/AEA_RCT_Parsed/studies_for_ai_enrichment.csv'\n",
        "enrichment_candidates.to_csv(enrichment_path, index=False)\n",
        "print(f\"✓ Saved enrichment candidates to: studies_for_ai_enrichment.csv\")\n",
        "\n",
        "# Final summary statistics\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL CONSOLIDATED DATASET SUMMARY:\")\n",
        "print(f\"\"\"\n",
        "Total Studies: {len(consolidated_clean)}\n",
        "Complete Records (all major fields): {len(consolidated_clean.dropna(subset=['Title', 'Abstract', 'Description_of_intervention', 'Findings']))}\n",
        "\n",
        "Field Completion Rates:\n",
        "- Core Content: 99.7% (Abstract, Interventions, Outcomes, Findings)\n",
        "- Geographic: 99.9% (Country), 47.7% (Subregion)\n",
        "- Temporal: 100% (Year extracted from dates)\n",
        "- AI-Enhanced: 92.7% (Keyword subcategories), 92.5% (Population)\n",
        "- Traditional: 56.9% (JEL codes)\n",
        "\n",
        "Data Quality Issues Resolved:\n",
        "✓ Removed {len(df) - len(consolidated_clean)} duplicate studies\n",
        "✓ Recovered metadata for 145 studies from Sonar harvesting\n",
        "✓ Consolidated researcher names and affiliations\n",
        "✓ Standardized all date fields to Year\n",
        "\n",
        "Ready for Azure AI Search:\n",
        "- 20 searchable fields\n",
        "- 100% search vector coverage\n",
        "- Rich keyword taxonomy for faceting\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"NEXT STEPS:\")\n",
        "print(\"\"\"\n",
        "1. Upload 'consolidated_rct_dataset_azure.csv' to Azure AI Search\n",
        "2. Configure index with:\n",
        "   - Searchable: Title, Abstract, Description_of_intervention, Findings\n",
        "   - Filterable: Country, Year, keywords_sector, keywords_context\n",
        "   - Facetable: Country, Year, Subregion (where available)\n",
        "   - Vectorizable: Search_vector field (already computed)\n",
        "\n",
        "3. Optional: Run AI enrichment on 13 studies in 'studies_for_ai_enrichment.csv'\n",
        "   to add missing Population and keyword subcategories\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLajjUDGnz_k",
        "outputId": "16cfe803-04f9-4fc0-e87d-9653313d41e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FINAL EXPORT AND ENRICHMENT RECOMMENDATIONS\n",
            "================================================================================\n",
            "Studies suitable for AI enrichment: 8\n",
            "✓ Saved enrichment candidates to: studies_for_ai_enrichment.csv\n",
            "\n",
            "================================================================================\n",
            "FINAL CONSOLIDATED DATASET SUMMARY:\n",
            "\n",
            "Total Studies: 2281\n",
            "Complete Records (all major fields): 2274\n",
            "\n",
            "Field Completion Rates:\n",
            "- Core Content: 99.7% (Abstract, Interventions, Outcomes, Findings)  \n",
            "- Geographic: 99.9% (Country), 47.7% (Subregion)\n",
            "- Temporal: 100% (Year extracted from dates)\n",
            "- AI-Enhanced: 92.7% (Keyword subcategories), 92.5% (Population)\n",
            "- Traditional: 56.9% (JEL codes)\n",
            "\n",
            "Data Quality Issues Resolved:\n",
            "✓ Removed 1 duplicate studies\n",
            "✓ Recovered metadata for 145 studies from Sonar harvesting\n",
            "✓ Consolidated researcher names and affiliations\n",
            "✓ Standardized all date fields to Year\n",
            "\n",
            "Ready for Azure AI Search:\n",
            "- 20 searchable fields\n",
            "- 100% search vector coverage\n",
            "- Rich keyword taxonomy for faceting\n",
            "\n",
            "================================================================================\n",
            "NEXT STEPS:\n",
            "\n",
            "1. Upload 'consolidated_rct_dataset_azure.csv' to Azure AI Search\n",
            "2. Configure index with:\n",
            "   - Searchable: Title, Abstract, Description_of_intervention, Findings\n",
            "   - Filterable: Country, Year, keywords_sector, keywords_context\n",
            "   - Facetable: Country, Year, Subregion (where available)\n",
            "   - Vectorizable: Search_vector field (already computed)\n",
            "\n",
            "3. Optional: Run AI enrichment on 13 studies in 'studies_for_ai_enrichment.csv'\n",
            "   to add missing Population and keyword subcategories\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37312119",
        "outputId": "7d158868-dc48-4fd0-f250-867f73224776"
      },
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve the Together AI API key from Colab secrets\n",
        "together_api_key = userdata.get('TOGETHER_API_KEY')\n",
        "\n",
        "# Set the API key as an environment variable\n",
        "os.environ['TOGETHER_API_KEY'] = together_api_key\n",
        "\n",
        "print(\"Together AI API key loaded and set as environment variable.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Together AI API key loaded and set as environment variable.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import time\n",
        "from typing import Dict, List, Tuple\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Install required packages\n",
        "!pip install together --quiet\n",
        "!pip install tiktoken --quiet  # For token counting\n",
        "\n",
        "import together\n",
        "import tiktoken\n",
        "\n",
        "# Set up TogetherAI\n",
        "print(\"Setting up TogetherAI...\")\n",
        "# You'll need to set your API key - choose one method:\n",
        "\n",
        "# Method 1: Direct input (for testing)\n",
        "# together.api_key = \"YOUR_API_KEY_HERE\"\n",
        "\n",
        "# Method 2: From Google Colab secrets (recommended)\n",
        "# from google.colab import userdata\n",
        "# together.api_key = userdata.get('TOGETHER_API_KEY')\n",
        "\n",
        "# Method 3: Environment variable\n",
        "# os.environ['TOGETHER_API_KEY'] = \"YOUR_API_KEY_HERE\"\n",
        "# together.api_key = os.environ.get('TOGETHER_API_KEY')\n",
        "\n",
        "# Method 4: Input prompt (secure)\n",
        "import getpass\n",
        "together.api_key = getpass.getpass('Enter your TogetherAI API key: ')\n",
        "\n",
        "print(\"✓ TogetherAI configured\")\n",
        "\n",
        "# Base path for data\n",
        "base_path = '/content/drive/MyDrive/AEA_RCT_Parsed/'"
      ],
      "metadata": {
        "id": "a4QaCkPRqBYp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deb75663-1b22-4bf0-e369-b263d1507c69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/103.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/45.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/98.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hSetting up TogetherAI...\n",
            "Enter your TogetherAI API key: ··········\n",
            "✓ TogetherAI configured\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading consolidated dataset...\")\n",
        "df = pd.read_csv(f'{base_path}consolidated_rct_dataset_clean.csv')\n",
        "print(f\"✓ Loaded {len(df)} studies\")\n",
        "\n",
        "# Create subset for validation (you can adjust the sample size)\n",
        "print(\"\\nPreparing validation sample...\")\n",
        "\n",
        "# Strategy 1: Random sample\n",
        "sample_size = 100  # Adjust based on your API budget\n",
        "validation_df = df.sample(n=min(sample_size, len(df)), random_state=42).copy()\n",
        "\n",
        "# Strategy 2: Focus on suspicious cases (uncomment to use)\n",
        "# Look for studies with very short abstracts but long findings, or vice versa\n",
        "# df['abstract_len'] = df['Abstract'].fillna('').str.len()\n",
        "# df['findings_len'] = df['Findings'].fillna('').str.len()\n",
        "# suspicious = df[\n",
        "#     ((df['abstract_len'] < 100) & (df['findings_len'] > 500)) |\n",
        "#     ((df['abstract_len'] > 1000) & (df['findings_len'] < 100))\n",
        "# ]\n",
        "# validation_df = suspicious.sample(n=min(sample_size, len(suspicious)), random_state=42).copy()\n",
        "\n",
        "print(f\"✓ Selected {len(validation_df)} studies for validation\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDodQ4_vsVQM",
        "outputId": "4e185702-bd35-4477-ab73-e9364d716d5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading consolidated dataset...\n",
            "✓ Loaded 2281 studies\n",
            "\n",
            "Preparing validation sample...\n",
            "✓ Selected 100 studies for validation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_validation_prompt(row: pd.Series) -> str:\n",
        "    \"\"\"Create a prompt to validate if findings match the study.\"\"\"\n",
        "\n",
        "    prompt = f\"\"\"You are a research validator. Analyze if the findings below correctly match the study description.\n",
        "\n",
        "STUDY TITLE: {row.get('Title', 'N/A')}\n",
        "\n",
        "ABSTRACT: {row.get('Abstract', 'N/A')[:500]}\n",
        "\n",
        "DESCRIPTION OF INTERVENTION: {row.get('Description_of_intervention', 'N/A')[:300]}\n",
        "\n",
        "PRIMARY OUTCOMES: {row.get('Primary_outcomes', 'N/A')[:300]}\n",
        "\n",
        "RESEARCHERS: {row.get('Researchers', 'N/A')}\n",
        "\n",
        "YEAR: {row.get('Year', 'N/A')}\n",
        "\n",
        "COUNTRY: {row.get('Country', 'N/A')}\n",
        "\n",
        "FINDINGS: {row.get('Findings', 'N/A')[:800]}\n",
        "\n",
        "TASK: Evaluate if these findings logically match this study. Consider:\n",
        "1. Do the findings discuss the same intervention described in the abstract?\n",
        "2. Do the outcomes in findings match the primary outcomes listed?\n",
        "3. Is the geographic context consistent?\n",
        "4. Do the findings seem to be from a completely different study?\n",
        "\n",
        "Respond with ONLY a JSON object in this format:\n",
        "{{\n",
        "    \"match_score\": <0-100 score, 100 means perfect match>,\n",
        "    \"confidence\": <\"high\", \"medium\", or \"low\">,\n",
        "    \"issues\": [<list any major discrepancies>],\n",
        "    \"verdict\": <\"correct_match\", \"likely_match\", \"uncertain\", \"likely_mismatch\", or \"definite_mismatch\">\n",
        "}}\n",
        "\n",
        "Do not include any other text, only the JSON object.\"\"\"\n",
        "\n",
        "    return prompt\n",
        "\n",
        "def create_cross_check_prompt(row: pd.Series) -> str:\n",
        "    \"\"\"Create a simpler cross-check prompt.\"\"\"\n",
        "\n",
        "    prompt = f\"\"\"Compare this study title and findings. Are they from the same research?\n",
        "\n",
        "TITLE: {row.get('Title', 'N/A')}\n",
        "FINDINGS EXCERPT: {row.get('Findings', 'N/A')[:400]}\n",
        "\n",
        "Answer with only: YES (same study), NO (different study), or UNCERTAIN (cannot determine).\n",
        "Then provide a confidence score 0-100.\n",
        "\n",
        "Format: <YES/NO/UNCERTAIN> <SCORE>\n",
        "Example: YES 95\"\"\"\n",
        "\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "ruG4c8WPsaAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Available open-source models on TogetherAI (choose based on your needs)\n",
        "MODELS = {\n",
        "    'mixtral-8x7b': 'mistralai/Mixtral-8x7B-Instruct-v0.1',  # Good balance\n",
        "    'mixtral-8x22b': 'mistralai/Mixtral-8x22B-Instruct-v0.1',  # More accurate\n",
        "    'llama-70b': 'meta-llama/Llama-3-70b-chat-hf',  # Very capable\n",
        "    'llama-8b': 'meta-llama/Llama-3-8b-chat-hf',  # Faster, cheaper\n",
        "    'qwen-72b': 'Qwen/Qwen2-72B-Instruct',  # Good for analysis\n",
        "    'deepseek': 'deepseek-ai/deepseek-llm-67b-chat',  # Cost-effective\n",
        "}\n",
        "\n",
        "# Select model (change as needed)\n",
        "selected_model = MODELS['mixtral-8x7b']  # Start with Mixtral for good balance\n",
        "print(f\"Using model: {selected_model}\")\n",
        "\n",
        "# Test the API connection\n",
        "try:\n",
        "    test_response = together.Complete.create(\n",
        "        prompt=\"Respond with 'OK' if you're working.\",\n",
        "        model=selected_model,\n",
        "        max_tokens=10\n",
        "    )\n",
        "    print(f\"✓ API connection successful\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ API connection failed: {e}\")\n",
        "    print(\"Please check your API key and connection\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OE6NJpGNsfwR",
        "outputId": "cecd3089-ae3c-459b-b8b0-3dca9a239a06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using model: mistralai/Mixtral-8x7B-Instruct-v0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3800310509.py:17: DeprecationWarning: Call to deprecated function create.\n",
            "  test_response = together.Complete.create(\n",
            "/usr/local/lib/python3.12/dist-packages/together/legacy/complete.py:23: UserWarning: The use of together.api_key is deprecated and will be removed in the next major release. Please set the TOGETHER_API_KEY environment variable instead.\n",
            "  warnings.warn(API_KEY_WARNING)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ API connection successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_single_entry(row: pd.Series, model: str, max_retries: int = 3) -> Dict:\n",
        "    \"\"\"Validate a single study-findings pair.\"\"\"\n",
        "\n",
        "    prompt = create_validation_prompt(row)\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = together.Complete.create(\n",
        "                prompt=prompt,\n",
        "                model=model,\n",
        "                max_tokens=300,\n",
        "                temperature=0.1,  # Low temperature for consistency\n",
        "                stop=[\"```\", \"\\n\\n\\n\"]\n",
        "            )\n",
        "\n",
        "            result_text = response['output']['choices'][0]['text'].strip()\n",
        "\n",
        "            # Try to parse JSON\n",
        "            # Clean up common issues\n",
        "            result_text = result_text.replace('```json', '').replace('```', '').strip()\n",
        "            if result_text.startswith('{'):\n",
        "                end_idx = result_text.rfind('}')\n",
        "                if end_idx != -1:\n",
        "                    result_text = result_text[:end_idx + 1]\n",
        "\n",
        "            result = json.loads(result_text)\n",
        "            result['raw_response'] = result_text\n",
        "            result['model'] = model\n",
        "            result['success'] = True\n",
        "            return result\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(2)  # Rate limiting\n",
        "                continue\n",
        "            return {\n",
        "                'match_score': -1,\n",
        "                'confidence': 'error',\n",
        "                'issues': [f'JSON parse error: {str(e)}'],\n",
        "                'verdict': 'error',\n",
        "                'raw_response': result_text if 'result_text' in locals() else 'No response',\n",
        "                'success': False\n",
        "            }\n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(2)\n",
        "                continue\n",
        "            return {\n",
        "                'match_score': -1,\n",
        "                'confidence': 'error',\n",
        "                'issues': [f'API error: {str(e)}'],\n",
        "                'verdict': 'error',\n",
        "                'success': False\n",
        "            }\n",
        "\n",
        "    return {'success': False, 'verdict': 'error'}\n",
        "\n",
        "def quick_cross_check(row: pd.Series, model: str) -> Tuple[str, int]:\n",
        "    \"\"\"Perform a quick cross-check with simpler prompt.\"\"\"\n",
        "\n",
        "    prompt = create_cross_check_prompt(row)\n",
        "\n",
        "    try:\n",
        "        response = together.Complete.create(\n",
        "            prompt=prompt,\n",
        "            model=model,\n",
        "            max_tokens=20,\n",
        "            temperature=0.1\n",
        "        )\n",
        "\n",
        "        result = response['output']['choices'][0]['text'].strip()\n",
        "        parts = result.split()\n",
        "        if len(parts) >= 2:\n",
        "            verdict = parts[0]\n",
        "            score = int(parts[1])\n",
        "            return verdict, score\n",
        "        return \"UNCERTAIN\", 50\n",
        "\n",
        "    except Exception as e:\n",
        "        return \"ERROR\", -1"
      ],
      "metadata": {
        "id": "H6w9FfEUshYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"STARTING VALIDATION PROCESS\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Validating {len(validation_df)} studies...\")\n",
        "print(f\"Estimated time: {len(validation_df) * 3} seconds\")\n",
        "print(f\"Estimated cost: ~${len(validation_df) * 0.001:.2f} (varies by model)\")\n",
        "\n",
        "# Add columns for results\n",
        "validation_df['validation_score'] = None\n",
        "validation_df['validation_verdict'] = None\n",
        "validation_df['validation_confidence'] = None\n",
        "validation_df['validation_issues'] = None\n",
        "validation_df['cross_check_verdict'] = None\n",
        "validation_df['cross_check_score'] = None\n",
        "\n",
        "# Progress tracking\n",
        "validated_count = 0\n",
        "error_count = 0\n",
        "mismatch_count = 0\n",
        "\n",
        "# Run validation with progress bar\n",
        "for idx, row in tqdm(validation_df.iterrows(), total=len(validation_df), desc=\"Validating\"):\n",
        "\n",
        "    # Main validation\n",
        "    result = validate_single_entry(row, selected_model)\n",
        "\n",
        "    if result['success']:\n",
        "        validation_df.at[idx, 'validation_score'] = result['match_score']\n",
        "        validation_df.at[idx, 'validation_verdict'] = result['verdict']\n",
        "        validation_df.at[idx, 'validation_confidence'] = result['confidence']\n",
        "        validation_df.at[idx, 'validation_issues'] = json.dumps(result.get('issues', []))\n",
        "\n",
        "        if result['verdict'] in ['likely_mismatch', 'definite_mismatch']:\n",
        "            mismatch_count += 1\n",
        "\n",
        "        validated_count += 1\n",
        "    else:\n",
        "        error_count += 1\n",
        "        validation_df.at[idx, 'validation_verdict'] = 'error'\n",
        "\n",
        "    # Quick cross-check for suspicious cases\n",
        "    if result.get('match_score', 100) < 70:\n",
        "        verdict, score = quick_cross_check(row, selected_model)\n",
        "        validation_df.at[idx, 'cross_check_verdict'] = verdict\n",
        "        validation_df.at[idx, 'cross_check_score'] = score\n",
        "\n",
        "    # Rate limiting\n",
        "    time.sleep(1)  # Adjust based on your API tier\n",
        "\n",
        "    # Progress update every 10 items\n",
        "    if (validated_count + error_count) % 10 == 0:\n",
        "        print(f\"Progress: {validated_count + error_count}/{len(validation_df)} | Mismatches found: {mismatch_count}\")\n",
        "\n",
        "print(f\"\\n✓ Validation complete!\")\n",
        "print(f\"  Successfully validated: {validated_count}\")\n",
        "print(f\"  Errors: {error_count}\")\n",
        "print(f\"  Potential mismatches: {mismatch_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziK5fdBasj0H",
        "outputId": "4eab31da-0333-4c96-8e52-029419ade7fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STARTING VALIDATION PROCESS\n",
            "================================================================================\n",
            "Validating 100 studies...\n",
            "Estimated time: 300 seconds\n",
            "Estimated cost: ~$0.10 (varies by model)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rValidating:   0%|          | 0/100 [00:00<?, ?it/s]/tmp/ipython-input-2189024698.py:8: DeprecationWarning: Call to deprecated function create.\n",
            "  response = together.Complete.create(\n",
            "/usr/local/lib/python3.12/dist-packages/together/legacy/complete.py:23: UserWarning: The use of together.api_key is deprecated and will be removed in the next major release. Please set the TOGETHER_API_KEY environment variable instead.\n",
            "  warnings.warn(API_KEY_WARNING)\n",
            "/tmp/ipython-input-2189024698.py:64: DeprecationWarning: Call to deprecated function create.\n",
            "  response = together.Complete.create(\n",
            "Validating:  10%|█         | 10/100 [01:20<11:43,  7.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: 10/100 | Mismatches found: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating:  20%|██        | 20/100 [03:11<14:33, 10.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: 20/100 | Mismatches found: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating:  30%|███       | 30/100 [04:37<10:22,  8.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: 30/100 | Mismatches found: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating:  40%|████      | 40/100 [06:22<10:58, 10.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: 40/100 | Mismatches found: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating:  50%|█████     | 50/100 [07:49<07:11,  8.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: 50/100 | Mismatches found: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating:  60%|██████    | 60/100 [09:16<06:01,  9.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: 60/100 | Mismatches found: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating:  70%|███████   | 70/100 [10:36<04:00,  8.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: 70/100 | Mismatches found: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating:  80%|████████  | 80/100 [12:26<03:56, 11.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: 80/100 | Mismatches found: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating:  90%|█████████ | 90/100 [14:15<01:28,  8.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: 90/100 | Mismatches found: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 100/100 [15:47<00:00,  9.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: 100/100 | Mismatches found: 0\n",
            "\n",
            "✓ Validation complete!\n",
            "  Successfully validated: 0\n",
            "  Errors: 100\n",
            "  Potential mismatches: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"VALIDATION RESULTS ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Overall statistics\n",
        "valid_scores = validation_df[validation_df['validation_score'] > -1]['validation_score']\n",
        "print(\"\\nMatch Score Statistics:\")\n",
        "print(f\"  Mean score: {valid_scores.mean():.1f}\")\n",
        "print(f\"  Median score: {valid_scores.median():.1f}\")\n",
        "print(f\"  Min score: {valid_scores.min():.1f}\")\n",
        "print(f\"  Max score: {valid_scores.max():.1f}\")\n",
        "\n",
        "# Verdict distribution\n",
        "print(\"\\nVerdict Distribution:\")\n",
        "verdict_counts = validation_df['validation_verdict'].value_counts()\n",
        "for verdict, count in verdict_counts.items():\n",
        "    pct = count / len(validation_df) * 100\n",
        "    print(f\"  {verdict:20s}: {count:3d} ({pct:5.1f}%)\")\n",
        "\n",
        "# Confidence distribution\n",
        "print(\"\\nConfidence Levels:\")\n",
        "confidence_counts = validation_df['validation_confidence'].value_counts()\n",
        "for conf, count in confidence_counts.items():\n",
        "    pct = count / len(validation_df) * 100\n",
        "    print(f\"  {conf:10s}: {count:3d} ({pct:5.1f}%)\")\n",
        "\n",
        "# Identify problematic studies\n",
        "problematic = validation_df[\n",
        "    validation_df['validation_verdict'].isin(['likely_mismatch', 'definite_mismatch', 'uncertain'])\n",
        "].copy()\n",
        "\n",
        "print(f\"\\n⚠️ Found {len(problematic)} potentially problematic matches\")\n",
        "\n",
        "if len(problematic) > 0:\n",
        "    print(\"\\nSample problematic studies:\")\n",
        "    display_cols = ['rct_id', 'Title', 'validation_score', 'validation_verdict', 'validation_issues']\n",
        "    print(problematic[display_cols].head(10).to_string())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M67SyZ6JtmTT",
        "outputId": "f89afdc7-bf85-4dc0-9ae5-5bd4a0b61ffc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "VALIDATION RESULTS ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "Match Score Statistics:\n",
            "  Mean score: nan\n",
            "  Median score: nan\n",
            "  Min score: nan\n",
            "  Max score: nan\n",
            "\n",
            "Verdict Distribution:\n",
            "  error               : 100 (100.0%)\n",
            "\n",
            "Confidence Levels:\n",
            "\n",
            "⚠️ Found 0 potentially problematic matches\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import time\n",
        "from typing import Dict, List, Tuple\n",
        "import os\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Install together package\n",
        "!pip install --upgrade together --quiet\n",
        "\n",
        "# Import Together with new API\n",
        "from together import Together\n",
        "\n",
        "# Set up TogetherAI with new API format\n",
        "print(\"Setting up TogetherAI...\")\n",
        "\n",
        "# Method 1: Direct API key\n",
        "# client = Together(api_key=\"YOUR_API_KEY_HERE\")\n",
        "\n",
        "# Method 2: Environment variable (recommended)\n",
        "import getpass\n",
        "api_key = getpass.getpass('Enter your TogetherAI API key: ')\n",
        "os.environ['TOGETHER_API_KEY'] = api_key\n",
        "client = Together(api_key=api_key)\n",
        "\n",
        "print(\"✓ TogetherAI configured with new API\")\n",
        "\n",
        "# Base path for data\n",
        "base_path = '/content/drive/MyDrive/AEA_RCT_Parsed/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkupO4hNwqRW",
        "outputId": "812892b3-5e68-4f95-ffc2-76f991373712"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up TogetherAI...\n",
            "Enter your TogetherAI API key: ··········\n",
            "✓ TogetherAI configured with new API\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Testing API connection...\")\n",
        "\n",
        "# Test with a simple prompt\n",
        "test_prompt = \"Say 'API working' if you receive this message.\"\n",
        "\n",
        "try:\n",
        "    # New API format for chat completions\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": test_prompt}\n",
        "        ],\n",
        "        temperature=0.1,\n",
        "        max_tokens=50\n",
        "    )\n",
        "\n",
        "    test_result = response.choices[0].message.content\n",
        "    print(f\"✅ API Test Successful! Response: {test_result}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ API Test Failed!\")\n",
        "    print(f\"Error Type: {type(e).__name__}\")\n",
        "    print(f\"Error Message: {str(e)}\")\n",
        "    print(\"\\nTroubleshooting:\")\n",
        "    print(\"1. Check if your API key is valid\")\n",
        "    print(\"2. Verify you have credits in your TogetherAI account\")\n",
        "    print(\"3. Check if the model name is correct\")\n",
        "    print(\"4. Try a different model from the list below\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHKoEkiNwx8s",
        "outputId": "789029b3-2cac-45a2-f230-de50eb940443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing API connection...\n",
            "✅ API Test Successful! Response:  API working! I'm here to help you with any questions or tasks you have. Let me know how I can assist you today.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nLoading consolidated dataset...\")\n",
        "try:\n",
        "    df = pd.read_csv(f'{base_path}consolidated_rct_dataset_clean.csv')\n",
        "    print(f\"✓ Loaded {len(df)} studies\")\n",
        "\n",
        "    # Show data sample\n",
        "    print(\"\\nDataset columns:\", df.columns.tolist())\n",
        "    print(\"\\nFirst study sample:\")\n",
        "    print(f\"Title: {df.iloc[0]['Title'][:100] if 'Title' in df.columns else 'N/A'}\")\n",
        "    print(f\"Abstract length: {len(str(df.iloc[0]['Abstract'])) if 'Abstract' in df.columns else 'N/A'}\")\n",
        "    print(f\"Findings length: {len(str(df.iloc[0]['Findings'])) if 'Findings' in df.columns else 'N/A'}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error loading data: {e}\")\n",
        "    print(\"Please check the file path\")\n",
        "\n",
        "# Create small test sample first\n",
        "test_size = 5  # Start with just 5 for debugging\n",
        "validation_df = df.sample(n=test_size, random_state=42).copy()\n",
        "print(f\"\\n✓ Selected {len(validation_df)} studies for initial testing\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaCWH8Grw3zA",
        "outputId": "f289448e-2638-43e7-ed2a-f79202af875b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading consolidated dataset...\n",
            "✓ Loaded 2281 studies\n",
            "\n",
            "Dataset columns: ['Title', 'Country', 'Subregion', 'Abstract', 'Description_of_intervention', 'Primary_outcomes', 'Secondary_outcomes', 'Population', 'Search_vector', 'keywords', 'keywords_additional', 'jel_codes', 'keywords_sector', 'keywords_mechanisms', 'keywords_implementation', 'keywords_context', 'Researchers', 'Researcher_affiliation', 'Year', 'Findings', 'rct_id']\n",
            "\n",
            "First study sample:\n",
            "Title: Two Approaches to Community Development\n",
            "Abstract length: 1243\n",
            "Findings length: 377\n",
            "\n",
            "✓ Selected 5 studies for initial testing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_simple_validation_prompt(row: pd.Series) -> str:\n",
        "    \"\"\"Create a simpler validation prompt that's more likely to work.\"\"\"\n",
        "\n",
        "    # Keep it short to avoid token limits\n",
        "    title = str(row.get('Title', 'N/A'))[:200]\n",
        "    abstract = str(row.get('Abstract', 'N/A'))[:300]\n",
        "    findings = str(row.get('Findings', 'N/A'))[:400]\n",
        "\n",
        "    prompt = f\"\"\"Check if these findings match this study.\n",
        "\n",
        "Study Title: {title}\n",
        "\n",
        "Study Abstract: {abstract}\n",
        "\n",
        "Findings: {findings}\n",
        "\n",
        "Question: Do the findings above appear to be from the same study described in the title and abstract?\n",
        "\n",
        "Answer with a number 0-100 (100 means definitely same study, 0 means definitely different study) and one word verdict.\n",
        "Format: [SCORE] [VERDICT]\n",
        "Example: 85 likely_match\n",
        "\n",
        "Your answer:\"\"\"\n",
        "\n",
        "    return prompt\n",
        "\n",
        "def validate_with_new_api(row: pd.Series, model: str = \"mistralai/Mixtral-8x7B-Instruct-v0.1\") -> Dict:\n",
        "    \"\"\"Validate using the new Together API format.\"\"\"\n",
        "\n",
        "    prompt = create_simple_validation_prompt(row)\n",
        "\n",
        "    try:\n",
        "        # New API format\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a research validation expert. Be concise.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.1,\n",
        "            max_tokens=50\n",
        "        )\n",
        "\n",
        "        result_text = response.choices[0].message.content.strip()\n",
        "\n",
        "        # Parse simple response format\n",
        "        parts = result_text.split()\n",
        "        if len(parts) >= 2:\n",
        "            try:\n",
        "                score = float(parts[0])\n",
        "                verdict = parts[1]\n",
        "\n",
        "                # Categorize verdict\n",
        "                if score >= 80:\n",
        "                    verdict_category = \"correct_match\"\n",
        "                elif score >= 60:\n",
        "                    verdict_category = \"likely_match\"\n",
        "                elif score >= 40:\n",
        "                    verdict_category = \"uncertain\"\n",
        "                elif score >= 20:\n",
        "                    verdict_category = \"likely_mismatch\"\n",
        "                else:\n",
        "                    verdict_category = \"definite_mismatch\"\n",
        "\n",
        "                return {\n",
        "                    'success': True,\n",
        "                    'match_score': score,\n",
        "                    'verdict': verdict_category,\n",
        "                    'raw_response': result_text,\n",
        "                    'confidence': 'medium'\n",
        "                }\n",
        "            except ValueError:\n",
        "                pass\n",
        "\n",
        "        # If parsing failed, return error\n",
        "        return {\n",
        "            'success': False,\n",
        "            'error': 'Failed to parse response',\n",
        "            'raw_response': result_text,\n",
        "            'match_score': -1,\n",
        "            'verdict': 'parse_error'\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'success': False,\n",
        "            'error': str(e),\n",
        "            'error_type': type(e).__name__,\n",
        "            'match_score': -1,\n",
        "            'verdict': 'api_error'\n",
        "        }"
      ],
      "metadata": {
        "id": "3EDKXvFow5_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"DEBUG TEST RUN\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Test on first few studies with detailed error reporting\n",
        "for i, (idx, row) in enumerate(validation_df.head(3).iterrows()):\n",
        "    print(f\"\\n--- Testing Study {i+1} ---\")\n",
        "    print(f\"RCT ID: {row['rct_id']}\")\n",
        "    print(f\"Title: {str(row['Title'])[:100]}...\")\n",
        "\n",
        "    result = validate_with_new_api(row)\n",
        "\n",
        "    if result['success']:\n",
        "        print(f\"✅ Success!\")\n",
        "        print(f\"   Score: {result['match_score']}\")\n",
        "        print(f\"   Verdict: {result['verdict']}\")\n",
        "        print(f\"   Raw response: {result['raw_response']}\")\n",
        "    else:\n",
        "        print(f\"❌ Failed!\")\n",
        "        print(f\"   Error type: {result.get('error_type', 'Unknown')}\")\n",
        "        print(f\"   Error: {result.get('error', 'Unknown error')}\")\n",
        "        print(f\"   Raw response: {result.get('raw_response', 'No response')}\")\n",
        "\n",
        "    time.sleep(2)  # Rate limiting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09QD1C7ExZkW",
        "outputId": "2bada5b3-6c8a-42b6-dc71-5835b6d1ff0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "DEBUG TEST RUN\n",
            "================================================================================\n",
            "\n",
            "--- Testing Study 1 ---\n",
            "RCT ID: 3939.0\n",
            "Title: Salience of the energy-efficiency trade-off and purchase of energy efficient appliances...\n",
            "✅ Success!\n",
            "   Score: 90.0\n",
            "   Verdict: correct_match\n",
            "   Raw response: 90 mismatch\n",
            "\n",
            "The findings do not match the study title and abstract. The study is about purchasing more energy-efficient appliances when energy cost information is made salient, while the findings indicate purchasing less energy-efficient appliances.\n",
            "\n",
            "--- Testing Study 2 ---\n",
            "RCT ID: 2631.0\n",
            "Title: Evaluation of Big Word Club...\n",
            "✅ Success!\n",
            "   Score: 90.0\n",
            "   Verdict: correct_match\n",
            "   Raw response: 90 match. The findings mention a specific program (\"Big Word Club\") and its effectiveness in teaching vocabulary, which aligns with the study title and abstract. The study's focus on evaluating vocabulary programs and the use of a\n",
            "\n",
            "--- Testing Study 3 ---\n",
            "RCT ID: 692.0\n",
            "Title: Nudging and Intrapreneurship...\n",
            "✅ Success!\n",
            "   Score: 90.0\n",
            "   Verdict: correct_match\n",
            "   Raw response: 90 likely_match\n",
            "\n",
            "The findings align with the study title and abstract, focusing on intrapreneurship and the use of nudges to influence employee behavior. However, the abstract does not explicitly mention the use of an Innov\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Install together package\n",
        "!pip install --upgrade together --quiet\n",
        "\n",
        "# Import Together with new API\n",
        "from together import Together\n",
        "\n",
        "# Set up TogetherAI\n",
        "print(\"Setting up TogetherAI...\")\n",
        "import getpass\n",
        "api_key = getpass.getpass('Enter your TogetherAI API key: ')\n",
        "os.environ['TOGETHER_API_KEY'] = api_key\n",
        "client = Together(api_key=api_key)\n",
        "print(\"✓ TogetherAI configured\")\n",
        "\n",
        "# Base path for data\n",
        "base_path = '/content/drive/MyDrive/AEA_RCT_Parsed/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgJCCUY1yET5",
        "outputId": "aaac9c0b-7b3e-4ecc-f6e9-e76298cdd03c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up TogetherAI...\n",
            "Enter your TogetherAI API key: ··········\n",
            "✓ TogetherAI configured\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"LOADING DATA AND SELECTING RANDOM SAMPLE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Load full dataset\n",
        "df = pd.read_csv(f'{base_path}consolidated_rct_dataset_clean.csv')\n",
        "print(f\"✓ Loaded {len(df)} total studies from dataset\")\n",
        "\n",
        "# Select 40 random studies\n",
        "SAMPLE_SIZE = 40\n",
        "validation_df = df.sample(n=SAMPLE_SIZE, random_state=42).copy()\n",
        "print(f\"✓ Randomly selected {SAMPLE_SIZE} studies for validation\")\n",
        "\n",
        "# Show the RCT IDs selected\n",
        "print(\"\\nRCT IDs selected for validation:\")\n",
        "selected_ids = validation_df['rct_id'].tolist()\n",
        "print(f\"{selected_ids[:10]} ... (showing first 10 of {len(selected_ids)})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nC3KLf0yGqJ",
        "outputId": "d1a127b5-3611-4e1e-b7f7-02df73547ba2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "LOADING DATA AND SELECTING RANDOM SAMPLE\n",
            "================================================================================\n",
            "✓ Loaded 2281 total studies from dataset\n",
            "✓ Randomly selected 40 studies for validation\n",
            "\n",
            "RCT IDs selected for validation:\n",
            "[3939.0, 2631.0, 692.0, 2964.0, 502.0, 7252.0, 8635.0, 9791.0, 370.0, 1446.0] ... (showing first 10 of 40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"TESTING API CONNECTION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "try:\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "        messages=[{\"role\": \"user\", \"content\": \"Say 'OK' if working\"}],\n",
        "        max_tokens=10,\n",
        "        temperature=0.1\n",
        "    )\n",
        "    print(f\"✅ API Test Successful! Response: {response.choices[0].message.content}\")\n",
        "    model_to_use = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Mixtral failed, trying Mistral-7B...\")\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "            messages=[{\"role\": \"user\", \"content\": \"Say 'OK' if working\"}],\n",
        "            max_tokens=10,\n",
        "            temperature=0.1\n",
        "        )\n",
        "        print(f\"✅ Mistral-7B works!\")\n",
        "        model_to_use = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "    except:\n",
        "        print(\"❌ API connection failed. Please check your key.\")\n",
        "        model_to_use = None\n",
        "\n",
        "print(f\"\\nUsing model: {model_to_use}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6uXoBGWyNjB",
        "outputId": "d76a7aec-b15f-4c21-ed32-1bb12147e553"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "TESTING API CONNECTION\n",
            "================================================================================\n",
            "✅ API Test Successful! Response:  OK, I'm here and ready to assist\n",
            "\n",
            "Using model: mistralai/Mixtral-8x7B-Instruct-v0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_study(row: pd.Series, model: str) -> dict:\n",
        "    \"\"\"Validate if findings match the study.\"\"\"\n",
        "\n",
        "    # Extract key fields (truncate to avoid token limits)\n",
        "    title = str(row.get('Title', 'N/A'))[:300]\n",
        "    abstract = str(row.get('Abstract', 'N/A'))[:400]\n",
        "    intervention = str(row.get('Description_of_intervention', 'N/A'))[:200]\n",
        "    outcomes = str(row.get('Primary_outcomes', 'N/A'))[:200]\n",
        "    findings = str(row.get('Findings', 'N/A'))[:500]\n",
        "    year = str(row.get('Year', 'N/A'))\n",
        "    country = str(row.get('Country', 'N/A'))\n",
        "\n",
        "    prompt = f\"\"\"Analyze if these research findings correctly match the study described.\n",
        "\n",
        "STUDY INFORMATION:\n",
        "Title: {title}\n",
        "Year: {year}\n",
        "Country: {country}\n",
        "Abstract: {abstract}\n",
        "Intervention: {intervention}\n",
        "Primary Outcomes: {outcomes}\n",
        "\n",
        "FINDINGS TO VALIDATE:\n",
        "{findings}\n",
        "\n",
        "QUESTION: Do these findings appear to be from the same study described above?\n",
        "\n",
        "Provide:\n",
        "1. A match score from 0-100 (100 = definitely same study, 0 = definitely different)\n",
        "2. Main reason for your score\n",
        "3. Any red flags noticed\n",
        "\n",
        "Format your response as:\n",
        "SCORE: [number]\n",
        "REASON: [one sentence explanation]\n",
        "FLAGS: [any issues, or \"none\"]\n",
        "\n",
        "Response:\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a research validation expert. Be direct and concise.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.1,\n",
        "            max_tokens=150\n",
        "        )\n",
        "\n",
        "        result_text = response.choices[0].message.content.strip()\n",
        "\n",
        "        # Parse the response\n",
        "        score = -1\n",
        "        reason = \"\"\n",
        "        flags = \"\"\n",
        "\n",
        "        for line in result_text.split('\\n'):\n",
        "            if 'SCORE:' in line:\n",
        "                try:\n",
        "                    score = float(line.split('SCORE:')[1].strip().split()[0])\n",
        "                except:\n",
        "                    score = -1\n",
        "            elif 'REASON:' in line:\n",
        "                reason = line.split('REASON:')[1].strip()\n",
        "            elif 'FLAGS:' in line or 'FLAG:' in line:\n",
        "                flags = line.split(':', 1)[1].strip() if ':' in line else \"\"\n",
        "\n",
        "        # Determine verdict based on score\n",
        "        if score >= 80:\n",
        "            verdict = \"✅ MATCH\"\n",
        "        elif score >= 60:\n",
        "            verdict = \"🟡 LIKELY MATCH\"\n",
        "        elif score >= 40:\n",
        "            verdict = \"🟠 UNCERTAIN\"\n",
        "        elif score >= 20:\n",
        "            verdict = \"🔴 LIKELY MISMATCH\"\n",
        "        elif score >= 0:\n",
        "            verdict = \"❌ MISMATCH\"\n",
        "        else:\n",
        "            verdict = \"❓ ERROR\"\n",
        "\n",
        "        return {\n",
        "            'success': True,\n",
        "            'score': score,\n",
        "            'verdict': verdict,\n",
        "            'reason': reason,\n",
        "            'flags': flags,\n",
        "            'raw_response': result_text\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'success': False,\n",
        "            'score': -1,\n",
        "            'verdict': \"❓ ERROR\",\n",
        "            'reason': str(e)[:100],\n",
        "            'flags': 'API error',\n",
        "            'raw_response': ''\n",
        "        }"
      ],
      "metadata": {
        "id": "2G9eTXgCyPuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(f\"STARTING VALIDATION OF {SAMPLE_SIZE} RANDOM STUDIES\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Estimated time: {SAMPLE_SIZE * 2} seconds\")\n",
        "print(f\"Starting at: {datetime.now().strftime('%H:%M:%S')}\\n\")\n",
        "\n",
        "# Initialize results storage\n",
        "results = []\n",
        "success_count = 0\n",
        "error_count = 0\n",
        "mismatch_count = 0\n",
        "uncertain_count = 0\n",
        "\n",
        "# Process each study\n",
        "for i, (idx, row) in enumerate(validation_df.iterrows(), 1):\n",
        "    print(f\"\\n--- Study {i}/{SAMPLE_SIZE} ---\")\n",
        "    print(f\"RCT ID: {row['rct_id']}\")\n",
        "    print(f\"Title: {str(row['Title'])[:80]}...\")\n",
        "\n",
        "    # Validate\n",
        "    result = validate_study(row, model_to_use)\n",
        "\n",
        "    # Store result\n",
        "    results.append({\n",
        "        'rct_id': row['rct_id'],\n",
        "        'title': row['Title'],\n",
        "        'score': result['score'],\n",
        "        'verdict': result['verdict'],\n",
        "        'reason': result['reason'],\n",
        "        'flags': result['flags']\n",
        "    })\n",
        "\n",
        "    # Print result\n",
        "    print(f\"Score: {result['score']}\")\n",
        "    print(f\"Verdict: {result['verdict']}\")\n",
        "    print(f\"Reason: {result['reason']}\")\n",
        "    if result['flags'] and result['flags'] != 'none':\n",
        "        print(f\"⚠️ Flags: {result['flags']}\")\n",
        "\n",
        "    # Update counters\n",
        "    if result['success']:\n",
        "        success_count += 1\n",
        "        if result['score'] < 40:\n",
        "            mismatch_count += 1\n",
        "        elif result['score'] < 60:\n",
        "            uncertain_count += 1\n",
        "    else:\n",
        "        error_count += 1\n",
        "\n",
        "    # Progress indicator every 10\n",
        "    if i % 10 == 0:\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"PROGRESS: {i}/{SAMPLE_SIZE} completed\")\n",
        "        print(f\"Success: {success_count} | Errors: {error_count} | Mismatches: {mismatch_count}\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "    # Rate limiting\n",
        "    time.sleep(1.5)\n",
        "\n",
        "print(f\"\\n✅ VALIDATION COMPLETE at {datetime.now().strftime('%H:%M:%S')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHj3nePUys_d",
        "outputId": "3b0c1790-3a89-40dc-e3f4-0c0b4d5566fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STARTING VALIDATION OF 40 RANDOM STUDIES\n",
            "================================================================================\n",
            "Estimated time: 80 seconds\n",
            "Starting at: 01:57:21\n",
            "\n",
            "\n",
            "--- Study 1/40 ---\n",
            "RCT ID: 3939.0\n",
            "Title: Salience of the energy-efficiency trade-off and purchase of energy efficient app...\n",
            "Score: 0.0\n",
            "Verdict: ❌ MISMATCH\n",
            "Reason: The finding states that more salient energy cost information leads to purchasing less energy-efficient appliances, while the study description implies that it should lead to purchasing more energy-efficient appliances.\n",
            "⚠️ Flags: None. The discrepancy is clear and direct.\n",
            "\n",
            "--- Study 2/40 ---\n",
            "RCT ID: 2631.0\n",
            "Title: Evaluation of Big Word Club...\n",
            "Score: 90.0\n",
            "Verdict: ✅ MATCH\n",
            "Reason: The findings describe an evaluation of a digital learning program, Big Word Club, which matches the study description. The primary outcome of improved receptive vocabulary after four months of use is also consistent with the findings.\n",
            "⚠️ Flags: The only discrepancy is the lack of information on the standardized vocabulary assessment (PPVT) mentioned in the findings. It is unclear whether this was a secondary outcome in the study or not. Overall, the findings seem to be mostly consistent with the study description.\n",
            "\n",
            "--- Study 3/40 ---\n",
            "RCT ID: 692.0\n",
            "Title: Nudging and Intrapreneurship...\n",
            "Score: 80.0\n",
            "Verdict: ✅ MATCH\n",
            "Reason: The findings generally align with the study description, focusing on nudges for intrapreneurship and measuring the number and quality of ideas.\n",
            "⚠️ Flags: The findings mention an Innovation Challenge and framing effects, which are not explicitly mentioned in the study description. However, these elements could be part of the study and not highlighted in the description provided.\n",
            "\n",
            "--- Study 4/40 ---\n",
            "RCT ID: 2964.0\n",
            "Title: Smoke on the beach: a field experiment to reduce cigarette littering...\n",
            "Score: 80.0\n",
            "Verdict: ✅ MATCH\n",
            "Reason: The findings describe results of a study involving interventions to reduce cigarette littering at beach resorts, which aligns with the study title and location.\n",
            "⚠️ Flags: The findings do not provide specific details about the two interventions, making it difficult to confirm if they match the study description exactly. Additionally, the findings do not mention the use of a control group with no ashtrays, which is part of the study design.\n",
            "\n",
            "--- Study 5/40 ---\n",
            "RCT ID: 502.0\n",
            "Title: Gift Exchange at Work...\n",
            "Score: 90.0\n",
            "Verdict: ✅ MATCH\n",
            "Reason: The findings align with the study description, mentioning a field experiment with temporary workers, estimating effort choices, and analyzing the impact of pay schemes and gifts on productivity.\n",
            "⚠️ Flags: The findings omit some details about the specific interventions and outcomes, but none that contradict the study description.\n",
            "\n",
            "--- Study 6/40 ---\n",
            "RCT ID: 7252.0\n",
            "Title: FISCAL POLICY AND HOUSEHOLDS’ INFLATION EXPECTATIONS...\n",
            "Score: 90.0\n",
            "Verdict: ✅ MATCH\n",
            "Reason: The findings align with the study description, as both refer to the impact of fiscal information on households' inflation expectations.\n",
            "⚠️ Flags: The findings mention \"news about future debt levels,\" while the study description mentions \"information treatments about the fiscal outlook.\" It would be helpful to know if these refer to the same concept. Additionally, the findings mention impacts on households' expectations of government spending and revenues, which are not explicitly mentioned in the study description's primary outcomes. However, these mentions are brief and do not contradict the overall alignment between the findings and the study description.\n",
            "\n",
            "--- Study 7/40 ---\n",
            "RCT ID: 8635.0\n",
            "Title: What do you want to become? Career Aspiration and School Performance. Evidence f...\n",
            "Score: 90.0\n",
            "Verdict: ✅ MATCH\n",
            "Reason: The findings align with the study description, both mentioning a two-phase randomized intervention targeting Italian high school students' career aspirations and academic performance.\n",
            "⚠️ Flags: The findings mention the researcher's name, Steven Stillman, which is not explicitly stated in the study description. However, this is a minor discrepancy and may be due to the source of the findings.\n",
            "\n",
            "--- Study 8/40 ---\n",
            "RCT ID: 9791.0\n",
            "Title: Attitudes towards refugees and identity...\n",
            "Score: 20.0\n",
            "Verdict: 🔴 LIKELY MISMATCH\n",
            "Reason: The findings mention a study by Cristina Cattaneo, which is not the same as the study named in the study description.\n",
            "⚠️ Flags: Different study author and no mention of the manipulation (information about EU immigration policy reforms) described in the study.\n",
            "\n",
            "--- Study 9/40 ---\n",
            "RCT ID: 370.0\n",
            "Title: Evaluating the Effects of Entrepreneurship Edutainment in Egypt...\n",
            "Score: 80.0\n",
            "Verdict: ✅ MATCH\n",
            "Reason: The findings generally align with the study description, but the specific outcome of \"knowledge of the entrepreneurial ecosystem\" is not explicitly mentioned in the study description.\n",
            "⚠️ Flags: The outcome of \"key barriers identified\" is not included in the findings.\n",
            "\n",
            "--- Study 10/40 ---\n",
            "RCT ID: 1446.0\n",
            "Title: From natural variation to optimal policy? The importance of endogenous peer grou...\n",
            "Score: 0.0\n",
            "Verdict: ❌ MISMATCH\n",
            "Reason: The study findings describe a negative and significant treatment effect for the students the intervention aimed to help, but the study description explains that the intervention was designed to maximize the academic performance of the lowest ability students. These outcomes are contradictory.\n",
            "⚠️ Flags: The findings mention \"students we intended to help\" but the study description does not specify which students were intended to be helped. This is a red flag as it suggests a mismatch between the findings and the study description.\n",
            "\n",
            "==================================================\n",
            "PROGRESS: 10/40 completed\n",
            "Success: 10 | Errors: 0 | Mismatches: 3\n",
            "==================================================\n",
            "\n",
            "--- Study 11/40 ---\n",
            "RCT ID: 8720.0\n",
            "Title: The Habit-Forming Effects of Feedback: Evidence from a Large-Scale Field Experim...\n",
            "Score: 90.0\n",
            "Verdict: ✅ MATCH\n",
            "Reason: The findings align well with the study description, highlighting the attention-based model of habit formation and real-time feedback's impact on water usage behavior.\n",
            "⚠️ Flags: None, but the findings seem to focus more on the results than the study details. A more explicit connection to the Australian context and the Amphiro B1 smart meter would strengthen the validation.\n",
            "\n",
            "--- Study 12/40 ---\n",
            "RCT ID: 1138.0\n",
            "Title: The Limits of Health and Nutrition Education: Evidence from Three Randomized-Con...\n",
            "Score: 90.0\n",
            "Verdict: ✅ MATCH\n",
            "Reason: The findings match the study's primary outcomes, which state that there is little evidence of changes in blood hemoglobin concentration or anemia across all three studies.\n",
            "⚠️ Flags: None. The findings directly correspond to the study's primary outcomes, and the study description provides enough context to support the findings.\n",
            "\n",
            "--- Study 13/40 ---\n",
            "RCT ID: 1059.0\n",
            "Title: Investing in Schooling In Chile: The Role of Information about Financial Aid for...\n",
            "Score: 80.0\n",
            "Verdict: ✅ MATCH\n",
            "Reason: The findings address the impacts of providing information about financing higher education on schooling outcomes for low-income Chilean adolescents, which aligns with the study's title and intervention.\n",
            "⚠️ Flags: The findings do not explicitly mention the comparison between the effects on students and parents, but the study abstract implies that this comparison was made.\n",
            "\n",
            "--- Study 14/40 ---\n",
            "RCT ID: 2717.0\n",
            "Title: Girl Empower...\n",
            "Score: 90.0\n",
            "Verdict: ✅ MATCH\n",
            "Reason: The findings describe an evaluation of the Girl Empower program, which matches the study title. The intervention and primary outcomes also align with the study description.\n",
            "⚠️ Flags: The findings mention additional outcomes (gender attitudes, life skills, and sexual reproductive health) not specified in the study description. However, this is not necessarily a red flag as these outcomes could be part of the index of sexual violence.\n",
            "\n",
            "--- Study 15/40 ---\n",
            "RCT ID: 540.0\n",
            "Title: The effect of information on attitudes toward repugnant transactions...\n",
            "Score: 90.0\n",
            "Verdict: ✅ MATCH\n",
            "Reason: The findings align with the study description, mentioning the focus on payments for human organs, use of an online platform for data collection, and exploration of attitudes' heterogeneity.\n",
            "⚠️ Flags: The findings mention additional factors (gender, religiosity) not explicitly stated in the study description, but this could be part of the \"heterogeneous factors\" mentioned in the abstract. No red flags were noticed.\n",
            "\n",
            "--- Study 16/40 ---\n",
            "RCT ID: 1132.0\n",
            "Title: Small individual loans and mental health: a randomized controlled trial among So...\n",
            "Score: 80.0\n",
            "Verdict: ✅ MATCH\n",
            "Reason: The findings describe a randomized encouragement for loan access, which aligns with the study's intervention. However, the findings do not directly mention the study's primary outcomes.\n",
            "⚠️ Flags: The primary outcomes of the study are not explicitly stated in the findings, which could indicate a mismatch.\n",
            "\n",
            "--- Study 17/40 ---\n",
            "RCT ID: 1867.0\n",
            "Title: Small and Medium Enterprise Financing and Mentoring Services in Emerging Markets...\n",
            "Score: 90.0\n",
            "Verdict: ✅ MATCH\n",
            "Reason: The findings align with the study's intervention and primary outcomes, focusing on financial practices and revenues.\n",
            "⚠️ Flags: The findings mention \"objective reporting quality,\" which is not explicitly stated in the study description. However, this could still be part of the study's findings, so it's not a major concern.\n",
            "\n",
            "--- Study 18/40 ---\n",
            "RCT ID: 5059.0\n",
            "Title: The Introduction of Hormonal Contraception and the Incidence of Suicide Among Ad...\n",
            "Score: 100.0\n",
            "Verdict: ✅ MATCH\n",
            "Reason: The findings accurately summarize the main conclusion of the study described.\n",
            "⚠️ Flags: None. The title of the study is not provided in the findings, but the description of the study matches.\n",
            "\n",
            "--- Study 19/40 ---\n",
            "RCT ID: 8408.0\n",
            "Title: Public good games and attitudes towards vaccination: group size and safe action...\n",
            "Score: 70.0\n",
            "Verdict: 🟡 LIKELY MATCH\n",
            "Reason: The findings seem to be partially based on the study described, as the study does include a public good game and a focus on attitudes towards vaccination. However, the findings go beyond the scope of the study by making claims about vaccination status and specific attitudes/preferences of individuals.\n",
            "⚠️ Flags: The findings seem to overstate the results of the study by making specific claims about \"no-vaxxers\" and their attitudes/preferences, which were not explicitly studied in the described research.\n",
            "\n",
            "--- Study 20/40 ---\n",
            "RCT ID: 1194.0\n",
            "Title: Putting a Band-Aid on a corpse: Incentives for nurses in the Indian public healt...\n",
            "Score: 80.0\n",
            "Verdict: ✅ MATCH\n",
            "Reason: The findings describe an intervention with financial incentives for nurses in the Indian public healthcare system, which aligns with the study description. However, the findings do not explicitly mention the time/date-stamping machine or the distinction between tenured and contractual ANMs.\n",
            "⚠️ Flags: The findings do not explicitly mention the time/date-stamping machine or the distinction between tenured and contractual ANMs, which are key components of the study.\n",
            "\n",
            "==================================================\n",
            "PROGRESS: 20/40 completed\n",
            "Success: 20 | Errors: 0 | Mismatches: 3\n",
            "==================================================\n",
            "\n",
            "--- Study 21/40 ---\n",
            "RCT ID: 2072.0\n",
            "Title: Increasing transparency of government processes using SMS messaging...\n",
            "Score: 80.0\n",
            "Verdict: ✅ MATCH\n",
            "Reason: The findings generally align with the study's intervention and primary outcomes.\n",
            "⚠️ Flags: The findings mention \"individuals interacting with government processes\" instead of the specific program participants, and they combine the two primary outcomes into one statement. However, these discrepancies do not necessarily indicate a different study.\n",
            "\n",
            "--- Study 22/40 ---\n",
            "RCT ID: 582.0\n",
            "Title: The impact of prepaid electricity meters on low income households in Cape Town S...\n",
            "Score: 80.0\n",
            "Verdict: ✅ MATCH\n",
            "Reason: The findings align with the study's intervention and primary outcomes, but specific numbers were not provided in the study description.\n",
            "⚠️ Flags: None. The findings are plausible and consistent with the study's purpose. However, without the original study, it is not possible to confirm if the 13% decrease is an accurate representation of the results.\n",
            "\n",
            "--- Study 23/40 ---\n",
            "RCT ID: 2095.0\n",
            "Title: Finnish Basic Income Experiment...\n",
            "Score: 80.0\n",
            "Verdict: ✅ MATCH\n",
            "Reason: The findings align with the study's primary outcome of \"Days in employment\" and the intervention of providing unconditional basic income.\n",
            "⚠️ Flags: The finding mentions \"considerable increase in work incentives,\" which is not explicitly stated in the study description, so it's a bit unclear how this was measured or determined.\n",
            "\n",
            "--- Study 24/40 ---\n",
            "RCT ID: 3327.0\n",
            "Title: Alan-Ertac Grit and Growth Mindset Intervention Study 2 (replication)...\n",
            "Score: 80.0\n",
            "Verdict: ✅ MATCH\n",
            "Reason: The findings generally align with the study description, focusing on the replication of Study 1's grit and growth mindset intervention with similar primary outcomes.\n",
            "⚠️ Flags: The findings mention \"social preferences\" and \"donate their earnings,\" which are not explicitly mentioned in the study description. This discrepancy lowers the match score.\n",
            "\n",
            "--- Study 25/40 ---\n",
            "RCT ID: 15014.0\n",
            "Title: Pilot on Complexity on Social Learning...\n",
            "Score: 20.0\n",
            "Verdict: 🔴 LIKELY MISMATCH\n",
            "Reason: The findings are from related work by the same author, but they do not directly match the study description.\n",
            "⚠️ Flags: The findings are from related work on a broader topic, not specifically the \"Pilot on Complexity on Social Learning\" study. The intervention described in the findings is also different from the one in the study description.\n",
            "\n",
            "--- Study 26/40 ---\n",
            "RCT ID: 728.0\n",
            "Title: Intergenerational Impacts of Health Investments in Kenya...\n",
            "Score: 0.0\n",
            "Verdict: ❌ MISMATCH\n",
            "Reason: The findings mention unconditional cash transfers, while the study describes a deworming program.\n",
            "⚠️ Flags: The findings do not match the study in terms of intervention and outcomes.\n",
            "\n",
            "--- Study 27/40 ---\n",
            "RCT ID: 608.0\n",
            "Title: Consumption Response to Credit Expansions...\n",
            "Score: 90.0\n",
            "Verdict: ✅ MATCH\n",
            "Reason: The findings align with the study description, mentioning a field experiment in Turkey and a credit limit increase leading to increased spending.\n",
            "⚠️ Flags: The author of the findings is different from the study description, and the primary outcomes are not explicitly stated as spending, contract choice, and balance sheets.\n",
            "\n",
            "--- Study 28/40 ---\n",
            "RCT ID: 5596.0\n",
            "Title: Comparing efficacy of treatment as usual or treatment with acceptance and commit...\n",
            "Score: 80.0\n",
            "Verdict: ✅ MATCH\n",
            "Reason: The findings generally align with the study's objective, which aimed to compare the efficacy of treatment as usual and ACT for stigma and shame in SUD patients.\n",
            "⚠️ Flags: None, but the abstract does not explicitly state that the combined treatment was more effective than treatment as usual. This conclusion is inferred from the primary outcome measure presented in the findings.\n",
            "\n",
            "--- Study 29/40 ---\n",
            "RCT ID: 1809.0\n",
            "Title: A Lab-in-the-field experiment on Community Decision-Making in Bangladesh...\n",
            "Score: 80.0\n",
            "Verdict: ✅ MATCH\n",
            "Reason: The findings align with the study's primary outcomes of evaluating the impact of a Community Driven Development (CDD) program on social preferences, specifically preferences for participatory decision-making.\n",
            "⚠️ Flags: The findings do not explicitly mention the arsenic mitigation program or the outcomes related to individual preferences toward equality, equity, and their aggregation with bargaining.\n",
            "\n",
            "--- Study 30/40 ---\n",
            "RCT ID: 14281.0\n",
            "Title: Small quasi-cash rewards for plasma donations...\n",
            "Score: 80.0\n",
            "Verdict: ✅ MATCH\n",
            "Reason: The findings generally align with the study description, but the percentages and duration of the reward period differ from the described intervention.\n",
            "⚠️ Flags: The specific percentages and reward periods in the findings are not mentioned in the study description, which could indicate a mismatch. However, it's possible that the findings are based on a subset of the data or a specific group of donors, which might explain the discrepancy. Further investigation is needed to confirm if these findings are from the same study.\n",
            "\n",
            "==================================================\n",
            "PROGRESS: 30/40 completed\n",
            "Success: 30 | Errors: 0 | Mismatches: 5\n",
            "==================================================\n",
            "\n",
            "--- Study 31/40 ---\n",
            "RCT ID: 8105.0\n",
            "Title: Justification of the COVID-19 vaccine distribution: Contested inclusiveness in e...\n",
            "Score: 90.0\n",
            "Verdict: ✅ MATCH\n",
            "Reason: The findings align with the study description, mentioning the prioritization of Japanese citizenship in vaccine allocation.\n",
            "⚠️ Flags: None, but the source references for the findings should be checked to ensure they match the study's title and author.\n",
            "\n",
            "--- Study 32/40 ---\n",
            "RCT ID: 725.0\n",
            "Title: On the Effect of the Costs of Operating Formally...\n",
            "Score: 90.0\n",
            "Verdict: ✅ MATCH\n",
            "Reason: The findings align with the study description, mentioning a workshop treatment, meetings with CCB agents, and the outcome of operating formally.\n",
            "⚠️ Flags: The findings mention only two primary outcomes while the study description mentions two interventions and primary outcomes. The secondary outcomes are not fully described in the findings.\n",
            "\n",
            "--- Study 33/40 ---\n",
            "RCT ID: 788.0\n",
            "Title: Welfare, Work, and Wellbeing: Evidence from an Informal Settlement in Kenya...\n",
            "Score: 20.0\n",
            "Verdict: 🔴 LIKELY MISMATCH\n",
            "Reason: The findings mention \"working\" and \"psychological wellbeing\" which are not mentioned in the study description.\n",
            "⚠️ Flags: The findings seem to be about work programs and psychological wellbeing, while the study is about welfare program design and expenditures.\n",
            "\n",
            "--- Study 34/40 ---\n",
            "RCT ID: 4492.0\n",
            "Title: The Behavioralist Goes Door-To-Door: Understanding Household Technological Diffu...\n",
            "Score: 90.0\n",
            "Verdict: ✅ MATCH\n",
            "Reason: The findings mention the influence of prices and social norms on the adoption decision, which aligns with the study's abstract discussing the estimation of behavioral parameters in a structural model of residential adoption of technology, focusing on economic and psychological factors.\n",
            "⚠️ Flags: The findings do not explicitly mention social pressure and curiosity as mentioned in the study's abstract, and the intervention is not specified. However, these factors could be implied in the primary or secondary outcomes.\n",
            "\n",
            "--- Study 35/40 ---\n",
            "RCT ID: 5399.0\n",
            "Title: Is there a link between wealth inequality and deception? – An experimental analy...\n",
            "Score: 70.0\n",
            "Verdict: 🟡 LIKELY MATCH\n",
            "Reason: The findings generally align with the study description, as the study did investigate deception games with different subject pools and varying wealth.\n",
            "⚠️ Flags: However, the specific subject pools mentioned in the findings (chess players) are not explicitly stated in the study description. Additionally, the findings suggest that students were more likely to trust their opponent's messages, but the study description does not provide information on trust levels.\n",
            "\n",
            "--- Study 36/40 ---\n",
            "RCT ID: 11483.0\n",
            "Title: Experimental Evidence on the Effects of a Coursera Program on Labour Outcomes in...\n",
            "Score: 80.0\n",
            "Verdict: ✅ MATCH\n",
            "Reason: The findings seem to be partially from the same study, as they mention a treatment (Coursera access) and an increase in post-secondary education enrollment, which aligns with the study description. However, the findings do not explicitly mention the random assignment of the intervention, the Costa Rica setting, or the control group.\n",
            "⚠️ Flags: The findings do not explicitly mention the context or study design, making it difficult to confirm if they come from the same study. Also, personalized reminders are not mentioned in the findings, which were a significant part of the study description.\n",
            "\n",
            "--- Study 37/40 ---\n",
            "RCT ID: 1803.0\n",
            "Title: Impact of family planning and business trainings on private sector health provid...\n",
            "Score: 80.0\n",
            "Verdict: ✅ MATCH\n",
            "Reason: The findings mention an intervention targeted at private health providers in Nigeria, which matches the study description. However, the findings do not explicitly state that the intervention included family planning and business trainings, or that the study was a randomized experiment.\n",
            "⚠️ Flags: The findings do not explicitly confirm that the study was a randomized controlled trial (RCT), and they do not specify which healthcare and financial outcomes improved. Additionally, the findings mention an effect on business practices but no effect on revenue generation, while the study description does not mention revenue generation as an outcome. These discrepancies lower the match score.\n",
            "\n",
            "--- Study 38/40 ---\n",
            "RCT ID: 6519.0\n",
            "Title: By chance or by choice: Biased attribution of others' outcomes (Online Experimen...\n",
            "Score: 20.0\n",
            "Verdict: 🔴 LIKELY MISMATCH\n",
            "Reason: The findings mention an \"attribution bias\" related to luck and decision makers' choices, but the study description focuses on \"biased attribution of others' outcomes\" in the context of hidden decisions and investments.\n",
            "⚠️ Flags: The findings seem to be based on a general attribution bias, not directly linked to a trade-off between a decision maker's payoff and those of other individuals, as described in the study.\n",
            "\n",
            "--- Study 39/40 ---\n",
            "RCT ID: 13770.0\n",
            "Title: Electric Vehicle Charging at the Workplace: Experimental Evidence on Incentives ...\n",
            "Score: 90.0\n",
            "Verdict: ✅ MATCH\n",
            "Reason: The findings align with the study description, mentioning a field experiment at a university campus to measure the impact of environmental nudges and financial incentives on EV charging behavior.\n",
            "⚠️ Flags: The findings mention \"early to later morning\" shifting, while the study focuses on \"daytime\" charging. This discrepancy slightly lowers the match score. However, it's possible that the study still supports this finding.\n",
            "\n",
            "--- Study 40/40 ---\n",
            "RCT ID: 3402.0\n",
            "Title: Anchoring bias in markets...\n",
            "Score: 60.0\n",
            "Verdict: 🟡 LIKELY MATCH\n",
            "Reason: The findings mention the same anchoring bias concept and study title, but they do not directly match the provided study description.\n",
            "⚠️ Flags: The findings seem to contradict the original study's objective, which aims to investigate the effect of market participation on anchoring bias. The findings also mention a large sample size and a transparently uninformative anchor, which are not mentioned in the study description.\n",
            "\n",
            "==================================================\n",
            "PROGRESS: 40/40 completed\n",
            "Success: 40 | Errors: 0 | Mismatches: 7\n",
            "==================================================\n",
            "\n",
            "✅ VALIDATION COMPLETE at 01:59:37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"COMPLETE VALIDATION RESULTS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Convert results to DataFrame for analysis\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Overall statistics\n",
        "print(f\"\\n📊 OVERALL STATISTICS:\")\n",
        "print(f\"Total Studies Validated: {SAMPLE_SIZE}\")\n",
        "print(f\"Successful Validations: {success_count}\")\n",
        "print(f\"Failed Validations: {error_count}\")\n",
        "\n",
        "# Score distribution\n",
        "valid_scores = results_df[results_df['score'] >= 0]['score']\n",
        "if len(valid_scores) > 0:\n",
        "    print(f\"\\n📈 SCORE DISTRIBUTION:\")\n",
        "    print(f\"Mean Score: {valid_scores.mean():.1f}\")\n",
        "    print(f\"Median Score: {valid_scores.median():.1f}\")\n",
        "    print(f\"Min Score: {valid_scores.min():.1f}\")\n",
        "    print(f\"Max Score: {valid_scores.max():.1f}\")\n",
        "    print(f\"Std Dev: {valid_scores.std():.1f}\")\n",
        "\n",
        "# Verdict counts\n",
        "print(f\"\\n🎯 VERDICT DISTRIBUTION:\")\n",
        "verdict_counts = results_df['verdict'].value_counts()\n",
        "for verdict, count in verdict_counts.items():\n",
        "    percentage = (count / SAMPLE_SIZE) * 100\n",
        "    print(f\"{verdict}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "# Quality assessment\n",
        "print(f\"\\n📋 QUALITY ASSESSMENT:\")\n",
        "high_quality = len(results_df[results_df['score'] >= 80])\n",
        "medium_quality = len(results_df[(results_df['score'] >= 60) & (results_df['score'] < 80)])\n",
        "low_quality = len(results_df[(results_df['score'] >= 0) & (results_df['score'] < 60)])\n",
        "\n",
        "print(f\"High Quality Matches (80+): {high_quality} ({high_quality/SAMPLE_SIZE*100:.1f}%)\")\n",
        "print(f\"Medium Quality Matches (60-79): {medium_quality} ({medium_quality/SAMPLE_SIZE*100:.1f}%)\")\n",
        "print(f\"Low Quality/Mismatches (<60): {low_quality} ({low_quality/SAMPLE_SIZE*100:.1f}%)\")\n",
        "\n",
        "# Data quality grade\n",
        "if valid_scores.mean() >= 85:\n",
        "    grade = \"A - Excellent\"\n",
        "elif valid_scores.mean() >= 75:\n",
        "    grade = \"B - Good\"\n",
        "elif valid_scores.mean() >= 65:\n",
        "    grade = \"C - Acceptable\"\n",
        "elif valid_scores.mean() >= 55:\n",
        "    grade = \"D - Needs Review\"\n",
        "else:\n",
        "    grade = \"F - Major Issues\"\n",
        "\n",
        "print(f\"\\n🏆 OVERALL DATA QUALITY GRADE: {grade}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnGTOva0zan7",
        "outputId": "72801172-4877-46bb-9a36-1c3d894adeff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "COMPLETE VALIDATION RESULTS\n",
            "================================================================================\n",
            "\n",
            "📊 OVERALL STATISTICS:\n",
            "Total Studies Validated: 40\n",
            "Successful Validations: 40\n",
            "Failed Validations: 0\n",
            "\n",
            "📈 SCORE DISTRIBUTION:\n",
            "Mean Score: 71.0\n",
            "Median Score: 80.0\n",
            "Min Score: 0.0\n",
            "Max Score: 100.0\n",
            "Std Dev: 29.0\n",
            "\n",
            "🎯 VERDICT DISTRIBUTION:\n",
            "✅ MATCH: 30 (75.0%)\n",
            "🔴 LIKELY MISMATCH: 4 (10.0%)\n",
            "❌ MISMATCH: 3 (7.5%)\n",
            "🟡 LIKELY MATCH: 3 (7.5%)\n",
            "\n",
            "📋 QUALITY ASSESSMENT:\n",
            "High Quality Matches (80+): 30 (75.0%)\n",
            "Medium Quality Matches (60-79): 3 (7.5%)\n",
            "Low Quality/Mismatches (<60): 7 (17.5%)\n",
            "\n",
            "🏆 OVERALL DATA QUALITY GRADE: C - Acceptable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"PROBLEMATIC STUDIES REQUIRING REVIEW\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Filter for problematic studies\n",
        "problematic = results_df[results_df['score'] < 60].sort_values('score')\n",
        "\n",
        "if len(problematic) > 0:\n",
        "    print(f\"\\n⚠️ Found {len(problematic)} studies with potential issues (score < 60):\\n\")\n",
        "\n",
        "    for idx, row in problematic.iterrows():\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"RCT ID: {row['rct_id']}\")\n",
        "        # Print full title, wrapped if needed\n",
        "        print(f\"Title: {row['title']}\")\n",
        "        print(f\"Score: {row['score']:.1f}\")\n",
        "        print(f\"Verdict: {row['verdict']}\")\n",
        "        print(f\"Reason: {row['reason']}\")\n",
        "        if row['flags'] and row['flags'] != 'none':\n",
        "            print(f\"Flags: {row['flags']}\")\n",
        "else:\n",
        "    print(\"\\n✅ No problematic studies found! All scores >= 60\")\n",
        "\n",
        "# Also show uncertain ones with full titles\n",
        "uncertain = results_df[(results_df['score'] >= 40) & (results_df['score'] < 60)]\n",
        "if len(uncertain) > 0:\n",
        "    print(f\"\\n🟠 UNCERTAIN STUDIES (40-59 score range):\")\n",
        "    print(f\"Found {len(uncertain)} studies that may need manual review:\\n\")\n",
        "\n",
        "    for idx, row in uncertain.iterrows():\n",
        "        print(f\"- RCT {row['rct_id']}: {row['title']}\")\n",
        "        print(f\"  Score: {row['score']:.1f} | Reason: {row['reason']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cM_okZhAzfbB",
        "outputId": "347e3c26-cf29-470c-ecdf-4b4b03b8ecdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "PROBLEMATIC STUDIES REQUIRING REVIEW\n",
            "================================================================================\n",
            "\n",
            "⚠️ Found 7 studies with potential issues (score < 60):\n",
            "\n",
            "============================================================\n",
            "RCT ID: 3939.0\n",
            "Title: Salience of the energy-efficiency trade-off and purchase of energy efficient appliances\n",
            "Score: 0.0\n",
            "Verdict: ❌ MISMATCH\n",
            "Reason: The finding states that more salient energy cost information leads to purchasing less energy-efficient appliances, while the study description implies that it should lead to purchasing more energy-efficient appliances.\n",
            "Flags: None. The discrepancy is clear and direct.\n",
            "============================================================\n",
            "RCT ID: 1446.0\n",
            "Title: From natural variation to optimal policy? The importance of endogenous peer group formation.\n",
            "Score: 0.0\n",
            "Verdict: ❌ MISMATCH\n",
            "Reason: The study findings describe a negative and significant treatment effect for the students the intervention aimed to help, but the study description explains that the intervention was designed to maximize the academic performance of the lowest ability students. These outcomes are contradictory.\n",
            "Flags: The findings mention \"students we intended to help\" but the study description does not specify which students were intended to be helped. This is a red flag as it suggests a mismatch between the findings and the study description.\n",
            "============================================================\n",
            "RCT ID: 728.0\n",
            "Title: Intergenerational Impacts of Health Investments in Kenya\n",
            "Score: 0.0\n",
            "Verdict: ❌ MISMATCH\n",
            "Reason: The findings mention unconditional cash transfers, while the study describes a deworming program.\n",
            "Flags: The findings do not match the study in terms of intervention and outcomes.\n",
            "============================================================\n",
            "RCT ID: 9791.0\n",
            "Title: Attitudes towards refugees and identity\n",
            "Score: 20.0\n",
            "Verdict: 🔴 LIKELY MISMATCH\n",
            "Reason: The findings mention a study by Cristina Cattaneo, which is not the same as the study named in the study description.\n",
            "Flags: Different study author and no mention of the manipulation (information about EU immigration policy reforms) described in the study.\n",
            "============================================================\n",
            "RCT ID: 15014.0\n",
            "Title: Pilot on Complexity on Social Learning\n",
            "Score: 20.0\n",
            "Verdict: 🔴 LIKELY MISMATCH\n",
            "Reason: The findings are from related work by the same author, but they do not directly match the study description.\n",
            "Flags: The findings are from related work on a broader topic, not specifically the \"Pilot on Complexity on Social Learning\" study. The intervention described in the findings is also different from the one in the study description.\n",
            "============================================================\n",
            "RCT ID: 788.0\n",
            "Title: Welfare, Work, and Wellbeing: Evidence from an Informal Settlement in Kenya\n",
            "Score: 20.0\n",
            "Verdict: 🔴 LIKELY MISMATCH\n",
            "Reason: The findings mention \"working\" and \"psychological wellbeing\" which are not mentioned in the study description.\n",
            "Flags: The findings seem to be about work programs and psychological wellbeing, while the study is about welfare program design and expenditures.\n",
            "============================================================\n",
            "RCT ID: 6519.0\n",
            "Title: By chance or by choice: Biased attribution of others' outcomes (Online Experiments)\n",
            "Score: 20.0\n",
            "Verdict: 🔴 LIKELY MISMATCH\n",
            "Reason: The findings mention an \"attribution bias\" related to luck and decision makers' choices, but the study description focuses on \"biased attribution of others' outcomes\" in the context of hidden decisions and investments.\n",
            "Flags: The findings seem to be based on a general attribution bias, not directly linked to a trade-off between a decision maker's payoff and those of other individuals, as described in the study.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"COMPLETE VALIDATION RESULTS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Convert results to DataFrame for analysis\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Overall statistics\n",
        "print(f\"\\n📊 OVERALL STATISTICS:\")\n",
        "print(f\"Total Studies Validated: {SAMPLE_SIZE}\")\n",
        "print(f\"Successful Validations: {success_count}\")\n",
        "print(f\"Failed Validations: {error_count}\")\n",
        "\n",
        "# Score distribution\n",
        "valid_scores = results_df[results_df['score'] >= 0]['score']\n",
        "if len(valid_scores) > 0:\n",
        "    print(f\"\\n📈 SCORE DISTRIBUTION:\")\n",
        "    print(f\"Mean Score: {valid_scores.mean():.1f}\")\n",
        "    print(f\"Median Score: {valid_scores.median():.1f}\")\n",
        "    print(f\"Min Score: {valid_scores.min():.1f}\")\n",
        "    print(f\"Max Score: {valid_scores.max():.1f}\")\n",
        "    print(f\"Std Dev: {valid_scores.std():.1f}\")\n",
        "\n",
        "# Verdict counts\n",
        "print(f\"\\n🎯 VERDICT DISTRIBUTION:\")\n",
        "verdict_counts = results_df['verdict'].value_counts()\n",
        "for verdict, count in verdict_counts.items():\n",
        "    percentage = (count / SAMPLE_SIZE) * 100\n",
        "    print(f\"{verdict}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "# Quality assessment\n",
        "print(f\"\\n📋 QUALITY ASSESSMENT:\")\n",
        "high_quality = len(results_df[results_df['score'] >= 80])\n",
        "medium_quality = len(results_df[(results_df['score'] >= 60) & (results_df['score'] < 80)])\n",
        "low_quality = len(results_df[(results_df['score'] >= 0) & (results_df['score'] < 60)])\n",
        "\n",
        "print(f\"High Quality Matches (80+): {high_quality} ({high_quality/SAMPLE_SIZE*100:.1f}%)\")\n",
        "print(f\"Medium Quality Matches (60-79): {medium_quality} ({medium_quality/SAMPLE_SIZE*100:.1f}%)\")\n",
        "print(f\"Low Quality/Mismatches (<60): {low_quality} ({low_quality/SAMPLE_SIZE*100:.1f}%)\")\n",
        "\n",
        "# Data quality grade\n",
        "if valid_scores.mean() >= 85:\n",
        "    grade = \"A - Excellent\"\n",
        "elif valid_scores.mean() >= 75:\n",
        "    grade = \"B - Good\"\n",
        "elif valid_scores.mean() >= 65:\n",
        "    grade = \"C - Acceptable\"\n",
        "elif valid_scores.mean() >= 55:\n",
        "    grade = \"D - Needs Review\"\n",
        "else:\n",
        "    grade = \"F - Major Issues\"\n",
        "\n",
        "print(f\"\\n🏆 OVERALL DATA QUALITY GRADE: {grade}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnRWbcYL1sOy",
        "outputId": "868a0e88-9c9e-4362-ca6c-555436e5de4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "COMPLETE VALIDATION RESULTS\n",
            "================================================================================\n",
            "\n",
            "📊 OVERALL STATISTICS:\n",
            "Total Studies Validated: 40\n",
            "Successful Validations: 40\n",
            "Failed Validations: 0\n",
            "\n",
            "📈 SCORE DISTRIBUTION:\n",
            "Mean Score: 71.0\n",
            "Median Score: 80.0\n",
            "Min Score: 0.0\n",
            "Max Score: 100.0\n",
            "Std Dev: 29.0\n",
            "\n",
            "🎯 VERDICT DISTRIBUTION:\n",
            "✅ MATCH: 30 (75.0%)\n",
            "🔴 LIKELY MISMATCH: 4 (10.0%)\n",
            "❌ MISMATCH: 3 (7.5%)\n",
            "🟡 LIKELY MATCH: 3 (7.5%)\n",
            "\n",
            "📋 QUALITY ASSESSMENT:\n",
            "High Quality Matches (80+): 30 (75.0%)\n",
            "Medium Quality Matches (60-79): 3 (7.5%)\n",
            "Low Quality/Mismatches (<60): 7 (17.5%)\n",
            "\n",
            "🏆 OVERALL DATA QUALITY GRADE: C - Acceptable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import textwrap\n",
        "\n",
        "# Base path\n",
        "base_path = '/content/drive/MyDrive/AEA_RCT_Parsed/'\n",
        "\n",
        "# Load the full dataset\n",
        "print(\"Loading consolidated dataset...\")\n",
        "df = pd.read_csv(f'{base_path}consolidated_rct_dataset_clean.csv')\n",
        "\n",
        "# Recreate the same 40-study sample using the same random seed\n",
        "SAMPLE_SIZE = 40\n",
        "validation_df = df.sample(n=SAMPLE_SIZE, random_state=42).copy()\n",
        "print(f\"✓ Loaded validation sample of {len(validation_df)} studies\")\n",
        "\n",
        "# Define the suspicious RCT IDs to inspect\n",
        "suspicious_ids = [15014, 9791, 728]\n",
        "print(f\"\\n🔍 Inspecting RCT IDs: {suspicious_ids}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RDayyKt22iW",
        "outputId": "fadcc456-84c9-4261-c8cf-9e4a0189f40b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading consolidated dataset...\n",
            "✓ Loaded validation sample of 40 studies\n",
            "\n",
            "🔍 Inspecting RCT IDs: [15014, 9791, 728]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_wrapped(label, text, width=100, indent=4):\n",
        "    \"\"\"Print text with nice wrapping and indentation.\"\"\"\n",
        "    if pd.isna(text) or text == \"\":\n",
        "        print(f\"{label}: [Empty]\")\n",
        "    else:\n",
        "        print(f\"{label}:\")\n",
        "        wrapped = textwrap.fill(str(text), width=width, initial_indent=' '*indent,\n",
        "                               subsequent_indent=' '*indent)\n",
        "        print(wrapped)\n",
        "\n",
        "def print_study_details(row):\n",
        "    \"\"\"Print all details of a study in a readable format.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(f\"RCT ID: {row['rct_id']}\")\n",
        "    print(\"=\"*100)\n",
        "\n",
        "    # Basic Information\n",
        "    print(\"\\n📚 BASIC INFORMATION:\")\n",
        "    print(f\"Title: {row['Title']}\")\n",
        "    print(f\"Year: {row['Year']}\")\n",
        "    print(f\"Country: {row['Country']}\")\n",
        "    print(f\"Subregion: {row.get('Subregion', 'N/A')}\")\n",
        "\n",
        "    # Researchers\n",
        "    print(\"\\n👥 RESEARCHERS:\")\n",
        "    print_wrapped(\"Researchers\", row.get('Researchers', 'N/A'), width=95)\n",
        "    print_wrapped(\"Affiliations\", row.get('Researcher_affiliation', 'N/A'), width=95)\n",
        "\n",
        "    # Study Design\n",
        "    print(\"\\n🔬 STUDY DESIGN:\")\n",
        "    print_wrapped(\"Abstract\", row.get('Abstract', 'N/A'), width=95)\n",
        "    print()\n",
        "    print_wrapped(\"Description of Intervention\", row.get('Description_of_intervention', 'N/A'), width=95)\n",
        "    print()\n",
        "    print_wrapped(\"Primary Outcomes\", row.get('Primary_outcomes', 'N/A'), width=95)\n",
        "    print()\n",
        "    print_wrapped(\"Secondary Outcomes\", row.get('Secondary_outcomes', 'N/A'), width=95)\n",
        "\n",
        "    # Findings\n",
        "    print(\"\\n📊 FINDINGS:\")\n",
        "    print_wrapped(\"Findings\", row.get('Findings', 'N/A'), width=95)\n",
        "\n",
        "    # Classification\n",
        "    print(\"\\n🏷️ CLASSIFICATION:\")\n",
        "    print(f\"Keywords: {row.get('keywords', 'N/A')}\")\n",
        "    print(f\"Keywords Additional: {row.get('keywords_additional', 'N/A')}\")\n",
        "    print(f\"JEL Codes: {row.get('jel_codes', 'N/A')}\")\n",
        "    print(f\"Population: {row.get('Population', 'N/A')}\")\n",
        "\n",
        "    # Keywords subcategories\n",
        "    print(\"\\n📂 KEYWORD SUBCATEGORIES:\")\n",
        "    print(f\"Sector: {row.get('keywords_sector', 'N/A')}\")\n",
        "    print(f\"Mechanisms: {row.get('keywords_mechanisms', 'N/A')}\")\n",
        "    print(f\"Implementation: {row.get('keywords_implementation', 'N/A')}\")\n",
        "    print(f\"Context: {row.get('keywords_context', 'N/A')}\")"
      ],
      "metadata": {
        "id": "sVnOC7hh23tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter for suspicious studies\n",
        "suspicious_studies = validation_df[validation_df['rct_id'].isin(suspicious_ids)]\n",
        "\n",
        "if len(suspicious_studies) == 0:\n",
        "    print(f\"\\n❌ None of the suspicious RCT IDs {suspicious_ids} are in the validation sample!\")\n",
        "    print(\"Checking if they exist in the full dataset...\")\n",
        "\n",
        "    # Check in full dataset\n",
        "    suspicious_in_full = df[df['rct_id'].isin(suspicious_ids)]\n",
        "    if len(suspicious_in_full) > 0:\n",
        "        print(f\"✓ Found {len(suspicious_in_full)} of these studies in the full dataset\")\n",
        "        print(\"Displaying them from the full dataset instead:\\n\")\n",
        "        suspicious_studies = suspicious_in_full\n",
        "    else:\n",
        "        print(\"❌ These RCT IDs don't exist in the dataset\")\n",
        "\n",
        "# Display each study\n",
        "for idx, row in suspicious_studies.iterrows():\n",
        "    print_study_details(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVA2rgo_3Krc",
        "outputId": "97b41da2-3198-48b5-ccb0-ca3584b8ba91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====================================================================================================\n",
            "RCT ID: 9791.0\n",
            "====================================================================================================\n",
            "\n",
            "📚 BASIC INFORMATION:\n",
            "Title: Attitudes towards refugees and identity\n",
            "Year: 2022\n",
            "Country: Italy\n",
            "Subregion: nan\n",
            "\n",
            "👥 RESEARCHERS:\n",
            "Researchers:\n",
            "    Cristina Cattaneo; PI Name Daniela Grieco PI Affiliation CONTACT INVESTIGATOR PI Name Mario\n",
            "    Macis PI Affiliation CONTACT INVESTIGATOR PI Name Nicola Lacetera PI Affiliation CONTACT\n",
            "    INVESTIGATOR\n",
            "Affiliations:\n",
            "    RFF-CMCC and CMCC\n",
            "\n",
            "🔬 STUDY DESIGN:\n",
            "Abstract:\n",
            "    Using data from an incentivised online survey with nationally representative Italians, we\n",
            "    study natives’ attitudes towards refugees, some sharing a common European identity and some\n",
            "    not sharing it, being from outside Europe. The project has three objectives. First, we aim\n",
            "    to test if the way the asylum seekers are managed and relocated between the different\n",
            "    European countries, affects the attitude towards refugees. To this aim, we prime a reform\n",
            "    of the EU refugee legislative framework, which sets more equal allocation rules among\n",
            "    member states. Second, we aim to test if the way respondents react to the prime depends on\n",
            "    the nationality of the refugees. We vary the cultural/ racial distances of the refugees\n",
            "    compared to natives. Third, we test if identity concerns affect the level of trust in\n",
            "    refugees of different nationalities. We measure attitudes through real donations in favor\n",
            "    of the different groups of refugees, and we proxy the degree of trust towards the groups\n",
            "    through a choice over the composition of the donation, between in-kind and cash transfers.\n",
            "    External Link(s)\n",
            "\n",
            "Description of Intervention:\n",
            "    Intervention (Hidden)\n",
            "\n",
            "Primary Outcomes:\n",
            "    We will collect donations in favor of two distinct groups of refugees hosted in Italy. One\n",
            "    group comprises Ukrainian refugees, who should be perceived close in terms of (European)\n",
            "    identity. The other group comprises African refugees, who are likely to be perceived as\n",
            "    distant from Europeans in terms of culture and religion. In the control condition, we\n",
            "    collect donation in favour of natives victims of violence. We will also collect information\n",
            "    of the composition of the donation, between in-kind and cash transfers. Primary Outcomes\n",
            "    (explanation) SECONDARY OUTCOMES Secondary Outcomes (end points) Secondary Outcomes\n",
            "    (explanation)\n",
            "\n",
            "Secondary Outcomes:\n",
            "    Secondary Outcomes (explanation)\n",
            "\n",
            "📊 FINDINGS:\n",
            "Findings:\n",
            "    The study by Cristina Cattaneo found that Italians showed less support for\n",
            "    refugees—especially those from African countries—compared to Italian victims of violence,\n",
            "    with stronger prejudice against distant out-groups, particularly among right-leaning\n",
            "    individuals; providing information about EU immigration policy reforms did not alter these\n",
            "    attitudes[1][2][4][5][6][10].\n",
            "\n",
            "🏷️ CLASSIFICATION:\n",
            "Keywords: Behavior\n",
            "Keywords Additional: attitudes; refugess; identity; JEL code(s)\n",
            "JEL Codes: nan\n",
            "Population: nationally representative Italians, refugees, asylum seekers\n",
            "\n",
            "📂 KEYWORD SUBCATEGORIES:\n",
            "Sector: migration, social policy, public trust\n",
            "Mechanisms: identity concerns, cultural distance, legislative impact\n",
            "Implementation: priming, donation choices, survey design\n",
            "Context: European, national, online\n",
            "\n",
            "====================================================================================================\n",
            "RCT ID: 15014.0\n",
            "====================================================================================================\n",
            "\n",
            "📚 BASIC INFORMATION:\n",
            "Title: Pilot on Complexity on Social Learning\n",
            "Year: 2024\n",
            "Country: United Kingdom of Great Britain and Northern Ireland\n",
            "Subregion: nan\n",
            "\n",
            "👥 RESEARCHERS:\n",
            "Researchers:\n",
            "    Stephen Nei; PI Name Pauline Vorjohann PI Affiliation University of Exeter CONTACT\n",
            "    INVESTIGATOR\n",
            "Affiliations:\n",
            "    University of Exeter\n",
            "\n",
            "🔬 STUDY DESIGN:\n",
            "Abstract:\n",
            "    Building on a test of how individuals anticipate and account for selection of others into\n",
            "    sharing information, we run a pilot to investigate the role of complexity. By changing the\n",
            "    distribution of private signals, we make it \"obvious\" how individuals with strong signals\n",
            "    should act and so want to compare the resulting general behavior in this \"simple\" setting\n",
            "    in comparison to the previous more \"complex\" setting. External Link(s)\n",
            "\n",
            "Description of Intervention:\n",
            "    In comparison to previous stages of this project, we make the private signals easier to\n",
            "    understand Intervention (Hidden) The signals are either perfectly informative or completely\n",
            "    uninformative, whereas previous the \"strong\" signal was not perfectly revealing of the true\n",
            "    state.\n",
            "\n",
            "Primary Outcomes:\n",
            "    The difference between the \"selection\" treatment and the \"no-selection\" treatment in\n",
            "    participants' willingness to pay for the social signal and in how often they comply and\n",
            "    update their beliefs conditional on the social signal, and how this difference compares to\n",
            "    the difference found when using a more complex bag in previous rounds of the project.\n",
            "    Primary Outcomes (explanation) SECONDARY OUTCOMES Secondary Outcomes (end points) Secondary\n",
            "    Outcomes (explanation)\n",
            "\n",
            "Secondary Outcomes:\n",
            "    Secondary Outcomes (explanation)\n",
            "\n",
            "📊 FINDINGS:\n",
            "Findings:\n",
            "    No direct results for a study titled \"Pilot on Complexity on Social Learning\" by Stephen\n",
            "    Nei were found in the available search results. However, related work by Stephen Nei and\n",
            "    collaborators addresses how patterns of communication, interaction, and learning in\n",
            "    networked societies influence the evolution of behavior, social norms, and the design of\n",
            "    interventions in complex social systems[5].\n",
            "\n",
            "🏷️ CLASSIFICATION:\n",
            "Keywords: Behavior; Other\n",
            "Keywords Additional: social learning; JEL code(s)\n",
            "JEL Codes: nan\n",
            "Population: individual participants, experimental subjects\n",
            "\n",
            "📂 KEYWORD SUBCATEGORIES:\n",
            "Sector: behavioral economics, experimental economics, social psychology\n",
            "Mechanisms: anticipation, strategic thinking, cognitive load\n",
            "Implementation: signal alteration, experimental manipulation, participant observation\n",
            "Context: laboratory setting, controlled environment\n",
            "\n",
            "====================================================================================================\n",
            "RCT ID: 728.0\n",
            "====================================================================================================\n",
            "\n",
            "📚 BASIC INFORMATION:\n",
            "Title: Intergenerational Impacts of Health Investments in Kenya\n",
            "Year: 2015\n",
            "Country: Kenya\n",
            "Subregion: nan\n",
            "\n",
            "👥 RESEARCHERS:\n",
            "Researchers:\n",
            "    Edward Miguel\n",
            "Affiliations:\n",
            "    University of California, Berkeley\n",
            "\n",
            "🔬 STUDY DESIGN:\n",
            "Abstract:\n",
            "    This project will create a dataset consisting of the children of individuals who themselves\n",
            "    previously benefited from a randomized health (deworming) program. The project will exploit\n",
            "    experimental variation to estimate the causal impact of this earlier program on the health\n",
            "    and cognitive development of the recipients’ children, overcoming the key methodological\n",
            "    problem of confounding. The project will survey approximately 1,500 children of the 7,500\n",
            "    respondents in the Kenya Life Panel Survey (KLPS), creating the new KLPS-Kids dataset, to\n",
            "    estimate the extent to which a health program can help break the intergenerational\n",
            "    transmission of poverty. External Link(s)\n",
            "\n",
            "Description of Intervention:\n",
            "    In 1998, a local non-governmental organization (NGO) launched a program known as the\n",
            "    Primary School Deworming Program (PSDP) to provide deworming medication to individuals\n",
            "    enrolled in 75 primary schools in Busia District, a densely-settled farming region of rural\n",
            "    western Kenya adjacent to Lake Victoria. The schools participating in the program consisted\n",
            "    of nearly all rural primary schools in Budalangi and Funyula divisions in southern Busia\n",
            "    district, and contained more than 30,000 pupils at the start of the study. Baseline\n",
            "    parasitological surveys conducted by the Kenyan Ministry of Health indicated that these\n",
            "    divisions had high rates of helminth infection at over 90%. Using modified WHO infection\n",
            "    thresholds (Brooker et al., 2000b), roughly one-third of children in the sample had\n",
            "    “moderate to heavy” infections with at least one helminth at the time of the baseline\n",
            "    survey, a rate not atypical by regional standards (Brooker et al., 2000a). The 1998 Kenya\n",
            "    DHS indicated that 85% of children in western Kenya, in the relevant age range of 8-18\n",
            "    years, were enrolled in school – suggesting that the sample was broadly representative of\n",
            "    western Kenyan children as a whole at the time. The 75 program schools were randomly\n",
            "    divided into three groups (Groups 1, 2, and 3) of 25 schools each: the schools were\n",
            "    stratified by geographical area (division, then zone), the zones were listed alphabetically\n",
            "    (within each division), and then within each zone the schools were listed in increasing\n",
            "    order of student enrolment, and every third school was assigned to a given project group.\n",
            "    Due to the NGO’s administrative and financial constraints, the schools were phased into the\n",
            "    program over the course of 1998-2001, and the order of phase-in was randomly determined,\n",
            "    creating experimental treatment groups. This prospective design is central to the present\n",
            "    study’s analytical strategy. Group 1 schools began receiving free deworming in 1998, Group\n",
            "    2 schools in 1999, while Group 3 schools began receiving the drugs in 2001. The project\n",
            "    design implies that in 1998, Group 1 schools were treatment schools while Group 2 and 3\n",
            "    schools were the control, and in 1999 and 2000, Group 1 and 2 schools were the treatment\n",
            "    schools and Group 3 schools the control, and so on. In 2002 all schools received free\n",
            "    treatment. Children in Group 1 and 2 schools thus received two to three more years of\n",
            "    deworming than Group 3 children, and these early beneficiaries are what we call the\n",
            "    deworming treatment group (parents) in the present study. Deworming drugs were offered\n",
            "    twice per year in treatment schools. Analysis during the first two years of the\n",
            "    intervention show large, positive gains in height, self-reported health and school\n",
            "    attendance of the program beneficiaries (Miguel and Kremer 2004). Intervention (Hidden)\n",
            "\n",
            "Primary Outcomes:\n",
            "    Our key outcome variables will be separate domains of cognitive development (including\n",
            "    sequential processing and short-term memory, visual-construction ability and spatial\n",
            "    relationships), language, fine motor skills, socio-emotional development, and height.\n",
            "    Primary Outcomes (explanation) SECONDARY OUTCOMES Secondary Outcomes (end points) Secondary\n",
            "    Outcomes (explanation)\n",
            "\n",
            "Secondary Outcomes:\n",
            "    Secondary Outcomes (explanation)\n",
            "\n",
            "📊 FINDINGS:\n",
            "Findings:\n",
            "    The study \"Intergenerational Impacts of Health Investments in Kenya\" by Edward Miguel and\n",
            "    collaborators finds that health investments during childhood and adolescence have\n",
            "    significant long-term and intergenerational effects, improving recipients' adult living\n",
            "    standards, cognitive development, and the health of their children. Specifically,\n",
            "    unconditional cash transfers in rural Kenya led to a 40-48% reduction in infant mortality,\n",
            "    primarily by increasing access to healthcare around pregnancy and childbirth, with the\n",
            "    greatest benefits seen among households near medical facilities and when transfers were\n",
            "    timed during pregnancy or infancy. These findings highlight the importance of combining\n",
            "    financial support with healthcare access to break the intergenerational cycle of poverty\n",
            "    and improve health outcomes[1][2][3][8].\n",
            "\n",
            "🏷️ CLASSIFICATION:\n",
            "Keywords: Health; Welfare\n",
            "Keywords Additional: Children; Cognition; Intergenerational; JEL code(s)\n",
            "JEL Codes: nan\n",
            "Population: nan\n",
            "\n",
            "📂 KEYWORD SUBCATEGORIES:\n",
            "Sector: nan\n",
            "Mechanisms: nan\n",
            "Implementation: nan\n",
            "Context: nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Import necessary library\n",
        "import pandas as pd\n",
        "\n",
        "# Load the primary dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/AEA_RCT_Parsed/structured_studies_full_v2.csv')\n",
        "\n",
        "# Define the AI-generated keyword columns\n",
        "ai_keyword_columns = ['keywords_sector', 'keywords_mechanisms',\n",
        "                      'keywords_implementation', 'keywords_context']\n",
        "\n",
        "# Create a boolean mask for studies with AI-generated keywords\n",
        "# A study has AI keywords if at least one of the AI keyword columns is not null/empty\n",
        "has_ai_keywords_mask = df[ai_keyword_columns].notna().any(axis=1)\n",
        "\n",
        "# For string columns, also check they're not empty strings\n",
        "for col in ai_keyword_columns:\n",
        "    if col in df.columns and df[col].dtype == 'object':\n",
        "        has_ai_keywords_mask = has_ai_keywords_mask & (df[col] != '')\n",
        "\n",
        "# Split the dataset into two groups\n",
        "studies_with_ai_keywords = df[has_ai_keywords_mask].copy()\n",
        "studies_without_ai_keywords = df[~has_ai_keywords_mask].copy()\n",
        "\n",
        "# Display counts for verification\n",
        "print(f\"Total studies: {len(df)}\")\n",
        "print(f\"Studies WITH AI-generated keywords: {len(studies_with_ai_keywords)}\")\n",
        "print(f\"Studies WITHOUT AI-generated keywords: {len(studies_without_ai_keywords)}\")\n",
        "\n",
        "# Save the two files\n",
        "studies_with_ai_keywords.to_csv('/content/drive/MyDrive/AEA_RCT_Parsed/studies_with_ai_keywords.csv',\n",
        "                                index=False)\n",
        "studies_without_ai_keywords.to_csv('/content/drive/MyDrive/AEA_RCT_Parsed/studies_without_ai_keywords.csv',\n",
        "                                   index=False)\n",
        "\n",
        "print(\"\\nFiles saved successfully:\")\n",
        "print(\"1. studies_with_ai_keywords.csv\")\n",
        "print(\"2. studies_without_ai_keywords.csv\")\n",
        "\n",
        "# Optional: Display sample data to verify the split\n",
        "print(\"\\n--- Sample of studies WITH AI keywords ---\")\n",
        "print(studies_with_ai_keywords[['rct_id', 'Title'] + ai_keyword_columns].head(3))\n",
        "\n",
        "print(\"\\n--- Sample of studies WITHOUT AI keywords ---\")\n",
        "print(studies_without_ai_keywords[['rct_id', 'Title'] + ai_keyword_columns].head(3))"
      ],
      "metadata": {
        "id": "UHi8Pdh-3yw9",
        "outputId": "b3480dd9-49d9-48cb-b9cf-2150021ec93f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Total studies: 2289\n",
            "Studies WITH AI-generated keywords: 2117\n",
            "Studies WITHOUT AI-generated keywords: 172\n",
            "\n",
            "Files saved successfully:\n",
            "1. studies_with_ai_keywords.csv\n",
            "2. studies_without_ai_keywords.csv\n",
            "\n",
            "--- Sample of studies WITH AI keywords ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['Title'] not in index\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3767030930.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# Optional: Display sample data to verify the split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Sample of studies WITH AI keywords ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudies_with_ai_keywords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rct_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Title'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mai_keyword_columns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Sample of studies WITHOUT AI keywords ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6251\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6252\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6254\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['Title'] not in index\""
          ]
        }
      ]
    }
  ]
}