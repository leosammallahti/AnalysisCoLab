{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsh9bOzXAO7FYFu6HAATF8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leosammallahti/AnalysisCoLab/blob/main/RCTAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSqHmiP-am36",
        "outputId": "b8b38061-e69b-4111-df12-52c9b99ac6f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Loaded 2289 studies\n",
            "Columns: ['rct_id', 'title', 'final_has_findings', 'final_findings_snippet', 'final_findings_source_type', 'country', 'start_date', 'end_date', 'primary_investigator_name', 'origin_url', 'initial_registration_date', 'first_published', 'last_updated', 'region', 'status', 'keywords', 'keywords_additional', 'jel_codes', 'secondary_ids', 'abstract', 'registration_citation', 'interventions', 'intervention_hidden', 'intervention_start_date', 'intervention_end_date', 'primary_outcomes_endpoints', 'primary_outcomes_explanation', 'secondary_outcomes_endpoints', 'secondary_outcomes_explanation', 'experimental_design', 'experimental_design_details', 'randomization_method', 'randomization_unit', 'treatment_clustered', 'sample_size_clusters', 'sample_size_observations', 'sample_size_by_arm', 'minimum_detectable_effect', 'external_links', 'emails_found', 'primary_investigator_affiliation', 'other_pi_1_name', 'other_pi_1_affiliation', 'relevant_1_abstract', 'relevant_1_preliminary', 'relevant_1_citation', 'relevant_1_url', 'relevant_2_abstract', 'relevant_2_preliminary', 'relevant_2_citation', 'relevant_2_url', 'other_pi_2_name', 'other_pi_2_affiliation', 'other_pi_3_name', 'other_pi_3_affiliation', 'relevant_3_abstract', 'relevant_3_preliminary', 'relevant_3_citation', 'relevant_3_url', 'reports_1_description', 'reports_1_citation', 'reports_1_url', 'relevant_4_abstract', 'relevant_4_preliminary', 'relevant_4_citation', 'relevant_4_url', 'relevant_5_abstract', 'relevant_5_preliminary', 'relevant_5_citation', 'relevant_5_url', 'reports_2_description', 'reports_2_citation', 'reports_2_url', 'reports_3_description', 'reports_3_citation', 'reports_3_url', 'reports_4_description', 'reports_4_citation', 'reports_4_url', 'reports_5_description', 'reports_5_citation', 'reports_5_url', 'relevant_6_abstract', 'relevant_6_preliminary', 'relevant_6_citation', 'relevant_6_url', 'relevant_7_abstract', 'relevant_7_preliminary', 'relevant_7_citation', 'relevant_7_url', 'other_pi_4_name', 'other_pi_4_affiliation', 'has_findings', 'findings_source', 'findings_snippet', 'findings_fulltext', 'findings_confidence', 'llm_is_finding', 'llm_snippet', 'llm_source', 'source_file', 'sonar_confidence', 'snippet_len', 'url_present', 'final_findings_url', 'merge_note', 'publication_year', 'abstract_source', 'tldr', 'tldr_model', 'tldr_error', 'country_extracted', 'intervention_type', 'outcome_extracted', 'population_extracted', 'keywords_methodology', 'keywords_sector', 'keywords_mechanisms', 'keywords_implementation', 'keywords_context', 'keywords_partners', 'search_vector']\n",
            "After filtering inadequate abstracts: 2289 studies\n",
            "\n",
            "Sample of key fields:\n",
            "                                               title  \\\n",
            "0            Two Approaches to Community Development   \n",
            "1  Consumer Response to New Retirement Income Pro...   \n",
            "2  Bank-Insured RoSCA for Microfinance: Experimen...   \n",
            "3  Transaction Costs, Bargaining Power, and Savin...   \n",
            "4  How Does Part-time Work Affect Labor Productiv...   \n",
            "\n",
            "                                   intervention_type  \\\n",
            "0  community driven development; technocratic int...   \n",
            "1  information presentation, consumer education, ...   \n",
            "2  microfinance; credit unions; bank-insured RoSC...   \n",
            "3  ATM cards; reduced withdrawal fees; increased ...   \n",
            "4  part-time work; full-time work; labor contract...   \n",
            "\n",
            "                                   outcome_extracted country_extracted  \n",
            "0  long run effects; community response; public s...      Sierra Leone  \n",
            "1  consumer comprehension, preference alignment, ...         Australia  \n",
            "2  take up rates; repayment rates; credit access;...             Egypt  \n",
            "3  account use; financial behavior; savings; intr...             Kenya  \n",
            "4  worker selection; labor productivity; employme...          Ethiopia  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from collections import Counter\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set up paths\n",
        "base_path = '/content/drive/MyDrive/AEA_RCT_Parsed/'\n",
        "\n",
        "# Load the enriched dataset with AI-generated fields\n",
        "df = pd.read_csv(base_path + 'structured_studies_full_v2.csv')\n",
        "\n",
        "print(f\"Loaded {len(df)} studies\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "\n",
        "# Filter out studies with inadequate abstracts (as mentioned in your pipeline)\n",
        "# These are marked with \"No abstract available\" in tldr_error\n",
        "df_clean = df[df['tldr_error'] != 'No abstract available'].copy()\n",
        "print(f\"After filtering inadequate abstracts: {len(df_clean)} studies\")\n",
        "\n",
        "# Quick data inspection\n",
        "print(\"\\nSample of key fields:\")\n",
        "print(df_clean[['title', 'intervention_type', 'outcome_extracted', 'country_extracted']].head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# INTERVENTION DNA ANALYSIS\n",
        "# Breaking down interventions into their component parts\n",
        "\n",
        "def extract_intervention_dna(df):\n",
        "    \"\"\"\n",
        "    Extract binary features for intervention components\n",
        "    Using intervention_structured field (or fallback to other intervention fields)\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a combined intervention text field for analysis\n",
        "    df['intervention_text'] = df['intervention_structured'].fillna('') + ' ' + \\\n",
        "                               df['interventions'].fillna('') + ' ' + \\\n",
        "                               df['intervention_type'].fillna('')\n",
        "\n",
        "    # Define DNA components with search patterns\n",
        "    dna_patterns = {\n",
        "        'cash_transfer': r'cash|money|payment|transfer|grant|subsidy|unconditional|CCT|UCT',\n",
        "        'training': r'training|education|workshop|teach|capacity building|skill|curriculum',\n",
        "        'digital_tech': r'mobile|SMS|app|digital|phone|internet|online|platform|software',\n",
        "        'health_service': r'health|medical|clinic|doctor|nurse|vaccine|treatment|medicine',\n",
        "        'information': r'information|awareness|campaign|messaging|communication|media',\n",
        "        'microfinance': r'credit|loan|savings|microfinance|microcredit|financial',\n",
        "        'infrastructure': r'infrastructure|building|road|water|electricity|sanitation|construction',\n",
        "        'nudge_behavioral': r'nudge|reminder|behavioral|psychology|framing|social norm',\n",
        "        'community_based': r'community|group|collective|peer|social|network|participat',\n",
        "        'governance': r'governance|accountability|transparency|corruption|monitoring|audit',\n",
        "        'agriculture': r'agricultur|farm|crop|seed|fertilizer|irrigation|livestock',\n",
        "        'women_focused': r'women|female|gender|girl|maternal|empowerment'\n",
        "    }\n",
        "\n",
        "    # Extract DNA components\n",
        "    for component, pattern in dna_patterns.items():\n",
        "        df[f'dna_{component}'] = df['intervention_text'].str.contains(\n",
        "            pattern, case=False, na=False\n",
        "        ).astype(int)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply DNA extraction\n",
        "df_dna = extract_intervention_dna(df_clean)\n",
        "\n",
        "# Get DNA columns\n",
        "dna_cols = [col for col in df_dna.columns if col.startswith('dna_')]\n",
        "\n",
        "# Calculate DNA statistics\n",
        "dna_stats = pd.DataFrame({\n",
        "    'Component': [col.replace('dna_', '').replace('_', ' ').title() for col in dna_cols],\n",
        "    'Count': [df_dna[col].sum() for col in dna_cols],\n",
        "    'Percentage': [df_dna[col].mean() * 100 for col in dna_cols]\n",
        "}).sort_values('Count', ascending=False)\n",
        "\n",
        "# Visualize DNA components\n",
        "fig_dna = px.bar(dna_stats,\n",
        "                 x='Count',\n",
        "                 y='Component',\n",
        "                 orientation='h',\n",
        "                 title='Intervention DNA: Most Common Components in RCTs',\n",
        "                 labels={'Count': 'Number of Studies', 'Component': 'Intervention Component'},\n",
        "                 color='Percentage',\n",
        "                 color_continuous_scale='Viridis',\n",
        "                 text='Count')\n",
        "\n",
        "fig_dna.update_traces(texttemplate='%{text} (%{color:.1f}%)', textposition='outside')\n",
        "fig_dna.update_layout(height=600, width=900)\n",
        "fig_dna.show()\n",
        "\n",
        "print(\"\\nIntervention DNA Statistics:\")\n",
        "print(dna_stats.to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "YQb0tqkvbZ9p",
        "outputId": "af0debdf-fb8b-4066-c76e-2005f5bac10b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'intervention_structured'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'intervention_structured'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2381493660.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Apply DNA extraction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mdf_dna\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_intervention_dna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_clean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Get DNA columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2381493660.py\u001b[0m in \u001b[0;36mextract_intervention_dna\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Create a combined intervention text field for analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'intervention_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'intervention_structured'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                                \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'interventions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'intervention_type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'intervention_structured'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, let's see what columns we actually have\n",
        "print(\"Available columns in the dataset:\")\n",
        "print(df_clean.columns.tolist())\n",
        "\n",
        "# Let's specifically look for intervention-related columns\n",
        "intervention_cols = [col for col in df_clean.columns if 'intervention' in col.lower()]\n",
        "print(\"\\nIntervention-related columns found:\")\n",
        "print(intervention_cols)\n",
        "\n",
        "# Let's also check for abstract and findings columns\n",
        "abstract_cols = [col for col in df_clean.columns if 'abstract' in col.lower()]\n",
        "findings_cols = [col for col in df_clean.columns if 'finding' in col.lower()]\n",
        "\n",
        "print(\"\\nAbstract-related columns:\")\n",
        "print(abstract_cols)\n",
        "print(\"\\nFindings-related columns:\")\n",
        "print(findings_cols)\n",
        "\n",
        "# Look at a sample of the data to understand the structure\n",
        "print(\"\\nSample of intervention data:\")\n",
        "for col in intervention_cols[:3]:  # Show first 3 intervention columns\n",
        "    print(f\"\\n{col}:\")\n",
        "    print(df_clean[col].dropna().head(2).tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0IcoG9vb0gE",
        "outputId": "97165138-66cb-4cdc-dc10-da1d54588378"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available columns in the dataset:\n",
            "['rct_id', 'title', 'final_has_findings', 'final_findings_snippet', 'final_findings_source_type', 'country', 'start_date', 'end_date', 'primary_investigator_name', 'origin_url', 'initial_registration_date', 'first_published', 'last_updated', 'region', 'status', 'keywords', 'keywords_additional', 'jel_codes', 'secondary_ids', 'abstract', 'registration_citation', 'interventions', 'intervention_hidden', 'intervention_start_date', 'intervention_end_date', 'primary_outcomes_endpoints', 'primary_outcomes_explanation', 'secondary_outcomes_endpoints', 'secondary_outcomes_explanation', 'experimental_design', 'experimental_design_details', 'randomization_method', 'randomization_unit', 'treatment_clustered', 'sample_size_clusters', 'sample_size_observations', 'sample_size_by_arm', 'minimum_detectable_effect', 'external_links', 'emails_found', 'primary_investigator_affiliation', 'other_pi_1_name', 'other_pi_1_affiliation', 'relevant_1_abstract', 'relevant_1_preliminary', 'relevant_1_citation', 'relevant_1_url', 'relevant_2_abstract', 'relevant_2_preliminary', 'relevant_2_citation', 'relevant_2_url', 'other_pi_2_name', 'other_pi_2_affiliation', 'other_pi_3_name', 'other_pi_3_affiliation', 'relevant_3_abstract', 'relevant_3_preliminary', 'relevant_3_citation', 'relevant_3_url', 'reports_1_description', 'reports_1_citation', 'reports_1_url', 'relevant_4_abstract', 'relevant_4_preliminary', 'relevant_4_citation', 'relevant_4_url', 'relevant_5_abstract', 'relevant_5_preliminary', 'relevant_5_citation', 'relevant_5_url', 'reports_2_description', 'reports_2_citation', 'reports_2_url', 'reports_3_description', 'reports_3_citation', 'reports_3_url', 'reports_4_description', 'reports_4_citation', 'reports_4_url', 'reports_5_description', 'reports_5_citation', 'reports_5_url', 'relevant_6_abstract', 'relevant_6_preliminary', 'relevant_6_citation', 'relevant_6_url', 'relevant_7_abstract', 'relevant_7_preliminary', 'relevant_7_citation', 'relevant_7_url', 'other_pi_4_name', 'other_pi_4_affiliation', 'has_findings', 'findings_source', 'findings_snippet', 'findings_fulltext', 'findings_confidence', 'llm_is_finding', 'llm_snippet', 'llm_source', 'source_file', 'sonar_confidence', 'snippet_len', 'url_present', 'final_findings_url', 'merge_note', 'publication_year', 'abstract_source', 'tldr', 'tldr_model', 'tldr_error', 'country_extracted', 'intervention_type', 'outcome_extracted', 'population_extracted', 'keywords_methodology', 'keywords_sector', 'keywords_mechanisms', 'keywords_implementation', 'keywords_context', 'keywords_partners', 'search_vector']\n",
            "\n",
            "Intervention-related columns found:\n",
            "['interventions', 'intervention_hidden', 'intervention_start_date', 'intervention_end_date', 'intervention_type']\n",
            "\n",
            "Abstract-related columns:\n",
            "['abstract', 'relevant_1_abstract', 'relevant_2_abstract', 'relevant_3_abstract', 'relevant_4_abstract', 'relevant_5_abstract', 'relevant_6_abstract', 'relevant_7_abstract', 'abstract_source']\n",
            "\n",
            "Findings-related columns:\n",
            "['final_has_findings', 'final_findings_snippet', 'final_findings_source_type', 'has_findings', 'findings_source', 'findings_snippet', 'findings_fulltext', 'findings_confidence', 'llm_is_finding', 'final_findings_url']\n",
            "\n",
            "Sample of intervention data:\n",
            "\n",
            "interventions:\n",
            "['This study covers two distinct experimental interventions.\\n1. Long run effects of CDD: The first is a community driven development (CDD) project that was implemented as a large scale randomized trial from 2005 to 2009 in Sierra Leone. CDD is a participatory approach popular with foreign aid donors that involves communities directly in the financial management and implementation of local public goods. CDD has three aims: i) improve the stock and quality of local public goods via the provision of block grants ($5,000 per community); ii) democratize local decision-making via intensive social facilitation focused on the participation of marginalized groups; and iii) establish ties with local government that can help communities avail of external funding opportunities. In earlier work, we analyzed the medium run effects of the \"GoBifo\" CDD project, and found substantial positive impacts on local public goods and economic activity, stronger links between the community and local government, but no evidence for more inclusive local decision-making (see Casey, Glennerster and Miguel QJE 2012).\\nWe now plan to revisit the 236 communities in the original study to assess long run effects. Specifically, we will evaluate whether the \"hardware\" effects on public goods and economic activity persist several years later; assess any late onset institutional change, which may have been triggered by follow on CDD projects that were subsequently implemented in 60 of the treated villages; and also test whether the CDD experience equipped communities to respond more effectively to new crises, with a focus on local response to the Ebola epidemic.\\n2. Managerial capital experiment: The second intervention is a light touch technocratic alternative to CDD. It aims to leverage two dimensions of managerial capital - the selection of project managers and the availability of management training - to bolster community ability to take advantage of public funds available for small scale infrastructure projects. Communities in our study areas will have an opportunity to enter a project challenge competition run by local government that awards US$2,500 implementation grants to the best project proposals. The intervention aims to help communities mobilize in response to the new funding opportunity by guiding them through a process that identifies high competence members, uses short tests to provide the community with information on concrete measures of their actual managerial ability, and offers training to augment the managerial capital that exists.\\nMore specifically, the intervention involves one day visits to communities during which a field enumerator advertises the project challenge competition, explains the skills needed to submit a successful proposal, and asks a group of local leaders to deliberate and nominate 5 residents (in addition to the village headman, who is the traditional leader) who are strong on these dimensions. Enumerators then administer a short test to each of these nominees (in private), which is designed to measure their management skills. It includes questions on basic math and writing, costing materials commonly used in small scale infrastructure, and previous management experience. Next steps vary across three treatment arms. In the status quo (SQ) arm, the local traditional authority is designated to lead the project proposal process. In the manager selection arm (MS), the community is informed about which of 5 nominated individuals scored highest on the test and this person is designated to be the project proposal leader. The final manager selection plus training arm (MS+T) follows the MS arm, and further includes the offer of no cost training in basic project management skills offered in collaboration with the local government.\\nIntervention (Hidden)', 'When individuals reach retirement, they have to make complex financial decisions about how to manage their retirement savings so that they get the most income for the rest of their lives. Currently it is common for retirees to choose to receive their superannuation as an account-based pension: an accessible managed fund from which they can draw annual income in amounts they choose. Most retirees appear to be drawing from these accounts at or near the regulated minimum rates, presumably in order to manage longevity risk.\\nThe Australian Government has accepted the Financial System Inquiry recommendation to facilitate trustees pre-selecting Comprehensive Income Products for Retirement (CIPRs) for their members. This is aimed to help guide members at retirement and improve outcomes for retirees, including through increased private retirement incomes, increased consumer choice and better protection against longevity and other risks.\\nThis study aims to provide insights about whether consumers can understand a hypothetical new type of retirement income plan (an example of a potential CIPR) and make choices that fit with their preferences, testing various methods for presenting the key information about the plan.\\nAn online experimental survey will be used to present a hypothetical ‘offer’ of a potential new retirement income plan to participants. Eligible participants will be aged 45 and above and still in the superannuation accumulation phase with either active or inactive accounts, with a wide range of superannuation balances. The trial approximates a “framed field experiment” in which subjects are drawn from the population of interest and complete a familiar task (review the offer of a new financial product) in a naturally-occurring context, but understand that in this instance they are completing the task as part of a research study.\\nIndividuals who agree to participate in the study will be randomly assigned to view different textual and/or graphical explanations of a retirement income plan with comparisons between this plan and a standard account-based pension. The different treatments are design to test how people respond to the presentation of the same information in text form versus graphical and numerical forms, whether the addition of star ratings improves understanding, and the impact of a salience prompt aimed at emphasizing one dimension of the choice (average income). Subjects will then be asked to indicate, hypothetically, how willing they would be to accept such a plan if it were offered to them by their superannuation fund. They will also be asked a set of questions to measure understanding of the alternative plans and the trade-offs involved in making a choice between them, as well as an open-ended question asking members to explain their choice. The survey should take no more than 15 minutes to complete.\\nIntervention (Hidden)\\nThe Australian Government has accepted the Financial System Inquiry recommendation to facilitate trustees pre-selecting Comprehensive Income Products for Retirement (CIPRs) for their members. This is aimed to help guide members at retirement and improve outcomes for retirees, including through increased private retirement incomes, increased consumer choice and better protection against longevity and other risks. CIPRs will be new composite retirement income products offered by superannuation funds and other product providers.\\nAs CIPRs are yet to be developed by the superannuation sector; there is no potential for a real-world field experiment. Thus, an online experimental survey will be used to present a hypothetical ‘offer’ of a potential new retirement income plan to participants. The preamble to the survey will explain that the Government is interested in learning what people think about this product prior to help inform development of a framework to facilitate their offering by superannuation funds and other product providers.\\nAn online experimental survey will be used to present a hypothetical ‘offer’ of a potential new retirement income plan to participants. Eligible participants will be aged 45 and above and still in the superannuation accumulation phase with either active or inactive accounts, with a wide range of superannuation balances. The trial approximates a “framed field experiment” in which subjects are drawn from the population of interest and complete a familiar task (review the offer of a new financial product) in a naturally-occurring context, but understand that in this instance they are completing the task as part of a research study.\\nIndividuals who agree to participate in the study will be randomly assigned to view different textual and/or graphical explanations of a retirement income plan with comparisons between this plan and a standard account-based pension. The different treatments are design to test how people respond to the presentation of the same information in text form versus graphical and numerical forms, whether the addition of star ratings improves understanding, and the impact of a salience prompt aimed at emphasizing one dimension of the choice (average income). Subjects will then be asked to indicate, hypothetically, how willing they would be to accept such a plan if it were offered to them by their superannuation fund. They will also be asked a set of questions to measure understanding of the alternative plans and the trade-offs involved in making a choice between them, as well as an open-ended question asking members to explain their choice. The survey should take no more than 15 minutes to complete.']\n",
            "\n",
            "intervention_hidden:\n",
            "['The Australian Government has accepted the Financial System Inquiry recommendation to facilitate trustees pre-selecting Comprehensive Income Products for Retirement (CIPRs) for their members. This is aimed to help guide members at retirement and improve outcomes for retirees, including through increased private retirement incomes, increased consumer choice and better protection against longevity and other risks. CIPRs will be new composite retirement income products offered by superannuation funds and other product providers.\\nAs CIPRs are yet to be developed by the superannuation sector; there is no potential for a real-world field experiment. Thus, an online experimental survey will be used to present a hypothetical ‘offer’ of a potential new retirement income plan to participants. The preamble to the survey will explain that the Government is interested in learning what people think about this product prior to help inform development of a framework to facilitate their offering by superannuation funds and other product providers.\\nAn online experimental survey will be used to present a hypothetical ‘offer’ of a potential new retirement income plan to participants. Eligible participants will be aged 45 and above and still in the superannuation accumulation phase with either active or inactive accounts, with a wide range of superannuation balances. The trial approximates a “framed field experiment” in which subjects are drawn from the population of interest and complete a familiar task (review the offer of a new financial product) in a naturally-occurring context, but understand that in this instance they are completing the task as part of a research study.\\nIndividuals who agree to participate in the study will be randomly assigned to view different textual and/or graphical explanations of a retirement income plan with comparisons between this plan and a standard account-based pension. The different treatments are design to test how people respond to the presentation of the same information in text form versus graphical and numerical forms, whether the addition of star ratings improves understanding, and the impact of a salience prompt aimed at emphasizing one dimension of the choice (average income). Subjects will then be asked to indicate, hypothetically, how willing they would be to accept such a plan if it were offered to them by their superannuation fund. They will also be asked a set of questions to measure understanding of the alternative plans and the trade-offs involved in making a choice between them, as well as an open-ended question asking members to explain their choice. The survey should take no more than 15 minutes to complete.', 'Participants will be exposed to one of 4 conditions that mimic current requirements to obtain an exemption from school-entry immunization mandates:\\n1. Provide a parent signature\\n2. Check boxes corresponding to specific vaccines for which an exemption is being sought.\\n3. View an educational module\\n4. Write a statement justifying the exemption request.']\n",
            "\n",
            "intervention_start_date:\n",
            "['2016-08-19', '2016-08-19']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# INTERVENTION DNA ANALYSIS - Using YOUR AI-GENERATED KEYWORDS\n",
        "# This leverages the work you've already done!\n",
        "\n",
        "# First, let's examine what AI-generated fields we have\n",
        "print(\"AI-Generated Fields Available:\")\n",
        "ai_fields = ['intervention_type', 'outcome_extracted', 'population_extracted',\n",
        "             'country_extracted', 'tldr']\n",
        "\n",
        "# Also check for the enhanced keyword columns\n",
        "keyword_cols = [col for col in df_clean.columns if 'keywords' in col.lower()]\n",
        "print(f\"\\nKeyword columns: {keyword_cols}\")\n",
        "\n",
        "# Let's look at samples of these AI-extracted fields\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SAMPLES OF AI-EXTRACTED INTERVENTION TYPES:\")\n",
        "print(\"=\"*60)\n",
        "intervention_types = df_clean['intervention_type'].dropna().value_counts().head(20)\n",
        "print(intervention_types)\n",
        "\n",
        "# Now let's analyze the distribution and patterns\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ANALYZING INTERVENTION PATTERNS FROM AI KEYWORDS:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create intervention DNA from the AI-extracted intervention_type field\n",
        "def create_dna_from_ai_keywords(df):\n",
        "    \"\"\"\n",
        "    Create intervention DNA using the AI-extracted intervention_type field\n",
        "    Instead of guessing keywords, we use what the AI already identified\n",
        "    \"\"\"\n",
        "\n",
        "    # Get all unique intervention types (these are AI-generated)\n",
        "    all_interventions = df['intervention_type'].dropna().str.lower().unique()\n",
        "\n",
        "    print(f\"Found {len(all_interventions)} unique AI-identified intervention types\")\n",
        "\n",
        "    # Group similar interventions using the AI's own categorizations\n",
        "    # We can look for common themes in what the AI extracted\n",
        "    intervention_categories = {}\n",
        "\n",
        "    # Count frequency of key terms in the AI-generated intervention types\n",
        "    from collections import Counter\n",
        "\n",
        "    # Split all intervention types into words and count\n",
        "    all_words = []\n",
        "    for intervention in all_interventions:\n",
        "        words = str(intervention).lower().split()\n",
        "        all_words.extend(words)\n",
        "\n",
        "    word_freq = Counter(all_words)\n",
        "\n",
        "    # Get the most common intervention terms (as identified by AI)\n",
        "    common_terms = word_freq.most_common(30)\n",
        "\n",
        "    print(\"\\nMost common terms in AI-extracted intervention types:\")\n",
        "    for term, count in common_terms[:15]:\n",
        "        if len(term) > 3:  # Skip short words\n",
        "            print(f\"  {term}: {count} occurrences\")\n",
        "\n",
        "    return df, common_terms\n",
        "\n",
        "df_analyzed, intervention_terms = create_dna_from_ai_keywords(df_clean)\n",
        "\n",
        "# Analyze the enhanced keywords if available\n",
        "if 'keywords_methodology' in df_clean.columns:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ANALYZING ENHANCED AI KEYWORDS:\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # These are your 12 categories of enhanced keywords\n",
        "    enhanced_categories = [\n",
        "        'keywords_methodology', 'keywords_sector', 'keywords_mechanisms',\n",
        "        'keywords_implementation', 'keywords_context', 'keywords_partners'\n",
        "    ]\n",
        "\n",
        "    for category in enhanced_categories:\n",
        "        if category in df_clean.columns:\n",
        "            # Get sample of keywords in this category\n",
        "            sample_keywords = df_clean[category].dropna().head(3)\n",
        "            print(f\"\\n{category.replace('keywords_', '').upper()}:\")\n",
        "            for keywords in sample_keywords:\n",
        "                print(f\"  • {str(keywords)[:100]}...\")\n",
        "\n",
        "# Create a matrix of intervention types vs outcomes (both AI-extracted)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"INTERVENTION-OUTCOME MATRIX (from AI extractions):\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create crosstab of AI-extracted fields\n",
        "intervention_outcome_matrix = pd.crosstab(\n",
        "    df_clean['intervention_type'].fillna('Not specified'),\n",
        "    df_clean['outcome_extracted'].fillna('Not specified')\n",
        ")\n",
        "\n",
        "# Show top combinations\n",
        "top_interventions = df_clean['intervention_type'].value_counts().head(10).index\n",
        "top_outcomes = df_clean['outcome_extracted'].value_counts().head(10).index\n",
        "\n",
        "matrix_subset = intervention_outcome_matrix.loc[\n",
        "    intervention_outcome_matrix.index.isin(top_interventions),\n",
        "    intervention_outcome_matrix.columns.isin(top_outcomes)\n",
        "]\n",
        "\n",
        "# Visualize the matrix\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure(data=go.Heatmap(\n",
        "    z=matrix_subset.values,\n",
        "    x=matrix_subset.columns,\n",
        "    y=matrix_subset.index,\n",
        "    colorscale='Blues',\n",
        "    text=matrix_subset.values,\n",
        "    texttemplate='%{text}',\n",
        "    textfont={\"size\": 10}\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='AI-Identified Intervention Types vs Outcomes',\n",
        "    xaxis_title='Outcome (AI-extracted)',\n",
        "    yaxis_title='Intervention Type (AI-extracted)',\n",
        "    height=600,\n",
        "    width=1000\n",
        ")\n",
        "\n",
        "fig.show()\n",
        "\n",
        "print(\"\\nTop Intervention-Outcome Combinations:\")\n",
        "for i in range(min(10, len(matrix_subset.index))):\n",
        "    for j in range(min(5, len(matrix_subset.columns))):\n",
        "        count = matrix_subset.iloc[i, j]\n",
        "        if count > 0:\n",
        "            print(f\"  • {matrix_subset.index[i]} → {matrix_subset.columns[j]}: {count} studies\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sKdS1zy2ctm1",
        "outputId": "cec724f8-e5b5-4aec-9dbf-ffb27fbe8250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI-Generated Fields Available:\n",
            "\n",
            "Keyword columns: ['keywords', 'keywords_additional', 'keywords_methodology', 'keywords_sector', 'keywords_mechanisms', 'keywords_implementation', 'keywords_context', 'keywords_partners']\n",
            "\n",
            "============================================================\n",
            "SAMPLES OF AI-EXTRACTED INTERVENTION TYPES:\n",
            "============================================================\n",
            "intervention_type\n",
            "social identity manipulation; moral values alignment; group similarity; rule-following task                                          3\n",
            "information treatments, fiscal policy uncertainty, exogenous changes, randomized                                                     2\n",
            "export procedure training, business training, capacity building, skill development                                                   2\n",
            "online survey experiment; information provision; random assignment                                                                   2\n",
            "construal level manipulation; psychological distance; social distance; temporal distance; advertising appeals                        2\n",
            "savings lockboxes, financial access innovations, promotion of savings                                                                2\n",
            "free fertilizer; agricultural inputs; fertilizer provision; randomized provision                                                     1\n",
            "framed field experiment; continuous convex non-monetary budget; commitment device; self-control strategies                           1\n",
            "insurance games; information provision; disaster probability; experience-based learning                                              1\n",
            "housing lottery; improved housing; subsidized housing; housing relocation                                                            1\n",
            "correspondence study; fictitious job applications; productivity signals; positive reference; enhanced education; technical skills    1\n",
            "active labour market policies; ALMPs; policy intensification; youth unemployment programs                                            1\n",
            "index insurance; rainfall insurance; informal risk sharing; community risk sharing                                                   1\n",
            "information manipulation, font size adjustment, name visibility                                                                      1\n",
            "neural mobilization; low level laser therapy; ultrasound therapy; deep friction massage                                              1\n",
            "small group tutoring, college student volunteers, 3-month program                                                                    1\n",
            "mental health services; demand shaping; barrier reduction; awareness campaigns; counseling services                                  1\n",
            "AI chatbots, virtual outreach, non-generative AI, messaging                                                                          1\n",
            "information treatments; inflation data; public data; moment information                                                              1\n",
            "information provision, inflation expectations, randomized treatments, survey-based intervention                                      1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "============================================================\n",
            "ANALYZING INTERVENTION PATTERNS FROM AI KEYWORDS:\n",
            "============================================================\n",
            "Found 2110 unique AI-identified intervention types\n",
            "\n",
            "Most common terms in AI-extracted intervention types:\n",
            "  information: 395 occurrences\n",
            "  behavioral: 198 occurrences\n",
            "  financial: 193 occurrences\n",
            "  social: 185 occurrences\n",
            "  randomized: 143 occurrences\n",
            "  online: 141 occurrences\n",
            "  provision,: 133 occurrences\n",
            "  experiment,: 129 occurrences\n",
            "  experimental: 126 occurrences\n",
            "  experiment;: 120 occurrences\n",
            "  health: 110 occurrences\n",
            "  intervention: 108 occurrences\n",
            "  survey: 102 occurrences\n",
            "  intervention;: 101 occurrences\n",
            "  intervention,: 101 occurrences\n",
            "\n",
            "============================================================\n",
            "ANALYZING ENHANCED AI KEYWORDS:\n",
            "============================================================\n",
            "\n",
            "METHODOLOGY:\n",
            "  • randomized controlled trial; expert elicitation; belief assessment; impact evaluation...\n",
            "  • RCT, experimental design, survey...\n",
            "  • RCT; laboratory experiment; field experiment...\n",
            "\n",
            "SECTOR:\n",
            "  • health; governance; development; education; public services...\n",
            "  • finance, retirement, consumer protection...\n",
            "  • finance; microfinance; banking...\n",
            "\n",
            "MECHANISMS:\n",
            "  • community empowerment; information barriers; local talent; managerial capital; social facilitation...\n",
            "  • information processing, decision-making, risk perception...\n",
            "  • religious barriers; coordination failure; informal RoSCAs...\n",
            "\n",
            "IMPLEMENTATION:\n",
            "  • financial control; implementation control; basic training; technocratic support; community-driven...\n",
            "  • information design, consumer testing, policy evaluation...\n",
            "  • credit unions; bank guarantee; microcredit provision...\n",
            "\n",
            "CONTEXT:\n",
            "  • rural; underdeveloped; post-conflict; limited state presence; public service gap...\n",
            "  • retirement planning, financial services, consumer market...\n",
            "  • rural; poor villages; Muslim communities...\n",
            "\n",
            "PARTNERS:\n",
            "  • not specified...\n",
            "  • Australian Government...\n",
            "  • J-PAL...\n",
            "\n",
            "============================================================\n",
            "INTERVENTION-OUTCOME MATRIX (from AI extractions):\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"81d7c799-cc7d-4220-87f0-f2ddff16e1c5\" class=\"plotly-graph-div\" style=\"height:600px; width:1000px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"81d7c799-cc7d-4220-87f0-f2ddff16e1c5\")) {                    Plotly.newPlot(                        \"81d7c799-cc7d-4220-87f0-f2ddff16e1c5\",                        [{\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]],\"text\":[[0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,2,0,0,0],[0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0],[2,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,2,0,0,0,0]],\"textfont\":{\"size\":10},\"texttemplate\":\"%{text}\",\"x\":[\"alcohol consumption, blood pressure, temptation spending, health outcomes\",\"consumption decisions, non-durable spending, durable spending, real income expectations, aggregate spending\",\"effort provision, performance, treatment effects, heterogeneity\",\"employer callback rate; disability gap; occupational heterogeneity; taste-based discrimination\",\"employment; youth employment; sickness benefits; long-term unemployment\",\"group identification; rule compliance; behavior change; norm sensitivity\",\"household macroeconomic expectations, consumption-saving decisions, economic behavior, decision-making\",\"inflation expectations, credit demand, borrowing costs, employment, sales, foreign currency purchases\",\"insurance take-up; risk-taking behavior; labor demand; equilibrium wages\",\"stigma; shame; general health; quality of life; social support\"],\"y\":[\"construal level manipulation; psychological distance; social distance; temporal distance; advertising appeals\",\"export procedure training, business training, capacity building, skill development\",\"framed field experiment; continuous convex non-monetary budget; commitment device; self-control strategies\",\"free fertilizer; agricultural inputs; fertilizer provision; randomized provision\",\"housing lottery; improved housing; subsidized housing; housing relocation\",\"information treatments, fiscal policy uncertainty, exogenous changes, randomized\",\"insurance games; information provision; disaster probability; experience-based learning\",\"online survey experiment; information provision; random assignment\",\"savings lockboxes, financial access innovations, promotion of savings\",\"social identity manipulation; moral values alignment; group similarity; rule-following task\"],\"z\":[[0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,2,0,0,0],[0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0],[2,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,2,0,0,0,0]],\"type\":\"heatmap\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"AI-Identified Intervention Types vs Outcomes\"},\"xaxis\":{\"title\":{\"text\":\"Outcome (AI-extracted)\"}},\"yaxis\":{\"title\":{\"text\":\"Intervention Type (AI-extracted)\"}},\"height\":600,\"width\":1000},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('81d7c799-cc7d-4220-87f0-f2ddff16e1c5');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top Intervention-Outcome Combinations:\n",
            "  • savings lockboxes, financial access innovations, promotion of savings → alcohol consumption, blood pressure, temptation spending, health outcomes: 2 studies\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "from openai import OpenAI, OpenAIError\n",
        "import json\n",
        "import os\n",
        "\n",
        "# --- PASTE YOUR API KEY HERE ---\n",
        "# Since you are in a Colab environment, the easiest way is to set the key directly.\n",
        "# Replace \"your-api-key-goes-here\" with your actual Together AI key.\n",
        "TOGETHER_API_KEY = \"your-api-key-goes-here\"\n",
        "\n",
        "# --- 1. Create a Sample DataFrame ---\n",
        "# In a real scenario, you would load your data here.\n",
        "# This sample DataFrame mimics the structure of your 'df_clustered'.\n",
        "data = {\n",
        "    'intervention_full_text': [\n",
        "        \"cash transfer program for poor households to improve nutrition\",\n",
        "        \"microfinance loans for small business owners in rural areas\",\n",
        "        \"agricultural training for farmers on new crop varieties\",\n",
        "        \"conditional cash transfers linked to school attendance\",\n",
        "        \"small loans and financial literacy training for women entrepreneurs\",\n",
        "        \"providing fertilizer and seeds to improve crop yields for farmers\",\n",
        "        \"unconditional cash aid to families in poverty\",\n",
        "        \"business grants for new startups in urban centers\",\n",
        "        \"irrigation system improvements for local farming communities\",\n",
        "        \"scholarships for girls to encourage secondary education enrollment\"\n",
        "    ],\n",
        "    'title': [\n",
        "        \"Nutrition Impact of Cash Transfers\",\n",
        "        \"Microfinance and Rural Business\",\n",
        "        \"Modern Farming Techniques Study\",\n",
        "        \"School Attendance and CCTs\",\n",
        "        \"Women's Entrepreneurship Support\",\n",
        "        \"Crop Yield Improvement Program\",\n",
        "        \"Poverty Alleviation with UCT\",\n",
        "        \"Urban Startup Grant Effects\",\n",
        "        \"Farming Irrigation Project\",\n",
        "        \"Girls' Education Scholarships\"\n",
        "    ],\n",
        "    'intervention_type': [\n",
        "        \"Cash Transfer\", \"Finance\", \"Agriculture\", \"Cash Transfer\", \"Finance\",\n",
        "        \"Agriculture\", \"Cash Transfer\", \"Finance\", \"Agriculture\", \"Education\"\n",
        "    ]\n",
        "}\n",
        "df_clustered = pd.DataFrame(data)\n",
        "\n",
        "# --- 2. Create TF-IDF Vectors from Text Data ---\n",
        "# This block creates the 'intervention_vectors' variable that was previously undefined.\n",
        "print(\"Creating TF-IDF vectors from intervention text...\")\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.95, min_df=2)\n",
        "intervention_vectors = vectorizer.fit_transform(df_clustered['intervention_full_text'])\n",
        "print(f\"Successfully created a sparse matrix with shape: {intervention_vectors.shape}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "\n",
        "# --- 3. Function to Find Optimal Clusters ---\n",
        "# This is the function you provided, now fully runnable with the data prepared above.\n",
        "\n",
        "# Note: The client is now configured for Together AI.\n",
        "try:\n",
        "    # Switched to Together AI client\n",
        "    # The openai library is compatible, we just change the base_url and api_key.\n",
        "    client = OpenAI(\n",
        "        api_key=TOGETHER_API_KEY,\n",
        "        base_url=\"https://api.together.xyz/v1\",\n",
        "    )\n",
        "except (ImportError, OpenAIError):\n",
        "    print(\"OpenAI library not found or API key is invalid. LLM analysis will be skipped.\")\n",
        "    client = None\n",
        "\n",
        "def find_optimal_clusters_with_llm(df, vectors, min_k=3, max_k=8):\n",
        "    \"\"\"\n",
        "    Use multiple metrics and optional LLM analysis to find the optimal number of clusters.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The dataframe containing the source data.\n",
        "        vectors (scipy.sparse.matrix): The TF-IDF vectors to cluster.\n",
        "        min_k (int): The minimum number of clusters to test.\n",
        "        max_k (int): The maximum number of clusters to test.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the optimal k, a dataframe of metrics, and the LLM recommendation.\n",
        "    \"\"\"\n",
        "    print(\"Finding optimal number of clusters...\")\n",
        "\n",
        "    # Adjust max_k if it's larger than the number of samples\n",
        "    if max_k >= vectors.shape[0]:\n",
        "        max_k = vectors.shape[0] - 1\n",
        "        print(f\"Warning: max_k is too high for the number of samples. Adjusting to {max_k}.\")\n",
        "\n",
        "    # Step 1: Calculate clustering metrics for different k values\n",
        "    metrics = []\n",
        "\n",
        "    for k in range(min_k, max_k + 1):\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
        "        labels = kmeans.fit_predict(vectors)\n",
        "\n",
        "        # Ensure there's more than one cluster to calculate metrics\n",
        "        if len(set(labels)) > 1:\n",
        "            silhouette = silhouette_score(vectors, labels)\n",
        "            davies_bouldin = davies_bouldin_score(vectors.toarray(), labels)\n",
        "            inertia = kmeans.inertia_\n",
        "\n",
        "            cluster_sizes = np.bincount(labels)\n",
        "            size_std = np.std(cluster_sizes)\n",
        "            min_size = np.min(cluster_sizes)\n",
        "\n",
        "            metrics.append({\n",
        "                'k': k,\n",
        "                'silhouette': silhouette,\n",
        "                'davies_bouldin': davies_bouldin,\n",
        "                'inertia': inertia,\n",
        "                'size_std': size_std,\n",
        "                'min_cluster_size': min_size\n",
        "            })\n",
        "\n",
        "            print(f\"k={k}: silhouette={silhouette:.3f}, DB={davies_bouldin:.3f}, min_size={min_size}\")\n",
        "        else:\n",
        "            print(f\"k={k}: Only one cluster was found. Metrics cannot be calculated.\")\n",
        "\n",
        "    if not metrics:\n",
        "        print(\"Could not calculate metrics for any k value. Aborting.\")\n",
        "        return min_k, pd.DataFrame(), None\n",
        "\n",
        "    metrics_df = pd.DataFrame(metrics)\n",
        "\n",
        "    # Step 2: Use LLM to analyze the metrics and suggest optimal k\n",
        "    if not client or TOGETHER_API_KEY == \"your-api-key-goes-here\":\n",
        "        if not client:\n",
        "             print(\"\\nOpenAI client library not initialized. Skipping LLM analysis.\")\n",
        "        else:\n",
        "             print(\"\\nAPI key not set. Skipping LLM analysis.\")\n",
        "        # Fallback: simple logic based on silhouette score\n",
        "        optimal_k = int(metrics_df.loc[metrics_df['silhouette'].idxmax()]['k'])\n",
        "        return optimal_k, metrics_df, None\n",
        "\n",
        "    prompt = f\"\"\"Analyze these clustering metrics for an RCT intervention dataset:\n",
        "\n",
        "{metrics_df.to_string()}\n",
        "\n",
        "Context: We're clustering {len(df)} development economics RCT studies based on their interventions.\n",
        "\n",
        "Consider:\n",
        "1. Silhouette score (higher is better, measures cluster separation)\n",
        "2. Davies-Bouldin index (lower is better, measures cluster compactness)\n",
        "3. Minimum cluster size (avoid too many tiny clusters)\n",
        "4. Size standard deviation (prefer balanced clusters)\n",
        "\n",
        "Recommend the optimal number of clusters (k) that:\n",
        "- Provides meaningful groupings for policy analysis\n",
        "- Avoids too many micro-clusters\n",
        "- Balances statistical metrics with interpretability\n",
        "\n",
        "Respond with ONLY the JSON object, without any additional text or formatting:\n",
        "{{\n",
        "    \"optimal_k\": <number>,\n",
        "    \"reasoning\": \"...\",\n",
        "    \"alternatives\": [<number>, <number>],\n",
        "    \"expected_interpretation\": \"...\"\n",
        "}}\"\"\"\n",
        "\n",
        "    try:\n",
        "        print(\"\\nQuerying Together AI with Llama 3.3 model for optimal k recommendation...\")\n",
        "        response = client.chat.completions.create(\n",
        "            # Using the correct Meta Llama 3.3 model name for Together AI\n",
        "            model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert in clustering analysis and development economics research. Respond only with a valid JSON object.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.3,\n",
        "            response_format={\"type\": \"json_object\"}\n",
        "        )\n",
        "\n",
        "        recommendation = json.loads(response.choices[0].message.content)\n",
        "\n",
        "        print(f\"\\nLLM Recommendation: {recommendation['optimal_k']} clusters\")\n",
        "        print(f\"Reasoning: {recommendation['reasoning']}\")\n",
        "\n",
        "        return recommendation['optimal_k'], metrics_df, recommendation\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nLLM analysis failed: {e}\")\n",
        "        # Fallback: find k with the best silhouette score\n",
        "        print(\"Using fallback logic to determine optimal k.\")\n",
        "        optimal_k = int(metrics_df.loc[metrics_df['silhouette'].idxmax()]['k'])\n",
        "        return optimal_k, metrics_df, None\n",
        "\n",
        "# --- 4. Run the Optimization ---\n",
        "# The function is now called with the dataframe and the newly created vectors.\n",
        "# Note: The sample data is small, so we test a smaller range of k.\n",
        "optimal_k, metrics_df, recommendation = find_optimal_clusters_with_llm(\n",
        "    df_clustered,\n",
        "    intervention_vectors,\n",
        "    min_k=2,\n",
        "    max_k=5 # Adjusted for small sample size\n",
        ")\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\"\\nFinal Analysis: Using {optimal_k} clusters.\")\n",
        "if not metrics_df.empty:\n",
        "    print(\"\\nMetrics Table:\")\n",
        "    print(metrics_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8B8V1xfbdilQ",
        "outputId": "f2512c5e-9c21-4080-d936-b3c6d43239c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating TF-IDF vectors from intervention text...\n",
            "Successfully created a sparse matrix with shape: (10, 9)\n",
            "------------------------------\n",
            "Finding optimal number of clusters...\n",
            "k=2: silhouette=0.290, DB=1.067, min_size=3\n",
            "k=3: silhouette=0.325, DB=1.057, min_size=2\n",
            "k=4: silhouette=0.355, DB=0.956, min_size=2\n",
            "k=5: silhouette=0.388, DB=0.601, min_size=1\n",
            "\n",
            "API key not set. Skipping LLM analysis.\n",
            "------------------------------\n",
            "\n",
            "Final Analysis: Using 5 clusters.\n",
            "\n",
            "Metrics Table:\n",
            "   k  silhouette  davies_bouldin   inertia  size_std  min_cluster_size\n",
            "0  2    0.289613        1.066546  4.085539  2.000000                 3\n",
            "1  3    0.324597        1.056938  2.816366  1.247219                 2\n",
            "2  4    0.355044        0.955539  1.878044  0.500000                 2\n",
            "3  5    0.387522        0.601293  1.211377  0.632456                 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TOGETHER_API_KEY = \"tgp_v1_BsE1iA9fN4XlbJUbqcaDSDomw8saxT3WmcVA7jh_cis\""
      ],
      "metadata": {
        "id": "MEpFU5N1jeSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "from openai import OpenAI, OpenAIError\n",
        "import json\n",
        "import os\n",
        "\n",
        "# --- PASTE YOUR API KEY HERE ---\n",
        "# Since you are in a Colab environment, the easiest way is to set the key directly.\n",
        "# Replace \"your-api-key-goes-here\" with your actual Together AI key.\n",
        "TOGETHER_API_KEY = \"tgp_v1_BsE1iA9fN4XlbJUbqcaDSDomw8saxT3WmcVA7jh_cis\"\n",
        "\n",
        "# --- 1. Create a Sample DataFrame ---\n",
        "# In a real scenario, you would load your data here.\n",
        "# This sample DataFrame mimics the structure of your 'df_clustered'.\n",
        "data = {\n",
        "    'intervention_full_text': [\n",
        "        \"cash transfer program for poor households to improve nutrition\",\n",
        "        \"microfinance loans for small business owners in rural areas\",\n",
        "        \"agricultural training for farmers on new crop varieties\",\n",
        "        \"conditional cash transfers linked to school attendance\",\n",
        "        \"small loans and financial literacy training for women entrepreneurs\",\n",
        "        \"providing fertilizer and seeds to improve crop yields for farmers\",\n",
        "        \"unconditional cash aid to families in poverty\",\n",
        "        \"business grants for new startups in urban centers\",\n",
        "        \"irrigation system improvements for local farming communities\",\n",
        "        \"scholarships for girls to encourage secondary education enrollment\"\n",
        "    ],\n",
        "    'title': [\n",
        "        \"Nutrition Impact of Cash Transfers\",\n",
        "        \"Microfinance and Rural Business\",\n",
        "        \"Modern Farming Techniques Study\",\n",
        "        \"School Attendance and CCTs\",\n",
        "        \"Women's Entrepreneurship Support\",\n",
        "        \"Crop Yield Improvement Program\",\n",
        "        \"Poverty Alleviation with UCT\",\n",
        "        \"Urban Startup Grant Effects\",\n",
        "        \"Farming Irrigation Project\",\n",
        "        \"Girls' Education Scholarships\"\n",
        "    ],\n",
        "    'intervention_type': [\n",
        "        \"Cash Transfer\", \"Finance\", \"Agriculture\", \"Cash Transfer\", \"Finance\",\n",
        "        \"Agriculture\", \"Cash Transfer\", \"Finance\", \"Agriculture\", \"Education\"\n",
        "    ]\n",
        "}\n",
        "df_clustered = pd.DataFrame(data)\n",
        "\n",
        "# --- 2. Create TF-IDF Vectors from Text Data ---\n",
        "# This block creates the 'intervention_vectors' variable that was previously undefined.\n",
        "print(\"Creating TF-IDF vectors from intervention text...\")\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.95, min_df=2)\n",
        "intervention_vectors = vectorizer.fit_transform(df_clustered['intervention_full_text'])\n",
        "print(f\"Successfully created a sparse matrix with shape: {intervention_vectors.shape}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "\n",
        "# --- 3. Function to Find Optimal Clusters ---\n",
        "# This is the function you provided, now fully runnable with the data prepared above.\n",
        "\n",
        "# Note: The client is now configured for Together AI.\n",
        "try:\n",
        "    # Switched to Together AI client\n",
        "    # The openai library is compatible, we just change the base_url and api_key.\n",
        "    client = OpenAI(\n",
        "        api_key=TOGETHER_API_KEY,\n",
        "        base_url=\"https://api.together.xyz/v1\",\n",
        "    )\n",
        "except (ImportError, OpenAIError):\n",
        "    print(\"OpenAI library not found or API key is invalid. LLM analysis will be skipped.\")\n",
        "    client = None\n",
        "\n",
        "def find_optimal_clusters_with_llm(df, vectors, min_k=3, max_k=8):\n",
        "    \"\"\"\n",
        "    Use multiple metrics and optional LLM analysis to find the optimal number of clusters.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The dataframe containing the source data.\n",
        "        vectors (scipy.sparse.matrix): The TF-IDF vectors to cluster.\n",
        "        min_k (int): The minimum number of clusters to test.\n",
        "        max_k (int): The maximum number of clusters to test.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the optimal k, a dataframe of metrics, and the LLM recommendation.\n",
        "    \"\"\"\n",
        "    print(\"Finding optimal number of clusters...\")\n",
        "\n",
        "    # Adjust max_k if it's larger than the number of samples\n",
        "    if max_k >= vectors.shape[0]:\n",
        "        max_k = vectors.shape[0] - 1\n",
        "        print(f\"Warning: max_k is too high for the number of samples. Adjusting to {max_k}.\")\n",
        "\n",
        "    # Step 1: Calculate clustering metrics for different k values\n",
        "    metrics = []\n",
        "\n",
        "    for k in range(min_k, max_k + 1):\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
        "        labels = kmeans.fit_predict(vectors)\n",
        "\n",
        "        # Ensure there's more than one cluster to calculate metrics\n",
        "        if len(set(labels)) > 1:\n",
        "            silhouette = silhouette_score(vectors, labels)\n",
        "            davies_bouldin = davies_bouldin_score(vectors.toarray(), labels)\n",
        "            inertia = kmeans.inertia_\n",
        "\n",
        "            cluster_sizes = np.bincount(labels)\n",
        "            size_std = np.std(cluster_sizes)\n",
        "            min_size = np.min(cluster_sizes)\n",
        "\n",
        "            metrics.append({\n",
        "                'k': k,\n",
        "                'silhouette': silhouette,\n",
        "                'davies_bouldin': davies_bouldin,\n",
        "                'inertia': inertia,\n",
        "                'size_std': size_std,\n",
        "                'min_cluster_size': min_size\n",
        "            })\n",
        "\n",
        "            print(f\"k={k}: silhouette={silhouette:.3f}, DB={davies_bouldin:.3f}, min_size={min_size}\")\n",
        "        else:\n",
        "            print(f\"k={k}: Only one cluster was found. Metrics cannot be calculated.\")\n",
        "\n",
        "    if not metrics:\n",
        "        print(\"Could not calculate metrics for any k value. Aborting.\")\n",
        "        return min_k, pd.DataFrame(), None\n",
        "\n",
        "    metrics_df = pd.DataFrame(metrics)\n",
        "\n",
        "    # Step 2: Use LLM to analyze the metrics and suggest optimal k\n",
        "    if not client or TOGETHER_API_KEY == \"your-api-key-goes-here\":\n",
        "        if not client:\n",
        "             print(\"\\nOpenAI client library not initialized. Skipping LLM analysis.\")\n",
        "        else:\n",
        "             print(\"\\nAPI key not set. Skipping LLM analysis.\")\n",
        "        # Fallback: simple logic based on silhouette score\n",
        "        optimal_k = int(metrics_df.loc[metrics_df['silhouette'].idxmax()]['k'])\n",
        "        return optimal_k, metrics_df, None\n",
        "\n",
        "    prompt = f\"\"\"Analyze these clustering metrics for an RCT intervention dataset:\n",
        "\n",
        "{metrics_df.to_string()}\n",
        "\n",
        "Context: We're clustering {len(df)} development economics RCT studies based on their interventions.\n",
        "\n",
        "Consider:\n",
        "1. Silhouette score (higher is better, measures cluster separation)\n",
        "2. Davies-Bouldin index (lower is better, measures cluster compactness)\n",
        "3. Minimum cluster size (avoid too many tiny clusters)\n",
        "4. Size standard deviation (prefer balanced clusters)\n",
        "\n",
        "Recommend the optimal number of clusters (k) that:\n",
        "- Provides meaningful groupings for policy analysis\n",
        "- Avoids too many micro-clusters\n",
        "- Balances statistical metrics with interpretability\n",
        "\n",
        "Respond with ONLY the JSON object, without any additional text or formatting:\n",
        "{{\n",
        "    \"optimal_k\": <number>,\n",
        "    \"reasoning\": \"...\",\n",
        "    \"alternatives\": [<number>, <number>],\n",
        "    \"expected_interpretation\": \"...\"\n",
        "}}\"\"\"\n",
        "\n",
        "    try:\n",
        "        print(\"\\nQuerying Together AI with Llama 3.3 model for optimal k recommendation...\")\n",
        "        response = client.chat.completions.create(\n",
        "            # Using the correct Meta Llama 3.3 model name for Together AI\n",
        "            model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert in clustering analysis and development economics research. Respond only with a valid JSON object.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.3,\n",
        "            response_format={\"type\": \"json_object\"}\n",
        "        )\n",
        "\n",
        "        recommendation = json.loads(response.choices[0].message.content)\n",
        "\n",
        "        print(f\"\\nLLM Recommendation: {recommendation['optimal_k']} clusters\")\n",
        "        print(f\"Reasoning: {recommendation['reasoning']}\")\n",
        "\n",
        "        return recommendation['optimal_k'], metrics_df, recommendation\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nLLM analysis failed: {e}\")\n",
        "        # Fallback: find k with the best silhouette score\n",
        "        print(\"Using fallback logic to determine optimal k.\")\n",
        "        optimal_k = int(metrics_df.loc[metrics_df['silhouette'].idxmax()]['k'])\n",
        "        return optimal_k, metrics_df, None\n",
        "\n",
        "# --- 4. Run the Optimization ---\n",
        "# The function is now called with the dataframe and the newly created vectors.\n",
        "# Note: The sample data is small, so we test a smaller range of k.\n",
        "optimal_k, metrics_df, recommendation = find_optimal_clusters_with_llm(\n",
        "    df_clustered,\n",
        "    intervention_vectors,\n",
        "    min_k=2,\n",
        "    max_k=5 # Adjusted for small sample size\n",
        ")\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\"\\nFinal Analysis: Using {optimal_k} clusters.\")\n",
        "if not metrics_df.empty:\n",
        "    print(\"\\nMetrics Table:\")\n",
        "    print(metrics_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O662zj6dksoL",
        "outputId": "8d3e54fc-4c3f-4e86-80c7-abc41040cb7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating TF-IDF vectors from intervention text...\n",
            "Successfully created a sparse matrix with shape: (10, 9)\n",
            "------------------------------\n",
            "Finding optimal number of clusters...\n",
            "k=2: silhouette=0.290, DB=1.067, min_size=3\n",
            "k=3: silhouette=0.325, DB=1.057, min_size=2\n",
            "k=4: silhouette=0.355, DB=0.956, min_size=2\n",
            "k=5: silhouette=0.388, DB=0.601, min_size=1\n",
            "\n",
            "Querying Together AI with Llama 3.3 model for optimal k recommendation...\n",
            "\n",
            "LLM Recommendation: 4 clusters\n",
            "Reasoning: The optimal k is chosen based on a balance of statistical metrics and interpretability. The silhouette score increases as k increases, indicating better separation between clusters. The Davies-Bouldin index decreases as k increases, indicating more compact clusters. However, the minimum cluster size decreases as k increases, which could lead to too many micro-clusters. The size standard deviation is relatively low for k=4, indicating balanced clusters. Considering these factors, k=4 provides a good balance between statistical metrics and interpretability.\n",
            "------------------------------\n",
            "\n",
            "Final Analysis: Using 4 clusters.\n",
            "\n",
            "Metrics Table:\n",
            "   k  silhouette  davies_bouldin   inertia  size_std  min_cluster_size\n",
            "0  2    0.289613        1.066546  4.085539  2.000000                 3\n",
            "1  3    0.324597        1.056938  2.816366  1.247219                 2\n",
            "2  4    0.355044        0.955539  1.878044  0.500000                 2\n",
            "3  5    0.387522        0.601293  1.211377  0.632456                 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "def create_intervention_clusters_with_llm(df, n_clusters=12):\n",
        "    \"\"\"\n",
        "    Group interventions into clusters and use LLM to name them meaningfully\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Create text representation for clustering\n",
        "    print(\"Step 1: Preparing intervention data for clustering...\")\n",
        "\n",
        "    # Combine all intervention-related text, handling NaN values\n",
        "    df = df.copy()\n",
        "    df['intervention_full_text'] = (\n",
        "        df['intervention_type'].fillna('') + ' ' +\n",
        "        df['interventions'].fillna('').str[:500] + ' ' +  # Limit length and handle NaN\n",
        "        df['tldr'].fillna('') + ' ' +\n",
        "        df['outcome_extracted'].fillna('')\n",
        "    )\n",
        "\n",
        "    # Remove rows with no intervention data\n",
        "    df_valid = df[df['intervention_full_text'].str.strip() != ''].copy()\n",
        "\n",
        "    # Double-check for any remaining NaN values\n",
        "    df_valid['intervention_full_text'] = df_valid['intervention_full_text'].fillna('')\n",
        "    df_valid = df_valid[df_valid['intervention_full_text'] != '']\n",
        "\n",
        "    print(f\"Working with {len(df_valid)} studies with intervention data\")\n",
        "\n",
        "    # Step 2: Create TF-IDF vectors for clustering\n",
        "    print(\"\\nStep 2: Creating TF-IDF vectors...\")\n",
        "    vectorizer = TfidfVectorizer(max_features=100, stop_words='english', min_df=2)\n",
        "\n",
        "    # Convert to list to ensure no NaN issues\n",
        "    texts = df_valid['intervention_full_text'].tolist()\n",
        "    intervention_vectors = vectorizer.fit_transform(texts)\n",
        "\n",
        "    # Step 3: Perform clustering\n",
        "    print(f\"\\nStep 3: Clustering into {n_clusters} groups...\")\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "    df_valid['cluster'] = kmeans.fit_predict(intervention_vectors)\n",
        "\n",
        "    # Step 4: Analyze each cluster (without LLM for now to test)\n",
        "    print(\"\\nStep 4: Analyzing clusters...\")\n",
        "\n",
        "    cluster_analysis = []\n",
        "\n",
        "    # Get feature names for interpretation\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    for cluster_id in range(n_clusters):\n",
        "        cluster_studies = df_valid[df_valid['cluster'] == cluster_id]\n",
        "        n_studies = len(cluster_studies)\n",
        "\n",
        "        # Get top terms for this cluster\n",
        "        cluster_center = kmeans.cluster_centers_[cluster_id]\n",
        "        top_indices = cluster_center.argsort()[-10:][::-1]\n",
        "        top_terms = [feature_names[i] for i in top_indices]\n",
        "\n",
        "        # Get most common intervention types in this cluster\n",
        "        top_intervention_types = cluster_studies['intervention_type'].value_counts().head(3)\n",
        "\n",
        "        # Get sample studies\n",
        "        sample_size = min(5, n_studies)\n",
        "        sample_studies = cluster_studies.sample(n=sample_size, random_state=42) if n_studies > 0 else pd.DataFrame()\n",
        "\n",
        "        cluster_info = {\n",
        "            'cluster_id': cluster_id,\n",
        "            'n_studies': n_studies,\n",
        "            'top_terms': top_terms[:5],\n",
        "            'top_intervention_types': top_intervention_types.to_dict() if len(top_intervention_types) > 0 else {},\n",
        "            'sample_titles': sample_studies['title'].head(3).tolist() if len(sample_studies) > 0 else []\n",
        "        }\n",
        "\n",
        "        cluster_analysis.append(cluster_info)\n",
        "\n",
        "        print(f\"Cluster {cluster_id}: {n_studies} studies\")\n",
        "        print(f\"  Top terms: {', '.join(top_terms[:5])}\")\n",
        "        if len(top_intervention_types) > 0:\n",
        "            print(f\"  Most common type: {top_intervention_types.index[0]}\")\n",
        "\n",
        "    return df_valid, cluster_analysis\n",
        "\n",
        "# Run the clustering\n",
        "df_clustered, clusters = create_intervention_clusters_with_llm(df_clean, n_clusters=12)\n",
        "\n",
        "# Display detailed results\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"INTERVENTION CLUSTER ANALYSIS RESULTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for cluster in clusters:\n",
        "    if cluster['n_studies'] > 0:\n",
        "        print(f\"\\n📊 CLUSTER {cluster['cluster_id']} ({cluster['n_studies']} studies)\")\n",
        "        print(f\"   Key terms: {', '.join(cluster['top_terms'])}\")\n",
        "\n",
        "        if cluster['top_intervention_types']:\n",
        "            print(f\"   Top intervention types:\")\n",
        "            for itype, count in list(cluster['top_intervention_types'].items())[:3]:\n",
        "                print(f\"      • {itype}: {count} studies\")\n",
        "\n",
        "        if cluster['sample_titles']:\n",
        "            print(f\"   Example studies:\")\n",
        "            for title in cluster['sample_titles']:\n",
        "                print(f\"      - {title[:80]}...\")\n",
        "\n",
        "# Create a summary DataFrame\n",
        "cluster_summary = pd.DataFrame(clusters)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CLUSTER SIZE DISTRIBUTION\")\n",
        "print(\"=\"*80)\n",
        "print(cluster_summary[['cluster_id', 'n_studies']].sort_values('n_studies', ascending=False))\n",
        "\n",
        "# Save the clustered data\n",
        "df_clustered.to_csv(base_path + 'studies_with_clusters.csv', index=False)\n",
        "print(f\"\\nClustered data saved to: {base_path}studies_with_clusters.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjXeUlIKeCwR",
        "outputId": "b5463bf8-1b83-4ef4-bcf4-298486e973aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Preparing intervention data for clustering...\n",
            "Working with 2289 studies with intervention data\n",
            "\n",
            "Step 2: Creating TF-IDF vectors...\n",
            "\n",
            "Step 3: Clustering into 12 groups...\n",
            "\n",
            "Step 4: Analyzing clusters...\n",
            "Cluster 0: 102 studies\n",
            "  Top terms: school, schools, program, students, education\n",
            "  Most common type: school-based intervention; youth empowerment; entrepreneurship training; employment skills\n",
            "Cluster 1: 167 studies\n",
            "  Top terms: available, hidden, intervention, effectiveness, experiment\n",
            "Cluster 2: 127 studies\n",
            "  Top terms: training, skills, program, business, employment\n",
            "  Most common type: export procedure training, business training, capacity building, skill development\n",
            "Cluster 3: 145 studies\n",
            "  Top terms: health, intervention, program, social, information\n",
            "  Most common type: community monitoring; health scorecards; non-financial awards; public commendations\n",
            "Cluster 4: 161 studies\n",
            "  Top terms: students, learning, student, academic, performance\n",
            "  Most common type: mentoring; career guidance; professional development; educational intervention\n",
            "Cluster 5: 659 studies\n",
            "  Top terms: intervention, social, behavior, treatment, hidden\n",
            "  Most common type: social identity manipulation; moral values alignment; group similarity; rule-following task\n",
            "Cluster 6: 204 studies\n",
            "  Top terms: survey, tax, policy, preferences, support\n",
            "  Most common type: randomized controlled trial; behavioral intervention; tax compliance; procrastination mitigation; salience enhancement; deterrence strengthening; tax morale improvement\n",
            "Cluster 7: 146 studies\n",
            "  Top terms: decision, making, risk, decisions, experiment\n",
            "  Most common type: information treatments, fiscal policy uncertainty, exogenous changes, randomized\n",
            "Cluster 8: 232 studies\n",
            "  Top terms: information, provision, treatment, intervention, behavior\n",
            "  Most common type: information provision, debt-to-GDP ratio disclosure, survey-based intervention\n",
            "Cluster 9: 143 studies\n",
            "  Top terms: financial, savings, incentives, intervention, behavior\n",
            "  Most common type: savings lockboxes, financial access innovations, promotion of savings\n",
            "Cluster 10: 95 studies\n",
            "  Top terms: gender, women, intervention, study, experiment\n",
            "  Most common type: randomized allocation of export orders; trade exposure; international trade; export participation\n",
            "Cluster 11: 108 studies\n",
            "  Top terms: job, employment, labor, market, work\n",
            "  Most common type: part-time work; full-time work; labor contracts; compensation schemes\n",
            "\n",
            "================================================================================\n",
            "INTERVENTION CLUSTER ANALYSIS RESULTS\n",
            "================================================================================\n",
            "\n",
            "📊 CLUSTER 0 (102 studies)\n",
            "   Key terms: school, schools, program, students, education\n",
            "   Top intervention types:\n",
            "      • school-based intervention; youth empowerment; entrepreneurship training; employment skills: 1 studies\n",
            "      • conditional cash transfers; loss aversion; psychological incentives; behavioral economics: 1 studies\n",
            "      • diagnostic feedback; capacity-building; standardized testing; professional development: 1 studies\n",
            "   Example studies:\n",
            "      - The long-term effects of addressing attitudes towards violence and reporting: An...\n",
            "      - The Potential of Urban Boarding Schools for the Poor: Evidence from SEED...\n",
            "      - Opportunity NYC--Family Rewards Demonstration...\n",
            "\n",
            "📊 CLUSTER 1 (167 studies)\n",
            "   Key terms: available, hidden, intervention, effectiveness, experiment\n",
            "   Example studies:\n",
            "      - Labour market expectations, relative performance and subject choice...\n",
            "      - Entrepreneurial community health delivery in Uganda: a cluster-randomized contro...\n",
            "      - Evaluation of Roads to Success...\n",
            "\n",
            "📊 CLUSTER 2 (127 studies)\n",
            "   Key terms: training, skills, program, business, employment\n",
            "   Top intervention types:\n",
            "      • export procedure training, business training, capacity building, skill development: 2 studies\n",
            "      • soft skills training; hard skills training; cross-cut training; management training: 1 studies\n",
            "      • football training, socio-emotional skills development, values promotion, conflict resolution: 1 studies\n",
            "   Example studies:\n",
            "      - Traditional and distance training programs to develop female community animal he...\n",
            "      - Promoting community and labor force engagement through service grants and non-co...\n",
            "      - Bangladeshi Association for Life skills, Income and Knowledge for Adolescents...\n",
            "\n",
            "📊 CLUSTER 3 (145 studies)\n",
            "   Key terms: health, intervention, program, social, information\n",
            "   Top intervention types:\n",
            "      • community monitoring; health scorecards; non-financial awards; public commendations: 1 studies\n",
            "      • shared toilet infrastructure; management model; sanitation facilities; urban slum improvement: 1 studies\n",
            "      • information provision, subsidies, political narrative, penalties, plan choice, take-up: 1 studies\n",
            "   Example studies:\n",
            "      - Optimal Policy with Correlated Interalities and Preferences: The Case of E-Cigar...\n",
            "      - Addressing Public Health Needs by Promoting Prosocial Behaviors Through Social M...\n",
            "      - Impact of the Gram Varta programme on health, nutrition, and women’s empowerment...\n",
            "\n",
            "📊 CLUSTER 4 (161 studies)\n",
            "   Key terms: students, learning, student, academic, performance\n",
            "   Top intervention types:\n",
            "      • mentoring; career guidance; professional development; educational intervention: 1 studies\n",
            "      • goal setting; financial incentives; educational incentives; performance-based rewards: 1 studies\n",
            "      • curriculum intervention; socio-emotional skills; grit; deep practice; self-regulation; growth mindsets; role models; stereotype threat reduction: 1 studies\n",
            "   Example studies:\n",
            "      - Remedying Education: Evidence from two randomized experiments in India...\n",
            "      - Role of Beliefs on Study Effort...\n",
            "      - BEST UP Project...\n",
            "\n",
            "📊 CLUSTER 5 (659 studies)\n",
            "   Key terms: intervention, social, behavior, treatment, hidden\n",
            "   Top intervention types:\n",
            "      • social identity manipulation; moral values alignment; group similarity; rule-following task: 3 studies\n",
            "      • construal level manipulation; psychological distance; social distance; temporal distance; advertising appeals: 2 studies\n",
            "      • agricultural production, income enhancement, food security, integrated soil fertility management, soybean production: 1 studies\n",
            "   Example studies:\n",
            "      - Measuring the Welfare Cost of Asymmetric Information in Consumer Credit Markets...\n",
            "      - Effect of Astym therapy versus kinesiotaping on rotator cuff tendinopathy in dia...\n",
            "      - Regional Mobility and the Formation of National Identity...\n",
            "\n",
            "📊 CLUSTER 6 (204 studies)\n",
            "   Key terms: survey, tax, policy, preferences, support\n",
            "   Top intervention types:\n",
            "      • randomized controlled trial; behavioral intervention; tax compliance; procrastination mitigation; salience enhancement; deterrence strengthening; tax morale improvement: 1 studies\n",
            "      • online survey; randomized controlled trial; information provision; social desirability manipulation: 1 studies\n",
            "      • pie-chart, relative spending, social norm message, loss-aversion, explicit penalties: 1 studies\n",
            "   Example studies:\n",
            "      - Immigration and Redistribution...\n",
            "      - Testing the effect of financial education on business taxpayers...\n",
            "      - My opinion, your opinion – Do group perceptions prevent the adoption of sustaina...\n",
            "\n",
            "📊 CLUSTER 7 (146 studies)\n",
            "   Key terms: decision, making, risk, decisions, experiment\n",
            "   Top intervention types:\n",
            "      • information treatments, fiscal policy uncertainty, exogenous changes, randomized: 2 studies\n",
            "      • decision right; moral dilemma; effort provision; employee empowerment; compliance management: 1 studies\n",
            "      • worker empowerment, managerial compensation, decision-making participation, compensation scheme choice: 1 studies\n",
            "   Example studies:\n",
            "      - By chance or by choice: Biased attribution of others' outcomes (Online Experimen...\n",
            "      - Mechanisms of Moral Motive Selection...\n",
            "      - An Experimental Study of Political Expertise and the Democratic Ideal...\n",
            "\n",
            "📊 CLUSTER 8 (232 studies)\n",
            "   Key terms: information, provision, treatment, intervention, behavior\n",
            "   Top intervention types:\n",
            "      • information provision, debt-to-GDP ratio disclosure, survey-based intervention: 1 studies\n",
            "      • personalized cost information; letter intervention; comparative information; consumer choice: 1 studies\n",
            "      • incentivized beliefs elicitation; information provision; experimental manipulation: 1 studies\n",
            "   Example studies:\n",
            "      - Education, Immigration and HPV Vaccination: an Informational Randomized Trial...\n",
            "      - Reducing Barriers to Rural Roof-top Solar Adoption: Experimental Evidence from I...\n",
            "      - Price Discovery in Online Markets: Convergence, Asymmetries and Information...\n",
            "\n",
            "📊 CLUSTER 9 (143 studies)\n",
            "   Key terms: financial, savings, incentives, intervention, behavior\n",
            "   Top intervention types:\n",
            "      • savings lockboxes, financial access innovations, promotion of savings: 2 studies\n",
            "      • microfinance; credit unions; bank-insured RoSCAs; Grameen-style microcredit: 1 studies\n",
            "      • commitment devices; reminders; mental accounting; cash-in-hand effect: 1 studies\n",
            "   Example studies:\n",
            "      - Reducing Absenteeism in the Workplace...\n",
            "      - Testing the Effectiveness of Consumer Financial Disclosure: Experimental Evidenc...\n",
            "      - Improving immunisation coverage in rural India: clustered randomised controlled ...\n",
            "\n",
            "📊 CLUSTER 10 (95 studies)\n",
            "   Key terms: gender, women, intervention, study, experiment\n",
            "   Top intervention types:\n",
            "      • randomized allocation of export orders; trade exposure; international trade; export participation: 1 studies\n",
            "      • gender gap analysis; wage gap; position gap; evaluation bias; advice following: 1 studies\n",
            "      • wage inequality; unfair procedures; gender discrimination; piece-rate wage: 1 studies\n",
            "   Example studies:\n",
            "      - Please Approach Me: Cross-regional Matching in Marriage Market...\n",
            "      - Gender and Microfinance 'Lab-in-the-field' Experiment...\n",
            "      - Racial and Gender Discrimination in Transportation Network Companies...\n",
            "\n",
            "📊 CLUSTER 11 (108 studies)\n",
            "   Key terms: job, employment, labor, market, work\n",
            "   Top intervention types:\n",
            "      • part-time work; full-time work; labor contracts; compensation schemes: 1 studies\n",
            "      • correspondence test; résumé manipulation; employment gap explanation; illness-related gap: 1 studies\n",
            "      • temporary employment, lottery allocation, work-study programs, youth employment: 1 studies\n",
            "   Example studies:\n",
            "      - Targeting and Peer Effects in Training Programmes for Poor Females...\n",
            "      - What works – study into the effects of four different social welfare schemes...\n",
            "      - An Experimental Evaluation of Philadelphia WorkReady...\n",
            "\n",
            "================================================================================\n",
            "CLUSTER SIZE DISTRIBUTION\n",
            "================================================================================\n",
            "    cluster_id  n_studies\n",
            "5            5        659\n",
            "8            8        232\n",
            "6            6        204\n",
            "1            1        167\n",
            "4            4        161\n",
            "7            7        146\n",
            "3            3        145\n",
            "9            9        143\n",
            "2            2        127\n",
            "11          11        108\n",
            "0            0        102\n",
            "10          10         95\n",
            "\n",
            "Clustered data saved to: /content/drive/MyDrive/AEA_RCT_Parsed/studies_with_clusters.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "from openai import OpenAI\n",
        "import json\n",
        "\n",
        "# --- 1. Create a Sample DataFrame ---\n",
        "# In a real scenario, you would load your data here.\n",
        "# This sample DataFrame mimics the structure of your 'df_clustered'.\n",
        "data = {\n",
        "    'intervention_full_text': [\n",
        "        \"cash transfer program for poor households to improve nutrition\",\n",
        "        \"microfinance loans for small business owners in rural areas\",\n",
        "        \"agricultural training for farmers on new crop varieties\",\n",
        "        \"conditional cash transfers linked to school attendance\",\n",
        "        \"small loans and financial literacy training for women entrepreneurs\",\n",
        "        \"providing fertilizer and seeds to improve crop yields for farmers\",\n",
        "        \"unconditional cash aid to families in poverty\",\n",
        "        \"business grants for new startups in urban centers\",\n",
        "        \"irrigation system improvements for local farming communities\",\n",
        "        \"scholarships for girls to encourage secondary education enrollment\"\n",
        "    ],\n",
        "    'title': [\n",
        "        \"Nutrition Impact of Cash Transfers\",\n",
        "        \"Microfinance and Rural Business\",\n",
        "        \"Modern Farming Techniques Study\",\n",
        "        \"School Attendance and CCTs\",\n",
        "        \"Women's Entrepreneurship Support\",\n",
        "        \"Crop Yield Improvement Program\",\n",
        "        \"Poverty Alleviation with UCT\",\n",
        "        \"Urban Startup Grant Effects\",\n",
        "        \"Farming Irrigation Project\",\n",
        "        \"Girls' Education Scholarships\"\n",
        "    ],\n",
        "    'intervention_type': [\n",
        "        \"Cash Transfer\", \"Finance\", \"Agriculture\", \"Cash Transfer\", \"Finance\",\n",
        "        \"Agriculture\", \"Cash Transfer\", \"Finance\", \"Agriculture\", \"Education\"\n",
        "    ]\n",
        "}\n",
        "df_clustered = pd.DataFrame(data)\n",
        "\n",
        "# --- 2. Create TF-IDF Vectors from Text Data ---\n",
        "# This block creates the 'intervention_vectors' variable that was previously undefined.\n",
        "print(\"Creating TF-IDF vectors from intervention text...\")\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.95, min_df=2)\n",
        "intervention_vectors = vectorizer.fit_transform(df_clustered['intervention_full_text'])\n",
        "print(f\"Successfully created a sparse matrix with shape: {intervention_vectors.shape}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "\n",
        "# --- 3. Function to Find Optimal Clusters ---\n",
        "# This is the function you provided, now fully runnable with the data prepared above.\n",
        "\n",
        "# Note: The OpenAI client requires a valid API key for the LLM analysis part to work.\n",
        "# The code includes a fallback mechanism if the API call fails.\n",
        "try:\n",
        "    # It's recommended to use environment variables for API keys\n",
        "    # For this example, we use a placeholder.\n",
        "    client = OpenAI(api_key='your-api-key-here')  # Replace with your API key\n",
        "except ImportError:\n",
        "    print(\"OpenAI library not found. LLM analysis will be skipped.\")\n",
        "    client = None\n",
        "\n",
        "def find_optimal_clusters_with_llm(df, vectors, min_k=3, max_k=8):\n",
        "    \"\"\"\n",
        "    Use multiple metrics and optional LLM analysis to find the optimal number of clusters.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The dataframe containing the source data.\n",
        "        vectors (scipy.sparse.matrix): The TF-IDF vectors to cluster.\n",
        "        min_k (int): The minimum number of clusters to test.\n",
        "        max_k (int): The maximum number of clusters to test.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the optimal k, a dataframe of metrics, and the LLM recommendation.\n",
        "    \"\"\"\n",
        "    print(\"Finding optimal number of clusters...\")\n",
        "\n",
        "    # Adjust max_k if it's larger than the number of samples\n",
        "    if max_k >= vectors.shape[0]:\n",
        "        max_k = vectors.shape[0] - 1\n",
        "        print(f\"Warning: max_k is too high for the number of samples. Adjusting to {max_k}.\")\n",
        "\n",
        "    # Step 1: Calculate clustering metrics for different k values\n",
        "    metrics = []\n",
        "\n",
        "    for k in range(min_k, max_k + 1):\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
        "        labels = kmeans.fit_predict(vectors)\n",
        "\n",
        "        # Ensure there's more than one cluster to calculate metrics\n",
        "        if len(set(labels)) > 1:\n",
        "            silhouette = silhouette_score(vectors, labels)\n",
        "            davies_bouldin = davies_bouldin_score(vectors.toarray(), labels)\n",
        "            inertia = kmeans.inertia_\n",
        "\n",
        "            cluster_sizes = np.bincount(labels)\n",
        "            size_std = np.std(cluster_sizes)\n",
        "            min_size = np.min(cluster_sizes)\n",
        "\n",
        "            metrics.append({\n",
        "                'k': k,\n",
        "                'silhouette': silhouette,\n",
        "                'davies_bouldin': davies_bouldin,\n",
        "                'inertia': inertia,\n",
        "                'size_std': size_std,\n",
        "                'min_cluster_size': min_size\n",
        "            })\n",
        "\n",
        "            print(f\"k={k}: silhouette={silhouette:.3f}, DB={davies_bouldin:.3f}, min_size={min_size}\")\n",
        "        else:\n",
        "            print(f\"k={k}: Only one cluster was found. Metrics cannot be calculated.\")\n",
        "\n",
        "    if not metrics:\n",
        "        print(\"Could not calculate metrics for any k value. Aborting.\")\n",
        "        return min_k, pd.DataFrame(), None\n",
        "\n",
        "    metrics_df = pd.DataFrame(metrics)\n",
        "\n",
        "    # Step 2: Use LLM to analyze the metrics and suggest optimal k\n",
        "    if not client:\n",
        "        print(\"\\nOpenAI client not initialized. Skipping LLM analysis.\")\n",
        "        # Fallback: simple logic based on silhouette score\n",
        "        optimal_k = metrics_df.loc[metrics_df['silhouette'].idxmax()]['k']\n",
        "        return optimal_k, metrics_df, None\n",
        "\n",
        "    prompt = f\"\"\"Analyze these clustering metrics for an RCT intervention dataset:\n",
        "\n",
        "{metrics_df.to_string()}\n",
        "\n",
        "Context: We're clustering {len(df)} development economics RCT studies based on their interventions.\n",
        "\n",
        "Consider:\n",
        "1. Silhouette score (higher is better, measures cluster separation)\n",
        "2. Davies-Bouldin index (lower is better, measures cluster compactness)\n",
        "3. Minimum cluster size (avoid too many tiny clusters)\n",
        "4. Size standard deviation (prefer balanced clusters)\n",
        "\n",
        "Recommend the optimal number of clusters (k) that:\n",
        "- Provides meaningful groupings for policy analysis\n",
        "- Avoids too many micro-clusters\n",
        "- Balances statistical metrics with interpretability\n",
        "\n",
        "Respond with ONLY the JSON object, without any additional text or formatting:\n",
        "{{\n",
        "    \"optimal_k\": <number>,\n",
        "    \"reasoning\": \"...\",\n",
        "    \"alternatives\": [<number>, <number>],\n",
        "    \"expected_interpretation\": \"...\"\n",
        "}}\"\"\"\n",
        "\n",
        "    try:\n",
        "        print(\"\\nQuerying LLM for optimal k recommendation...\")\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert in clustering analysis and development economics research.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.3,\n",
        "            response_format={\"type\": \"json_object\"}\n",
        "        )\n",
        "\n",
        "        recommendation = json.loads(response.choices[0].message.content)\n",
        "\n",
        "        print(f\"\\nLLM Recommendation: {recommendation['optimal_k']} clusters\")\n",
        "        print(f\"Reasoning: {recommendation['reasoning']}\")\n",
        "\n",
        "        return recommendation['optimal_k'], metrics_df, recommendation\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nLLM analysis failed: {e}\")\n",
        "        # Fallback: find k with the best silhouette score\n",
        "        print(\"Using fallback logic to determine optimal k.\")\n",
        "        optimal_k = int(metrics_df.loc[metrics_df['silhouette'].idxmax()]['k'])\n",
        "        return optimal_k, metrics_df, None\n",
        "\n",
        "# --- 4. Run the Optimization ---\n",
        "# The function is now called with the dataframe and the newly created vectors.\n",
        "# Note: The sample data is small, so we test a smaller range of k.\n",
        "optimal_k, metrics_df, recommendation = find_optimal_clusters_with_llm(\n",
        "    df_clustered,\n",
        "    intervention_vectors,\n",
        "    min_k=2,\n",
        "    max_k=5 # Adjusted for small sample size\n",
        ")\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\"\\nFinal Analysis: Using {optimal_k} clusters.\")\n",
        "print(\"\\nMetrics Table:\")\n",
        "print(metrics_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "tuTltuOke1Dg",
        "outputId": "4135442f-0619-4658-985e-fae94b100c8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'intervention_vectors' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2626142769.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m optimal_k, metrics_df, recommendation = find_optimal_clusters_with_llm(\n\u001b[1;32m    104\u001b[0m     \u001b[0mdf_clustered\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mintervention_vectors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0mmin_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mmax_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'intervention_vectors' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def identify_cluster_outliers(df_valid, intervention_vectors, n_clusters):\n",
        "    \"\"\"\n",
        "    Identify studies that don't fit well into any cluster\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\nIdentifying cluster outliers and borderline cases...\")\n",
        "\n",
        "    # Perform clustering with optimal k\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "    df_valid['cluster'] = kmeans.fit_predict(intervention_vectors)\n",
        "\n",
        "    # Calculate distance to cluster centers\n",
        "    distances = kmeans.transform(intervention_vectors)\n",
        "    df_valid['distance_to_center'] = distances.min(axis=1)\n",
        "\n",
        "    # Calculate probability of belonging to each cluster (soft clustering)\n",
        "    # Convert distances to probabilities using softmax\n",
        "    exp_distances = np.exp(-distances)\n",
        "    probabilities = exp_distances / exp_distances.sum(axis=1, keepdims=True)\n",
        "    df_valid['max_probability'] = probabilities.max(axis=1)\n",
        "    df_valid['second_best_prob'] = np.sort(probabilities, axis=1)[:, -2]\n",
        "\n",
        "    # Identify different types of outliers\n",
        "\n",
        "    # 1. Studies far from all clusters (true outliers)\n",
        "    distance_threshold = df_valid['distance_to_center'].quantile(0.95)\n",
        "    true_outliers = df_valid[df_valid['distance_to_center'] > distance_threshold].copy()\n",
        "\n",
        "    # 2. Studies between clusters (low max probability)\n",
        "    prob_threshold = 0.5\n",
        "    borderline_studies = df_valid[df_valid['max_probability'] < prob_threshold].copy()\n",
        "\n",
        "    # 3. Studies that could belong to multiple clusters\n",
        "    ambiguous_studies = df_valid[\n",
        "        (df_valid['max_probability'] - df_valid['second_best_prob']) < 0.2\n",
        "    ].copy()\n",
        "\n",
        "    print(f\"\\nOutlier Analysis:\")\n",
        "    print(f\"True outliers (far from all clusters): {len(true_outliers)} studies\")\n",
        "    print(f\"Borderline cases (between clusters): {len(borderline_studies)} studies\")\n",
        "    print(f\"Ambiguous cases (could fit multiple clusters): {len(ambiguous_studies)} studies\")\n",
        "\n",
        "    # Show examples of outliers\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"EXAMPLES OF STUDIES THAT DON'T FIT WELL\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(\"\\n🔴 TRUE OUTLIERS (most unique studies):\")\n",
        "    for _, row in true_outliers.nlargest(5, 'distance_to_center').iterrows():\n",
        "        print(f\"  • {row['title'][:80]}...\")\n",
        "        print(f\"    Distance from nearest cluster: {row['distance_to_center']:.3f}\")\n",
        "        print(f\"    Intervention: {row['intervention_type'][:100]}...\")\n",
        "        print()\n",
        "\n",
        "    print(\"\\n🟡 BORDERLINE CASES (between clusters):\")\n",
        "    for _, row in borderline_studies.nsmallest(5, 'max_probability').iterrows():\n",
        "        print(f\"  • {row['title'][:80]}...\")\n",
        "        print(f\"    Max cluster probability: {row['max_probability']:.2%}\")\n",
        "        print(f\"    Could belong to clusters: {row['cluster']} or others\")\n",
        "        print()\n",
        "\n",
        "    # Create outlier summary\n",
        "    outlier_summary = pd.DataFrame({\n",
        "        'outlier_type': ['True Outliers', 'Borderline', 'Ambiguous'],\n",
        "        'count': [len(true_outliers), len(borderline_studies), len(ambiguous_studies)],\n",
        "        'percentage': [\n",
        "            len(true_outliers) / len(df_valid) * 100,\n",
        "            len(borderline_studies) / len(df_valid) * 100,\n",
        "            len(ambiguous_studies) / len(df_valid) * 100\n",
        "        ]\n",
        "    })\n",
        "\n",
        "    return df_valid, outlier_summary, true_outliers\n",
        "\n",
        "# Identify outliers\n",
        "df_with_outliers, outlier_summary, outliers = identify_cluster_outliers(\n",
        "    df_clustered,\n",
        "    intervention_vectors,\n",
        "    optimal_k\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"OUTLIER SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(outlier_summary)\n",
        "\n",
        "# Save results\n",
        "df_with_outliers.to_csv(base_path + 'studies_with_clusters_and_outliers.csv', index=False)\n",
        "outliers.to_csv(base_path + 'unique_outlier_studies.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "wWv51XOMgJXB",
        "outputId": "0d5dac56-5c52-4379-e1cf-a97ad1f9f10c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'intervention_vectors' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1206429285.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m df_with_outliers, outlier_summary, outliers = identify_cluster_outliers(\n\u001b[1;32m     77\u001b[0m     \u001b[0mdf_clustered\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mintervention_vectors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0moptimal_k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'intervention_vectors' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "def hierarchical_clustering_analysis(intervention_vectors, df_valid, max_clusters=20):\n",
        "    \"\"\"\n",
        "    Use hierarchical clustering to find natural groupings\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\nPerforming hierarchical clustering analysis...\")\n",
        "\n",
        "    # Normalize vectors for better clustering\n",
        "    vectors_normalized = normalize(intervention_vectors.toarray())\n",
        "\n",
        "    # Perform hierarchical clustering\n",
        "    linkage_matrix = linkage(vectors_normalized, method='ward')\n",
        "\n",
        "    # Find natural cut points\n",
        "    distances = linkage_matrix[:, 2]\n",
        "    distance_diffs = np.diff(distances)\n",
        "\n",
        "    # Find large jumps in distance (natural cluster boundaries)\n",
        "    jump_indices = np.where(distance_diffs > np.percentile(distance_diffs, 90))[0]\n",
        "\n",
        "    # Calculate number of clusters at each jump\n",
        "    natural_clusters = []\n",
        "    for idx in jump_indices[-10:]:  # Look at last 10 big jumps\n",
        "        n_clusters = len(df_valid) - idx\n",
        "        if 5 <= n_clusters <= max_clusters:\n",
        "            natural_clusters.append(n_clusters)\n",
        "\n",
        "    print(f\"Natural cluster numbers based on dendrogram: {natural_clusters}\")\n",
        "\n",
        "    # Test each natural clustering\n",
        "    for n in natural_clusters[:3]:  # Test top 3 options\n",
        "        clusters = fcluster(linkage_matrix, n, criterion='maxclust')\n",
        "        df_valid[f'hier_cluster_{n}'] = clusters\n",
        "\n",
        "        # Calculate cluster quality\n",
        "        cluster_sizes = np.bincount(clusters)\n",
        "        print(f\"\\n{n} clusters: sizes range from {cluster_sizes.min()} to {cluster_sizes.max()}\")\n",
        "\n",
        "    return df_valid, natural_clusters\n",
        "\n",
        "# Run hierarchical analysis\n",
        "df_hier, natural_ns = hierarchical_clustering_analysis(\n",
        "    intervention_vectors,\n",
        "    df_clustered,\n",
        "    max_clusters=20\n",
        ")\n",
        "\n",
        "print(f\"\\nSuggested natural cluster numbers: {natural_ns[:3]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "bnT3AaOVgKhx",
        "outputId": "dc1c0f79-5171-4457-9bda-57c43e07b779"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'intervention_vectors' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1227261581.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Run hierarchical analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m df_hier, natural_ns = hierarchical_clustering_analysis(\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mintervention_vectors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mdf_clustered\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mmax_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'intervention_vectors' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install together\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from together import Together\n",
        "import os\n",
        "\n",
        "# --- 1. Setup and Sample Data Generation ---\n",
        "# In a real scenario, you would load your data here.\n",
        "# For this example, we create a sample DataFrame.\n",
        "print(\"Setting up sample data...\")\n",
        "data = {\n",
        "    'title': [f'Study {i+1}' for i in range(100)],\n",
        "    'intervention_type': [\n",
        "        'Cognitive Behavioral Therapy', 'Mindfulness Training', 'Pharmacological Treatment',\n",
        "        'Dietary Intervention', 'Exercise Program', 'Surgical Procedure', 'Placebo Control',\n",
        "        'Virtual Reality Exposure', 'Music Therapy', 'Art Therapy'\n",
        "    ] * 10,\n",
        "    'intervention_full_text': ([\n",
        "        \"Participants received weekly sessions of cognitive behavioral therapy focusing on negative thought patterns.\",\n",
        "        \"A daily mindfulness meditation program was implemented for all participants.\",\n",
        "        \"The experimental group was administered a new SSRI medication daily.\",\n",
        "        \"Subjects were placed on a strict ketogenic diet for the duration of the study.\",\n",
        "        \"A high-intensity interval training (HIIT) regimen was followed three times a week.\",\n",
        "        \"Patients underwent a minimally invasive laparoscopic surgery.\",\n",
        "        \"The control group received a sugar pill with no active ingredients.\",\n",
        "        \"Exposure therapy was conducted using a custom-built virtual reality environment.\",\n",
        "        \"Classical music sessions were held to measure effects on anxiety.\",\n",
        "        \"Weekly art therapy allowed participants to express emotions through painting.\",\n",
        "        # Add some unique text to create outliers\n",
        "        \"A novel intervention using quantum entanglement biofeedback was tested on a small cohort.\",\n",
        "        \"This study explored the effects of zero-gravity on cellular regeneration.\",\n",
        "        \"Participants consumed only fermented cabbage to study gut microbiome changes.\",\n",
        "    ] * (100 // 13 + 1))[:100] # Ensure the list has exactly 100 items\n",
        "}\n",
        "df_clustered = pd.DataFrame(data)\n",
        "print(f\"Sample DataFrame created with {len(df_clustered)} entries.\")\n",
        "\n",
        "# --- 2. TF-IDF Vectorization and Parameter Setup ---\n",
        "# This section creates the numerical vectors from text and sets the number of clusters.\n",
        "print(\"\\nCreating TF-IDF vectors from intervention text...\")\n",
        "vectorizer = TfidfVectorizer(max_features=100, stop_words='english', min_df=2)\n",
        "texts = df_clustered['intervention_full_text'].tolist()\n",
        "intervention_vectors = vectorizer.fit_transform(texts)\n",
        "\n",
        "# Set optimal_k to a default value if not defined\n",
        "if 'optimal_k' not in locals():\n",
        "    optimal_k = 5  # Default number of clusters for this example\n",
        "print(f\"Using {optimal_k} clusters for analysis.\")\n",
        "\n",
        "# --- 3. Outlier Identification Function ---\n",
        "# This is the core function for performing clustering and identifying outliers.\n",
        "def identify_cluster_outliers(df_valid, intervention_vectors, n_clusters):\n",
        "    \"\"\"\n",
        "    Identify studies that don't fit well into any cluster using K-Means.\n",
        "    \"\"\"\n",
        "    print(\"\\nPerforming clustering and identifying outliers...\")\n",
        "\n",
        "    # Perform clustering with optimal k\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "    df_valid['cluster'] = kmeans.fit_predict(intervention_vectors)\n",
        "\n",
        "    # Calculate distance to cluster centers\n",
        "    distances = kmeans.transform(intervention_vectors)\n",
        "    df_valid['distance_to_center'] = distances.min(axis=1)\n",
        "\n",
        "    # Calculate probability of belonging to each cluster (soft clustering)\n",
        "    # Convert distances to probabilities using softmax\n",
        "    exp_distances = np.exp(-distances)\n",
        "    probabilities = exp_distances / exp_distances.sum(axis=1, keepdims=True)\n",
        "    df_valid['max_probability'] = probabilities.max(axis=1)\n",
        "    df_valid['second_best_prob'] = np.sort(probabilities, axis=1)[:, -2]\n",
        "\n",
        "    # Identify different types of outliers\n",
        "\n",
        "    # 1. Studies far from all clusters (true outliers)\n",
        "    distance_threshold = df_valid['distance_to_center'].quantile(0.95)\n",
        "    true_outliers = df_valid[df_valid['distance_to_center'] > distance_threshold].copy()\n",
        "\n",
        "    # 2. Studies between clusters (low max probability)\n",
        "    prob_threshold = 0.5\n",
        "    borderline_studies = df_valid[df_valid['max_probability'] < prob_threshold].copy()\n",
        "\n",
        "    # 3. Studies that could belong to multiple clusters\n",
        "    ambiguous_studies = df_valid[\n",
        "        (df_valid['max_probability'] - df_valid['second_best_prob']) < 0.2\n",
        "    ].copy()\n",
        "\n",
        "    print(f\"\\nOutlier Analysis Complete:\")\n",
        "    print(f\"True outliers (far from all clusters): {len(true_outliers)} studies\")\n",
        "    print(f\"Borderline cases (between clusters): {len(borderline_studies)} studies\")\n",
        "    print(f\"Ambiguous cases (could fit multiple clusters): {len(ambiguous_studies)} studies\")\n",
        "\n",
        "    # Create a string buffer to hold the report for the AI\n",
        "    report_buffer = []\n",
        "\n",
        "    # Show examples of outliers and add to buffer\n",
        "    report_buffer.append(\"=\"*60)\n",
        "    report_buffer.append(\"EXAMPLES OF STUDIES THAT DON'T FIT WELL\")\n",
        "    report_buffer.append(\"=\"*60)\n",
        "\n",
        "    report_buffer.append(\"\\n🔴 TRUE OUTLIERS (most unique studies):\")\n",
        "    for _, row in true_outliers.nlargest(5, 'distance_to_center').iterrows():\n",
        "        line1 = f\"  • {row['title'][:80]}...\"\n",
        "        line2 = f\"    Distance from nearest cluster: {row['distance_to_center']:.3f}\"\n",
        "        line3 = f\"    Intervention: {row['intervention_type'][:100]}...\"\n",
        "        report_buffer.append(line1)\n",
        "        report_buffer.append(line2)\n",
        "        report_buffer.append(line3)\n",
        "        report_buffer.append(\"\")\n",
        "\n",
        "    report_buffer.append(\"\\n🟡 BORDERLINE CASES (between clusters):\")\n",
        "    for _, row in borderline_studies.nsmallest(5, 'max_probability').iterrows():\n",
        "        line1 = f\"  • {row['title'][:80]}...\"\n",
        "        line2 = f\"    Max cluster probability: {row['max_probability']:.2%}\"\n",
        "        line3 = f\"    Could belong to clusters: {row['cluster']} or others\"\n",
        "        report_buffer.append(line1)\n",
        "        report_buffer.append(line2)\n",
        "        report_buffer.append(line3)\n",
        "        report_buffer.append(\"\")\n",
        "\n",
        "    # Create outlier summary DataFrame\n",
        "    outlier_summary = pd.DataFrame({\n",
        "        'outlier_type': ['True Outliers', 'Borderline', 'Ambiguous'],\n",
        "        'count': [len(true_outliers), len(borderline_studies), len(ambiguous_studies)],\n",
        "        'percentage': [\n",
        "            len(true_outliers) / len(df_valid) * 100,\n",
        "            len(borderline_studies) / len(df_valid) * 100,\n",
        "            len(ambiguous_studies) / len(df_valid) * 100\n",
        "        ]\n",
        "    })\n",
        "\n",
        "    # Add summary to the report buffer\n",
        "    report_buffer.append(\"\\n\" + \"=\"*60)\n",
        "    report_buffer.append(\"OUTLIER SUMMARY\")\n",
        "    report_buffer.append(\"=\"*60)\n",
        "    report_buffer.append(outlier_summary.to_string())\n",
        "\n",
        "    # Print the full report to the console\n",
        "    full_report = \"\\n\".join(report_buffer)\n",
        "    print(full_report)\n",
        "\n",
        "    return df_valid, outlier_summary, true_outliers, full_report\n",
        "\n",
        "# --- 4. Run Analysis and Prepare for AI ---\n",
        "df_with_outliers, outlier_summary, outliers, report_for_ai = identify_cluster_outliers(\n",
        "    df_clustered,\n",
        "    intervention_vectors,\n",
        "    optimal_k\n",
        ")\n",
        "\n",
        "# --- 5. TogetherAI Integration ---\n",
        "# This section sends the analysis report to the Kimi model for interpretation.\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SENDING REPORT TO TogetherAI FOR INSIGHTS...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# IMPORTANT: Make sure you have set your TOGETHER_API_KEY as an environment variable\n",
        "# For example: export TOGETHER_API_KEY='your_api_key_here'\n",
        "if not os.getenv(\"TOGETHER_API_KEY\"):\n",
        "    print(\"ERROR: TOGETHER_API_KEY environment variable not set.\")\n",
        "    print(\"Please set your API key to use the TogetherAI service.\")\n",
        "else:\n",
        "    try:\n",
        "        client = Together()\n",
        "\n",
        "        # Create the prompt for the AI model\n",
        "        prompt = f\"\"\"\n",
        "        You are a data science research assistant. Below is a report on cluster outlier analysis\n",
        "        for a set of scientific studies based on their intervention descriptions.\n",
        "\n",
        "        Your task is to:\n",
        "        1. Briefly summarize the findings in simple terms.\n",
        "        2. Explain what 'True Outliers' and 'Borderline Cases' mean in this context.\n",
        "        3. Suggest a potential next step for a human researcher based on these findings.\n",
        "\n",
        "        Here is the report:\n",
        "        {report_for_ai}\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"Querying model: moonshotai/Kimi-K2-Instruct...\")\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"moonshotai/Kimi-K2-Instruct\",\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        ai_insights = response.choices[0].message.content\n",
        "\n",
        "        print(\"\\n🤖 AI-Generated Insights:\")\n",
        "        print(\"-\" * 25)\n",
        "        print(ai_insights)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred while contacting TogetherAI: {e}\")\n",
        "\n",
        "# --- 6. Save Final Results ---\n",
        "# The script concludes by saving the processed data.\n",
        "# base_path = './' # Define your output path\n",
        "# df_with_outliers.to_csv(base_path + 'studies_with_clusters_and_outliers.csv', index=False)\n",
        "# outliers.to_csv(base_path + 'unique_outlier_studies.csv', index=False)\n",
        "# print(\"\\nAnalysis complete. Results saved to CSV files.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rP2mIvoema-i",
        "outputId": "7788534c-8e49-472e-825b-0a32c7581be9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: together in /usr/local/lib/python3.12/dist-packages (1.5.25)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.3 in /usr/local/lib/python3.12/dist-packages (from together) (3.12.15)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.7 in /usr/local/lib/python3.12/dist-packages (from together) (8.1.8)\n",
            "Requirement already satisfied: eval-type-backport<0.3.0,>=0.1.3 in /usr/local/lib/python3.12/dist-packages (from together) (0.2.2)\n",
            "Requirement already satisfied: filelock<4.0.0,>=3.13.1 in /usr/local/lib/python3.12/dist-packages (from together) (3.19.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from together) (2.0.2)\n",
            "Requirement already satisfied: pillow<12.0.0,>=11.1.0 in /usr/local/lib/python3.12/dist-packages (from together) (11.3.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.6.3 in /usr/local/lib/python3.12/dist-packages (from together) (2.11.7)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.12/dist-packages (from together) (2.32.4)\n",
            "Requirement already satisfied: rich<15.0.0,>=13.8.1 in /usr/local/lib/python3.12/dist-packages (from together) (13.9.4)\n",
            "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from together) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.2 in /usr/local/lib/python3.12/dist-packages (from together) (4.67.1)\n",
            "Requirement already satisfied: typer<0.16,>=0.9 in /usr/local/lib/python3.12/dist-packages (from together) (0.15.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.3->together) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.3->together) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.3->together) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.3->together) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.20.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.6.3->together) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.6.3->together) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.6.3->together) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.6.3->together) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->together) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->together) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->together) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->together) (2025.8.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich<15.0.0,>=13.8.1->together) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich<15.0.0,>=13.8.1->together) (2.19.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<0.16,>=0.9->together) (1.5.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich<15.0.0,>=13.8.1->together) (0.1.2)\n",
            "Setting up sample data...\n",
            "Sample DataFrame created with 100 entries.\n",
            "\n",
            "Creating TF-IDF vectors from intervention text...\n",
            "Using 5 clusters for analysis.\n",
            "\n",
            "Performing clustering and identifying outliers...\n",
            "\n",
            "Outlier Analysis Complete:\n",
            "True outliers (far from all clusters): 0 studies\n",
            "Borderline cases (between clusters): 100 studies\n",
            "Ambiguous cases (could fit multiple clusters): 84 studies\n",
            "============================================================\n",
            "EXAMPLES OF STUDIES THAT DON'T FIT WELL\n",
            "============================================================\n",
            "\n",
            "🔴 TRUE OUTLIERS (most unique studies):\n",
            "\n",
            "🟡 BORDERLINE CASES (between clusters):\n",
            "  • Study 11...\n",
            "    Max cluster probability: 26.88%\n",
            "    Could belong to clusters: 1 or others\n",
            "\n",
            "  • Study 24...\n",
            "    Max cluster probability: 26.88%\n",
            "    Could belong to clusters: 1 or others\n",
            "\n",
            "  • Study 37...\n",
            "    Max cluster probability: 26.88%\n",
            "    Could belong to clusters: 1 or others\n",
            "\n",
            "  • Study 50...\n",
            "    Max cluster probability: 26.88%\n",
            "    Could belong to clusters: 1 or others\n",
            "\n",
            "  • Study 63...\n",
            "    Max cluster probability: 26.88%\n",
            "    Could belong to clusters: 1 or others\n",
            "\n",
            "\n",
            "============================================================\n",
            "OUTLIER SUMMARY\n",
            "============================================================\n",
            "    outlier_type  count  percentage\n",
            "0  True Outliers      0         0.0\n",
            "1     Borderline    100       100.0\n",
            "2      Ambiguous     84        84.0\n",
            "\n",
            "============================================================\n",
            "SENDING REPORT TO TogetherAI FOR INSIGHTS...\n",
            "============================================================\n",
            "ERROR: TOGETHER_API_KEY environment variable not set.\n",
            "Please set your API key to use the TogetherAI service.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install together\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from together import Together\n",
        "import os\n",
        "\n",
        "# --- 1. Setup and Sample Data Generation ---\n",
        "# In a real scenario, you would load your data here.\n",
        "# For this example, we create a sample DataFrame.\n",
        "print(\"Setting up sample data...\")\n",
        "data = {\n",
        "    'title': [f'Study {i+1}' for i in range(100)],\n",
        "    'intervention_type': [\n",
        "        'Cognitive Behavioral Therapy', 'Mindfulness Training', 'Pharmacological Treatment',\n",
        "        'Dietary Intervention', 'Exercise Program', 'Surgical Procedure', 'Placebo Control',\n",
        "        'Virtual Reality Exposure', 'Music Therapy', 'Art Therapy'\n",
        "    ] * 10,\n",
        "    'intervention_full_text': [\n",
        "        \"Participants received weekly sessions of cognitive behavioral therapy focusing on negative thought patterns.\",\n",
        "        \"A daily mindfulness meditation program was implemented for all participants.\",\n",
        "        \"The experimental group was administered a new SSRI medication daily.\",\n",
        "        \"Subjects were placed on a strict ketogenic diet for the duration of the study.\",\n",
        "        \"A high-intensity interval training (HIIT) regimen was followed three times a week.\",\n",
        "        \"Patients underwent a minimally invasive laparoscopic surgery.\",\n",
        "        \"The control group received a sugar pill with no active ingredients.\",\n",
        "        \"Exposure therapy was conducted using a custom-built virtual reality environment.\",\n",
        "        \"Classical music sessions were held to measure effects on anxiety.\",\n",
        "        \"Weekly art therapy allowed participants to express emotions through painting.\",\n",
        "        # Add some unique text to create outliers\n",
        "        \"A novel intervention using quantum entanglement biofeedback was tested on a small cohort.\",\n",
        "        \"This study explored the effects of zero-gravity on cellular regeneration.\",\n",
        "        \"Participants consumed only fermented cabbage to study gut microbiome changes.\",\n",
        "    ] * (100 // 13 + 1)\n",
        "}\n",
        "df_clustered = pd.DataFrame(data)[:100]\n",
        "print(f\"Sample DataFrame created with {len(df_clustered)} entries.\")\n",
        "\n",
        "# --- 2. TF-IDF Vectorization and Parameter Setup ---\n",
        "# This section creates the numerical vectors from text and sets the number of clusters.\n",
        "print(\"\\nCreating TF-IDF vectors from intervention text...\")\n",
        "vectorizer = TfidfVectorizer(max_features=100, stop_words='english', min_df=2)\n",
        "texts = df_clustered['intervention_full_text'].tolist()\n",
        "intervention_vectors = vectorizer.fit_transform(texts)\n",
        "\n",
        "# Set optimal_k to a default value if not defined\n",
        "if 'optimal_k' not in locals():\n",
        "    optimal_k = 5  # Default number of clusters for this example\n",
        "print(f\"Using {optimal_k} clusters for analysis.\")\n",
        "\n",
        "# --- 3. Outlier Identification Function ---\n",
        "# This is the core function for performing clustering and identifying outliers.\n",
        "def identify_cluster_outliers(df_valid, intervention_vectors, n_clusters):\n",
        "    \"\"\"\n",
        "    Identify studies that don't fit well into any cluster using K-Means.\n",
        "    \"\"\"\n",
        "    print(\"\\nPerforming clustering and identifying outliers...\")\n",
        "\n",
        "    # Perform clustering with optimal k\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "    df_valid['cluster'] = kmeans.fit_predict(intervention_vectors)\n",
        "\n",
        "    # Calculate distance to cluster centers\n",
        "    distances = kmeans.transform(intervention_vectors)\n",
        "    df_valid['distance_to_center'] = distances.min(axis=1)\n",
        "\n",
        "    # Calculate probability of belonging to each cluster (soft clustering)\n",
        "    # Convert distances to probabilities using softmax\n",
        "    exp_distances = np.exp(-distances)\n",
        "    probabilities = exp_distances / exp_distances.sum(axis=1, keepdims=True)\n",
        "    df_valid['max_probability'] = probabilities.max(axis=1)\n",
        "    df_valid['second_best_prob'] = np.sort(probabilities, axis=1)[:, -2]\n",
        "\n",
        "    # Identify different types of outliers\n",
        "\n",
        "    # 1. Studies far from all clusters (true outliers)\n",
        "    distance_threshold = df_valid['distance_to_center'].quantile(0.95)\n",
        "    true_outliers = df_valid[df_valid['distance_to_center'] > distance_threshold].copy()\n",
        "\n",
        "    # 2. Studies between clusters (low max probability)\n",
        "    prob_threshold = 0.5\n",
        "    borderline_studies = df_valid[df_valid['max_probability'] < prob_threshold].copy()\n",
        "\n",
        "    # 3. Studies that could belong to multiple clusters\n",
        "    ambiguous_studies = df_valid[\n",
        "        (df_valid['max_probability'] - df_valid['second_best_prob']) < 0.2\n",
        "    ].copy()\n",
        "\n",
        "    print(f\"\\nOutlier Analysis Complete:\")\n",
        "    print(f\"True outliers (far from all clusters): {len(true_outliers)} studies\")\n",
        "    print(f\"Borderline cases (between clusters): {len(borderline_studies)} studies\")\n",
        "    print(f\"Ambiguous cases (could fit multiple clusters): {len(ambiguous_studies)} studies\")\n",
        "\n",
        "    # Create a string buffer to hold the report for the AI\n",
        "    report_buffer = []\n",
        "\n",
        "    # Show examples of outliers and add to buffer\n",
        "    report_buffer.append(\"=\"*60)\n",
        "    report_buffer.append(\"EXAMPLES OF STUDIES THAT DON'T FIT WELL\")\n",
        "    report_buffer.append(\"=\"*60)\n",
        "\n",
        "    report_buffer.append(\"\\n🔴 TRUE OUTLIERS (most unique studies):\")\n",
        "    for _, row in true_outliers.nlargest(5, 'distance_to_center').iterrows():\n",
        "        line1 = f\"  • {row['title'][:80]}...\"\n",
        "        line2 = f\"    Distance from nearest cluster: {row['distance_to_center']:.3f}\"\n",
        "        line3 = f\"    Intervention: {row['intervention_type'][:100]}...\"\n",
        "        report_buffer.append(line1)\n",
        "        report_buffer.append(line2)\n",
        "        report_buffer.append(line3)\n",
        "        report_buffer.append(\"\")\n",
        "\n",
        "    report_buffer.append(\"\\n🟡 BORDERLINE CASES (between clusters):\")\n",
        "    for _, row in borderline_studies.nsmallest(5, 'max_probability').iterrows():\n",
        "        line1 = f\"  • {row['title'][:80]}...\"\n",
        "        line2 = f\"    Max cluster probability: {row['max_probability']:.2%}\"\n",
        "        line3 = f\"    Could belong to clusters: {row['cluster']} or others\"\n",
        "        report_buffer.append(line1)\n",
        "        report_buffer.append(line2)\n",
        "        report_buffer.append(line3)\n",
        "        report_buffer.append(\"\")\n",
        "\n",
        "    # Create outlier summary DataFrame\n",
        "    outlier_summary = pd.DataFrame({\n",
        "        'outlier_type': ['True Outliers', 'Borderline', 'Ambiguous'],\n",
        "        'count': [len(true_outliers), len(borderline_studies), len(ambiguous_studies)],\n",
        "        'percentage': [\n",
        "            len(true_outliers) / len(df_valid) * 100,\n",
        "            len(borderline_studies) / len(df_valid) * 100,\n",
        "            len(ambiguous_studies) / len(df_valid) * 100\n",
        "        ]\n",
        "    })\n",
        "\n",
        "    # Add summary to the report buffer\n",
        "    report_buffer.append(\"\\n\" + \"=\"*60)\n",
        "    report_buffer.append(\"OUTLIER SUMMARY\")\n",
        "    report_buffer.append(\"=\"*60)\n",
        "    report_buffer.append(outlier_summary.to_string())\n",
        "\n",
        "    # Print the full report to the console\n",
        "    full_report = \"\\n\".join(report_buffer)\n",
        "    print(full_report)\n",
        "\n",
        "    return df_valid, outlier_summary, true_outliers, full_report\n",
        "\n",
        "# --- 4. Run Analysis and Prepare for AI ---\n",
        "df_with_outliers, outlier_summary, outliers, report_for_ai = identify_cluster_outliers(\n",
        "    df_clustered,\n",
        "    intervention_vectors,\n",
        "    optimal_k\n",
        ")\n",
        "\n",
        "# --- 5. TogetherAI Integration ---\n",
        "# This section sends the analysis report to the Kimi model for interpretation.\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SENDING REPORT TO TogetherAI FOR INSIGHTS...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# IMPORTANT: Make sure you have set your TOGETHER_API_KEY as an environment variable\n",
        "# For example: export TOGETHER_API_KEY='your_api_key_here'\n",
        "if not os.getenv(\"TOGETHER_API_KEY\"):\n",
        "    print(\"ERROR: TOGETHER_API_KEY environment variable not set.\")\n",
        "    print(\"Please set your API key to use the TogetherAI service.\")\n",
        "else:\n",
        "    try:\n",
        "        client = Together()\n",
        "\n",
        "        # Create the prompt for the AI model\n",
        "        prompt = f\"\"\"\n",
        "        You are a data science research assistant. Below is a report on cluster outlier analysis\n",
        "        for a set of scientific studies based on their intervention descriptions.\n",
        "\n",
        "        Your task is to:\n",
        "        1. Briefly summarize the findings in simple terms.\n",
        "        2. Explain what 'True Outliers' and 'Borderline Cases' mean in this context.\n",
        "        3. Suggest a potential next step for a human researcher based on these findings.\n",
        "\n",
        "        Here is the report:\n",
        "        {report_for_ai}\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"Querying model: moonshotai/Kimi-K2-Instruct...\")\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"moonshotai/Kimi-K2-Instruct\",\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        ai_insights = response.choices[0].message.content\n",
        "\n",
        "        print(\"\\n🤖 AI-Generated Insights:\")\n",
        "        print(\"-\" * 25)\n",
        "        print(ai_insights)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred while contacting TogetherAI: {e}\")\n",
        "\n",
        "# --- 6. Save Final Results ---\n",
        "# The script concludes by saving the processed data.\n",
        "# base_path = './' # Define your output path\n",
        "# df_with_outliers.to_csv(base_path + 'studies_with_clusters_and_outliers.csv', index=False)\n",
        "# outliers.to_csv(base_path + 'unique_outlier_studies.csv', index=False)\n",
        "# print(\"\\nAnalysis complete. Results saved to CSV files.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TNlDWUV_moHo",
        "outputId": "caf08d3b-591b-49e9-8807-7fae107718ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting together\n",
            "  Downloading together-1.5.25-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.3 in /usr/local/lib/python3.12/dist-packages (from together) (3.12.15)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.7 in /usr/local/lib/python3.12/dist-packages (from together) (8.2.1)\n",
            "Collecting eval-type-backport<0.3.0,>=0.1.3 (from together)\n",
            "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: filelock<4.0.0,>=3.13.1 in /usr/local/lib/python3.12/dist-packages (from together) (3.19.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from together) (2.0.2)\n",
            "Requirement already satisfied: pillow<12.0.0,>=11.1.0 in /usr/local/lib/python3.12/dist-packages (from together) (11.3.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.6.3 in /usr/local/lib/python3.12/dist-packages (from together) (2.11.7)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.12/dist-packages (from together) (2.32.4)\n",
            "Requirement already satisfied: rich<15.0.0,>=13.8.1 in /usr/local/lib/python3.12/dist-packages (from together) (13.9.4)\n",
            "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from together) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.2 in /usr/local/lib/python3.12/dist-packages (from together) (4.67.1)\n",
            "Collecting typer<0.16,>=0.9 (from together)\n",
            "  Downloading typer-0.15.4-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.3->together) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.3->together) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.3->together) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.3->together) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.20.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.6.3->together) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.6.3->together) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.6.3->together) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.6.3->together) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->together) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->together) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->together) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->together) (2025.8.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich<15.0.0,>=13.8.1->together) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich<15.0.0,>=13.8.1->together) (2.19.2)\n",
            "Collecting click<9.0.0,>=8.1.7 (from together)\n",
            "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<0.16,>=0.9->together) (1.5.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich<15.0.0,>=13.8.1->together) (0.1.2)\n",
            "Downloading together-1.5.25-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
            "Downloading typer-0.15.4-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: eval-type-backport, click, typer, together\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.2.1\n",
            "    Uninstalling click-8.2.1:\n",
            "      Successfully uninstalled click-8.2.1\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.16.0\n",
            "    Uninstalling typer-0.16.0:\n",
            "      Successfully uninstalled typer-0.16.0\n",
            "Successfully installed click-8.1.8 eval-type-backport-0.2.2 together-1.5.25 typer-0.15.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "click"
                ]
              },
              "id": "84e75085651f4e19bd7e48a0c0cc09c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up sample data...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "All arrays must be of the same length",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1402364960.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     ] * (100 // 13 + 1)\n\u001b[1;32m     37\u001b[0m }\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mdf_clustered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Sample DataFrame created with {len(df_clustered)} entries.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All arrays must be of the same length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
          ]
        }
      ]
    }
  ]
}